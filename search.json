[
  {
    "objectID": "chapter_1/pandas.html",
    "href": "chapter_1/pandas.html",
    "title": "Pandas and Data",
    "section": "",
    "text": "You know the basics. What are Jupyter notebooks, how do they work, and how do you run Python in them. It is time to start using them for data science (no, that simple math you did the last time doesn’t count as data science).\nYou are about to enter the PyData ecosystem. It means that you will start learning how to work with Python from the middle. This course does not explicitly cover the fundamentals of programming. It is expected that those parts you need you’ll be able to pick as you go through the specialised data science stack. If you’re stuck, confused or need further explanation, use Google (or your favourite search engine), ask AI to explain the code or ask on Discord or during the class. Not everything will be told during the course (by design), and the internet is a friend of every programmer, so let’s figure out how to use it efficiently from the beginning.\nLet’s dig in!"
  },
  {
    "objectID": "chapter_1/pandas.html#munging-and-wrangling",
    "href": "chapter_1/pandas.html#munging-and-wrangling",
    "title": "Pandas and Data",
    "section": "Munging and wrangling",
    "text": "Munging and wrangling\nReal-world datasets are messy. There is no way around it: datasets have “holes” (missing data), the amount of formats in which data can be stored is endless, and the best structure to share data is not always the optimum to analyse them, hence the need to munge[1] them. As has been correctly pointed out in many outlets, much of the time spent in what is called Data Science is related not only to sophisticated modelling and insight but has to do with much more basic and less exotic tasks such as obtaining data, processing, and turning them into a shape that makes analysis possible, and exploring it to get to know their basic properties.\nSurprisingly, very little has been published on patterns, techniques, and best practices for quick and efficient data cleaning, manipulation, and transformation because of how labour-intensive and relevant this aspect is. In this session, you will use a few real-world datasets and learn how to process them into Python so they can be transformed and manipulated, if necessary, and analysed. For this, you will introduce some of the bread and butter of data analysis and scientific computing in Python. These are fundamental tools that are constantly used in almost any task relating to data analysis.\nThis notebook covers the basics and the content that is expected to be learnt by every student. You use a prepared dataset that saves us much of the more intricate processing that goes beyond the introductory level the session is aimed at. If you are interested in how it was done, there is a notebook.\nThis notebook discusses several patterns to clean and structure data properly, including tidying, subsetting, and aggregating. You finish with some basic visualisation. An additional extension presents more advanced tricks to manipulate tabular data."
  },
  {
    "objectID": "chapter_1/pandas.html#dataset",
    "href": "chapter_1/pandas.html#dataset",
    "title": "Pandas and Data",
    "section": "Dataset",
    "text": "Dataset\nYou will be exploring demographic characteristics of Madrid. The data has been aggregated to a neighbourhood level by the statistic’s office of Madrid’s City Hall. It contains information per year (from 2018 to 2023) and genre.\nAs with many datasets that will be used during this course, the data was originally gound in the data portal on the following link\nThe main tool you should use for this task is the pandas package. As with the math you used before, you must import it first.\n[1] Data munging and data wrangling are used interchangeably. Pick the one you like.\n\nimport pandas as pd\n\nThe data is stored in a CSV file. To make things easier, you can read data from a file posted online so, for now, you do not need to download any dataset:\n\nmadrid_pop = pd.read_csv(\n    \"https://datos.madrid.es/egob/catalogo/300557-0-poblacion-distrito-barrio.csv\",\n    sep=\";\",\n)\n\n\nTip\nYou are using read_csv because the file you want to read is in CSV format. However, the current data is actually not separated by commas , but with a semicolon ;, hence the additional parameter in the code.\nNote that pandas allows for many more formats to be read and write. A full list of formats supported may be found in the documentation.\n\n\nAlternative\nInstead of reading the file directly off the web, it is possible to download it manually, store it on your computer, and read it locally. To do that, you can follow these steps:\n\nDownload the file by clicking on this link and saving the file\nPlace the file in the same folder as the notebook where you intend to read it\nReplace the code in the cell above with:\n\nmadrid_pop = pd.read_csv(\n    \"poblacion_1_enero.csv\",\n    sep=\";\",\n    index_col=\"distrito\",\n)"
  },
  {
    "objectID": "chapter_1/pandas.html#pandas-101",
    "href": "chapter_1/pandas.html#pandas-101",
    "title": "Pandas and Data",
    "section": "Pandas 101",
    "text": "Pandas 101\nNow, you are ready to start playing and interrogating the dataset! What you have at your fingertips is a table summarising, for each of the districts in Madrid, how many people lived there by genre. These tables are called DataFrame objects, and they have a lot of functionality built-in to explore and manipulate the data they contain. Let’s explore a few of those cool tricks!\n\nData Structures\nThe first aspect worth spending a bit of time on is the structure of a DataFrame. You can print it by simply typing its name:\n\nmadrid_pop\n\n\n\n\n\n\n\n\nfecha\ncod_municipio\nmunicipio\ncod_distrito\ndistrito\ncod_barrio\nbarrio\nnum_personas\nnum_personas_hombres\nnum_personas_mujeres\n\n\n\n\n0\n1 de enero de 2023\n28079\nMadrid\n1\nCentro\n1\nCentro\n139.687\n70.770\n68.917\n\n\n1\n1 de enero de 2023\n28079\nMadrid\n2\nArganzuela\n2\nArganzuela\n153.304\n71.754\n81.550\n\n\n2\n1 de enero de 2023\n28079\nMadrid\n3\nRetiro\n3\nRetiro\n117.918\n53.458\n64.460\n\n\n3\n1 de enero de 2023\n28079\nMadrid\n4\nSalamanca\n4\nSalamanca\n145.702\n64.452\n81.250\n\n\n4\n1 de enero de 2023\n28079\nMadrid\n5\nChamartín\n5\nChamartín\n144.796\n65.245\n79.551\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n913\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n212\nAeropuerto\n1.794\n922\n872\n\n\n914\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n213\nCascoHistóricodeBarajas\n7.336\n3.550\n3.786\n\n\n915\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n214\nTimón\n11.750\n5.651\n6.099\n\n\n916\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n215\nCorralejos\n7.510\n3.657\n3.853\n\n\n917\n1 de enero de 2018\n28079\nMadrid\nTodos\nTodos\nTodos\nTodos\n3.221.824\n1.500.340\n1.721.484\n\n\n\n\n918 rows × 10 columns\n\n\n\nAs you can expect, the dataset was inputed in the original local languange so to make things a little easier, we are going to translate the column names using one list of texts.\n\nNote that there are multiple ways of arriving at the same output, the below is just one of them\n\n\nen_cols = [\n    \"date\",\n    \"code_municipality\",\n    \"municipality\",\n    \"code_district\",\n    \"district\",\n    \"code_neighbourhood\",\n    \"neighbourhood\",\n    \"num_people\",\n    \"num_people_men\",\n    \"num_people_women\",\n]\n\nmadrid_pop.columns = en_cols\nmadrid_pop\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ndistrict\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\n\n\n\n\n0\n1 de enero de 2023\n28079\nMadrid\n1\nCentro\n1\nCentro\n139.687\n70.770\n68.917\n\n\n1\n1 de enero de 2023\n28079\nMadrid\n2\nArganzuela\n2\nArganzuela\n153.304\n71.754\n81.550\n\n\n2\n1 de enero de 2023\n28079\nMadrid\n3\nRetiro\n3\nRetiro\n117.918\n53.458\n64.460\n\n\n3\n1 de enero de 2023\n28079\nMadrid\n4\nSalamanca\n4\nSalamanca\n145.702\n64.452\n81.250\n\n\n4\n1 de enero de 2023\n28079\nMadrid\n5\nChamartín\n5\nChamartín\n144.796\n65.245\n79.551\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n913\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n212\nAeropuerto\n1.794\n922\n872\n\n\n914\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n213\nCascoHistóricodeBarajas\n7.336\n3.550\n3.786\n\n\n915\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n214\nTimón\n11.750\n5.651\n6.099\n\n\n916\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n215\nCorralejos\n7.510\n3.657\n3.853\n\n\n917\n1 de enero de 2018\n28079\nMadrid\nTodos\nTodos\nTodos\nTodos\n3.221.824\n1.500.340\n1.721.484\n\n\n\n\n918 rows × 10 columns\n\n\n\nNote the printing is cut to keep a nice and compact view but enough to see its structure. Since they represent a table of data, DataFrame objects have two dimensions: rows and columns. Each of these is automatically assigned a name in what you will call its index. When printing, the index of each dimension is rendered in bold, as opposed to the standard rendering for the content. The example above shows how the column index is automatically picked up from the .csv file’s column names. For rows, we will specify disctrict to be used as the index.\n\nIn our case, because we had not established the index_col from the loading of the file pandas automatically generates a sequence starting in 0 and going all the way to the number of rows minus one. This is the standard structure of a DataFrame object, so you will come to it over and over. Importantly, even when you move to spatial data, your datasets will have a similar structure.\n\n\n# Remove leading and trailing spaces from 'distrito'\nmadrid_pop[\"district\"] = madrid_pop[\"district\"].str.strip()\nmadrid_pop.set_index(\"district\", inplace=True)\n\nOne final feature that is worth mentioning about these tables is that they can hold columns with different types of data. In this example, you have counts (or int for integer types) and ratios (or ‘float’ for floating point numbers - a number with decimals) for each column. But it is useful to keep in mind that you can combine this with columns that hold other types of data such as categories, text (str, for string), dates or, as you will see later in the course, geographic features.\nTo extract a single column from this DataFrame, specify its name in the square brackets ([]). Note that the name, in this case, is a string. A piece of text. As such, it needs to be within single (') or double quotes (\"). The resulting data structure is no longer a DataFrame, but you have a Series because you deal with a single column.\n\nmadrid_pop[\"num_people_women\"]\n\ndistrict\nCentro           68.917\nArganzuela       81.550\nRetiro           64.460\nSalamanca        81.250\nChamartín        79.551\n                ...    \nBarajas             872\nBarajas           3.786\nBarajas           6.099\nBarajas           3.853\nTodos         1.721.484\nName: num_people_women, Length: 918, dtype: object\n\n\n\n\nInspect\nInspecting what it looks like. You can check the table’s top (or bottom) X lines by passing X to the method head (tail). For example, for the top/bottom five lines:\n\nmadrid_pop.head()\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\n\n\ndistrict\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n1\nCentro\n139.687\n70.770\n68.917\n\n\nArganzuela\n1 de enero de 2023\n28079\nMadrid\n2\n2\nArganzuela\n153.304\n71.754\n81.550\n\n\nRetiro\n1 de enero de 2023\n28079\nMadrid\n3\n3\nRetiro\n117.918\n53.458\n64.460\n\n\nSalamanca\n1 de enero de 2023\n28079\nMadrid\n4\n4\nSalamanca\n145.702\n64.452\n81.250\n\n\nChamartín\n1 de enero de 2023\n28079\nMadrid\n5\n5\nChamartín\n144.796\n65.245\n79.551\n\n\n\n\n\n\n\n\nmadrid_pop.tail(3)\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\n\n\ndistrict\n\n\n\n\n\n\n\n\n\n\n\n\n\nBarajas\n1 de enero de 2018\n28079\nMadrid\n21\n214\nTimón\n11.750\n5.651\n6.099\n\n\nBarajas\n1 de enero de 2018\n28079\nMadrid\n21\n215\nCorralejos\n7.510\n3.657\n3.853\n\n\nTodos\n1 de enero de 2018\n28079\nMadrid\nTodos\nTodos\nTodos\n3.221.824\n1.500.340\n1.721.484\n\n\n\n\n\n\n\nInspecting your datasets is vital to find errors that could skew your analysis. As you can see, the last row of our dataset has been aggregated to consolidate the sum of all column values ( Spanish: Todos -&gt; English: All ).\nBefore continuing, let’s fix that. First we are going to check if we have any more occurrences of Todos.\n\nmadrid_pop.loc[\"Todos\"]\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\n\n\ndistrict\n\n\n\n\n\n\n\n\n\n\n\n\n\nTodos\n1 de enero de 2023\n28079\nMadrid\nTodos\nTodos\nTodos\n3.339.931\n1.559.866\n1.780.065\n\n\nTodos\n1 de enero de 2022\n28079\nMadrid\nTodos\nTodos\nTodos\n3.286.662\n1.534.824\n1.751.838\n\n\nTodos\n1 de enero de 2021\n28079\nMadrid\nTodos\nTodos\nTodos\n3.312.310\n1.545.157\n1.767.153\n\n\nTodos\n1 de enero de 2020\n28079\nMadrid\nTodos\nTodos\nTodos\n3.334.730\n1.554.732\n1.779.998\n\n\nTodos\n1 de enero de 2019\n28079\nMadrid\nTodos\nTodos\nTodos\n3.266.126\n1.521.178\n1.744.948\n\n\nTodos\n1 de enero de 2018\n28079\nMadrid\nTodos\nTodos\nTodos\n3.221.824\n1.500.340\n1.721.484\n\n\n\n\n\n\n\nNow that we know that for every year the data has been aggregated and added back as a new row, we know that removing the last row would not be enough to correct our DataFrame.\nWe will make use of the drop function in combination with the native loc function of DataFrames.\n\nprint(\"Rows before droping values: \", len(madrid_pop))\nmadrid_pop.drop(index=\"Todos\", inplace=True)\nprint(\"Rows after droping values: \", len(madrid_pop))\n\nRows before droping values:  918\nRows after droping values:  912\n\n\nNow, let’s get an overview of the table:\n\nmadrid_pop.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 912 entries, Centro to Barajas\nData columns (total 9 columns):\n #   Column              Non-Null Count  Dtype \n---  ------              --------------  ----- \n 0   date                912 non-null    object\n 1   code_municipality   912 non-null    int64 \n 2   municipality        912 non-null    object\n 3   code_district       912 non-null    object\n 4   code_neighbourhood  912 non-null    object\n 5   neighbourhood       912 non-null    object\n 6   num_people          912 non-null    object\n 7   num_people_men      912 non-null    object\n 8   num_people_women    912 non-null    object\ndtypes: int64(1), object(8)\nmemory usage: 71.2+ KB\n\n\n\nCan you spot something wrong ?\n\nIntereger numbers can sometimes be used as category tags (like for the case of code_district or code_neighbourhood). However, the actual values of population counts would be better stored as full integers. For that, we will perform the following operations :\n\nList the columns that have people counts\nLoop throuhg each one and :\n\n\nRemove all dots .\nConvert the values to numeric; allowing for potentia NaN values\nSpecify that the values are Int64\n\n\n# List of columns to process\ncolumns_to_process = [\"num_people\", \"num_people_men\", \"num_people_women\"]\n\n# Loop through columns and perform operations\nfor column in columns_to_process:\n    madrid_pop[column] = madrid_pop[column].str.replace(\".\", \"\")  # Remove dots\n    madrid_pop[column] = pd.to_numeric(\n        madrid_pop[column], errors=\"coerce\"\n    )  # Convert to numeric\n    madrid_pop[column] = madrid_pop[column].astype(\"Int64\")  # Convert to Int64\n\nmadrid_pop.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 912 entries, Centro to Barajas\nData columns (total 9 columns):\n #   Column              Non-Null Count  Dtype \n---  ------              --------------  ----- \n 0   date                912 non-null    object\n 1   code_municipality   912 non-null    int64 \n 2   municipality        912 non-null    object\n 3   code_district       912 non-null    object\n 4   code_neighbourhood  912 non-null    object\n 5   neighbourhood       912 non-null    object\n 6   num_people          912 non-null    Int64 \n 7   num_people_men      912 non-null    Int64 \n 8   num_people_women    912 non-null    Int64 \ndtypes: Int64(3), int64(1), object(5)\nmemory usage: 73.9+ KB\n\n\n\n\nSummarise\nOr of the values of the table:\n\nmadrid_pop.describe()\n\n\n\n\n\n\n\n\ncode_municipality\nnum_people\nnum_people_men\nnum_people_women\n\n\n\n\ncount\n912.0\n912.0\n912.0\n912.0\n\n\nmean\n28079.0\n43336.804825\n20210.739035\n23126.065789\n\n\nstd\n0.0\n51564.255101\n24072.657139\n27522.444389\n\n\nmin\n28079.0\n945.0\n490.0\n455.0\n\n\n25%\n28079.0\n17330.5\n8132.0\n9267.25\n\n\n50%\n28079.0\n24700.5\n11580.0\n13640.0\n\n\n75%\n28079.0\n42166.5\n19700.25\n22349.75\n\n\nmax\n28079.0\n262339.0\n122632.0\n139707.0\n\n\n\n\n\n\n\nNote how the output is also a DataFrame object, so you can do with it the same things you would with the original table (e.g. writing it to a file).\nIn this case, the summary might be better presented if the table is “transposed”:\n\nmadrid_pop[columns_to_process].describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nnum_people\n912.0\n43336.804825\n51564.255101\n945.0\n17330.5\n24700.5\n42166.5\n262339.0\n\n\nnum_people_men\n912.0\n20210.739035\n24072.657139\n490.0\n8132.0\n11580.0\n19700.25\n122632.0\n\n\nnum_people_women\n912.0\n23126.065789\n27522.444389\n455.0\n9267.25\n13640.0\n22349.75\n139707.0\n\n\n\n\n\n\n\nEqually, common descriptive statistics are also available. To obtain minimum values for each column, you can use .min().\n\nmadrid_pop.min()\n\ndate                  1 de enero de 2018\ncode_municipality                  28079\nmunicipality                      Madrid\ncode_district                          1\ncode_neighbourhood                     1\nneighbourhood                 Arganzuela\nnum_people                           945\nnum_people_men                       490\nnum_people_women                     455\ndtype: object\n\n\nOr to obtain a minimum for a single column only.\n\nmadrid_pop[\"num_people_women\"].min()\n\n455\n\n\nNote here how you have restricted the calculation of the minimum value to one column only by getting the Series and calling .min() on that.\nSimilarly, you can restrict the calculations to a single district using .loc[] indexer:\n\nmadrid_pop.loc[\"Centro\"].min()\n\ndate                  1 de enero de 2018\ncode_municipality                  28079\nmunicipality                      Madrid\ncode_district                          1\ncode_neighbourhood                     1\nneighbourhood                     Centro\nnum_people                          7201\nnum_people_men                      3672\nnum_people_women                    3529\ndtype: object\n\n\nLet’s see when and where did said minimum ocurred.\n\nmadrid_pop[madrid_pop[\"num_people_women\"] == 3529]\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\n\n\ndistrict\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n16\nSol\n7201\n3672\n3529\n\n\n\n\n\n\n\n\n\nCreate new columns\nYou can generate new variables by applying operations to existing ones. For example, you can calculate the ratio of women.\n\nratio_women = madrid_pop[\"num_people_women\"] / madrid_pop[\"num_people\"]\nratio_women.head()\n\ndistrict\nCentro        0.493367\nArganzuela     0.53195\nRetiro        0.546651\nSalamanca     0.557645\nChamartín     0.549401\ndtype: Float64\n\n\nOnce you have created the variable, you can make it part of the table:\n\nmadrid_pop[\"ratio_women\"] = ratio_women\nmadrid_pop.head()\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\n\n\ndistrict\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n1\nCentro\n139687\n70770\n68917\n0.493367\n\n\nArganzuela\n1 de enero de 2023\n28079\nMadrid\n2\n2\nArganzuela\n153304\n71754\n81550\n0.53195\n\n\nRetiro\n1 de enero de 2023\n28079\nMadrid\n3\n3\nRetiro\n117918\n53458\n64460\n0.546651\n\n\nSalamanca\n1 de enero de 2023\n28079\nMadrid\n4\n4\nSalamanca\n145702\n64452\n81250\n0.557645\n\n\nChamartín\n1 de enero de 2023\n28079\nMadrid\n5\n5\nChamartín\n144796\n65245\n79551\n0.549401\n\n\n\n\n\n\n\n\n\nIndex-based queries\nHere, you explore how to subset parts of a DataFrame if you know exactly which bits you want. For example, if you want to extract the influenza mortality and total population of the first four areas in the table, you use loc with lists:\n\nwomen_ratio_2districts = madrid_pop.loc[\n    [\"Centro\", \"Retiro\"],\n    [\"date\", \"ratio_women\"],\n]\nwomen_ratio_2districts\n\n\n\n\n\n\n\n\ndate\nratio_women\n\n\ndistrict\n\n\n\n\n\n\nCentro\n1 de enero de 2023\n0.493367\n\n\nCentro\n1 de enero de 2023\n0.508234\n\n\nCentro\n1 de enero de 2023\n0.47853\n\n\nCentro\n1 de enero de 2023\n0.505362\n\n\nCentro\n1 de enero de 2023\n0.491685\n\n\n...\n...\n...\n\n\nRetiro\n1 de enero de 2018\n0.540638\n\n\nRetiro\n1 de enero de 2018\n0.535416\n\n\nRetiro\n1 de enero de 2018\n0.562966\n\n\nRetiro\n1 de enero de 2018\n0.531449\n\n\nRetiro\n1 de enero de 2018\n0.545746\n\n\n\n\n84 rows × 2 columns\n\n\n\nYou can see how you can create a list with the names (index IDs) along each of the two dimensions of a DataFrame (rows and columns), and loc will return a subset of the original table only with the elements queried for.\nAn alternative to list-based queries is what is called “range-based” queries. These work on the indices of the table, but instead of requiring the ID of each item you want to retrieve, they operate by requiring only two IDs: the first and last element in a range of items. Range queries are expressed with a colon (:). However, to perform this operation Index IDs need to be unique. Since this is not our case we will create a new index composed the year and the code_neighbourhood as a new index to our DataFrame.\n\n# Reset the index to bring back 'district' as a regular column\nmadrid_pop.reset_index(inplace=True)\n# Extract the last 4 digits from the 'date' column and create a new 'year' column\nmadrid_pop[\"year\"] = madrid_pop[\"date\"].str[-4:]\n# Create a new column with the combination of 'year' and 'code_neighbourhood'\nmadrid_pop[\"new_index\"] = madrid_pop[\"year\"] + \"_\" + madrid_pop[\"code_neighbourhood\"]\n# Set the new index\nmadrid_pop.set_index(\"new_index\", inplace=True)\nmadrid_pop.head(3)\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023_1\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n1\nCentro\n139687\n70770\n68917\n0.493367\n2023\n\n\n2023_2\nArganzuela\n1 de enero de 2023\n28079\nMadrid\n2\n2\nArganzuela\n153304\n71754\n81550\n0.53195\n2023\n\n\n2023_3\nRetiro\n1 de enero de 2023\n28079\nMadrid\n3\n3\nRetiro\n117918\n53458\n64460\n0.546651\n2023\n\n\n\n\n\n\n\nLook at the order of the index. This is important beacuase “range-based” queries assume you understand what come first in the data to input the index cutoff values.\n\nrange_query = madrid_pop.loc[\"2019_1\":\"2018_1\", \"num_people\":\"num_people_women\"]\n\nrange_query\n\n\n\n\n\n\n\n\nnum_people\nnum_people_men\nnum_people_women\n\n\nnew_index\n\n\n\n\n\n\n\n2019_1\n134881\n67829\n67052\n\n\n2019_2\n153830\n71631\n82199\n\n\n2019_3\n119379\n54098\n65281\n\n\n2019_4\n146148\n64395\n81753\n\n\n2019_5\n145865\n65565\n80300\n\n\n...\n...\n...\n...\n\n\n2019_212\n1851\n952\n899\n\n\n2019_213\n7565\n3648\n3917\n\n\n2019_214\n12388\n5916\n6472\n\n\n2019_215\n7642\n3746\n3896\n\n\n2018_1\n132352\n66320\n66032\n\n\n\n\n153 rows × 3 columns\n\n\n\nThe range query picks up all the elements between the specified IDs. Note that for this to work, the first ID in the range needs to be placed before the second one in the table’s index.\nOnce you know about list and range-based queries, you can combine them!\n\n\nCondition-based queries\nHowever, sometimes, you do not know exactly which observations you want, but you do know what conditions they need to satisfy (e.g. areas with more than 2,000 inhabitants). For these cases, DataFrames support selection based on conditions. Let us see a few examples. Suppose you want to select…\n… neighbourhoods wich over time have had less than 50% of women\n\nfewer_women = madrid_pop[madrid_pop[\"ratio_women\"] &lt; 0.5]\nfewer_women\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023_1\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n1\nCentro\n139687\n70770\n68917\n0.493367\n2023\n\n\n2023_12\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n12\nEmbajadores\n46204\n24094\n22110\n0.47853\n2023\n\n\n2023_14\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n14\nJusticia\n18219\n9261\n8958\n0.491685\n2023\n\n\n2023_16\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n16\nSol\n8164\n4239\n3925\n0.480769\n2023\n\n\n2023_81\nFuencarral-El Pardo\n1 de enero de 2023\n28079\nMadrid\n8\n81\nElPardo\n3421\n1716\n1705\n0.498392\n2023\n\n\n2023_106\nLatina\n1 de enero de 2023\n28079\nMadrid\n10\n106\nCuatroVientos\n6122\n3068\n3054\n0.498857\n2023\n\n\n2023_194\nVicálvaro\n1 de enero de 2023\n28079\nMadrid\n19\n194\nElCañaveral\n13054\n6652\n6402\n0.490424\n2023\n\n\n2023_212\nBarajas\n1 de enero de 2023\n28079\nMadrid\n21\n212\nAeropuerto\n1902\n965\n937\n0.492639\n2023\n\n\n2022_1\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n1\nCentro\n139682\n70986\n68696\n0.491803\n2022\n\n\n2022_12\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n12\nEmbajadores\n46444\n24271\n22173\n0.477414\n2022\n\n\n2022_14\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n14\nJusticia\n18015\n9221\n8794\n0.488149\n2022\n\n\n2022_16\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n16\nSol\n8117\n4232\n3885\n0.478625\n2022\n\n\n2022_106\nLatina\n1 de enero de 2022\n28079\nMadrid\n10\n106\nCuatroVientos\n5966\n2996\n2970\n0.497821\n2022\n\n\n2022_194\nVicálvaro\n1 de enero de 2022\n28079\nMadrid\n19\n194\nElCañaveral\n8944\n4525\n4419\n0.494074\n2022\n\n\n2022_212\nBarajas\n1 de enero de 2022\n28079\nMadrid\n21\n212\nAeropuerto\n1895\n967\n928\n0.48971\n2022\n\n\n2021_1\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n1\nCentro\n141236\n71881\n69355\n0.491058\n2021\n\n\n2021_12\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n12\nEmbajadores\n47238\n24767\n22471\n0.475698\n2021\n\n\n2021_14\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n14\nJusticia\n18208\n9291\n8917\n0.48973\n2021\n\n\n2021_16\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n16\nSol\n7993\n4120\n3873\n0.484549\n2021\n\n\n2021_81\nFuencarral-El Pardo\n1 de enero de 2021\n28079\nMadrid\n8\n81\nElPardo\n3443\n1723\n1720\n0.499564\n2021\n\n\n2021_194\nVicálvaro\n1 de enero de 2021\n28079\nMadrid\n19\n194\nElCañaveral\n4430\n2254\n2176\n0.491196\n2021\n\n\n2021_212\nBarajas\n1 de enero de 2021\n28079\nMadrid\n21\n212\nAeropuerto\n1918\n988\n930\n0.48488\n2021\n\n\n2020_1\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n1\nCentro\n140473\n71127\n69346\n0.493661\n2020\n\n\n2020_12\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n12\nEmbajadores\n47048\n24497\n22551\n0.479319\n2020\n\n\n2020_14\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n14\nJusticia\n18021\n9161\n8860\n0.491649\n2020\n\n\n2020_16\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n16\nSol\n7622\n3895\n3727\n0.488979\n2020\n\n\n2020_106\nLatina\n1 de enero de 2020\n28079\nMadrid\n10\n106\nCuatroVientos\n5881\n2958\n2923\n0.497024\n2020\n\n\n2020_194\nVicálvaro\n1 de enero de 2020\n28079\nMadrid\n19\n194\nElCañaveral\n2398\n1230\n1168\n0.487073\n2020\n\n\n2020_212\nBarajas\n1 de enero de 2020\n28079\nMadrid\n21\n212\nAeropuerto\n1900\n975\n925\n0.486842\n2020\n\n\n2019_1\nCentro\n1 de enero de 2019\n28079\nMadrid\n1\n1\nCentro\n134881\n67829\n67052\n0.49712\n2019\n\n\n2019_12\nCentro\n1 de enero de 2019\n28079\nMadrid\n1\n12\nEmbajadores\n45259\n23390\n21869\n0.483197\n2019\n\n\n2019_14\nCentro\n1 de enero de 2019\n28079\nMadrid\n1\n14\nJusticia\n17153\n8675\n8478\n0.494258\n2019\n\n\n2019_16\nCentro\n1 de enero de 2019\n28079\nMadrid\n1\n16\nSol\n7337\n3749\n3588\n0.489028\n2019\n\n\n2019_106\nLatina\n1 de enero de 2019\n28079\nMadrid\n10\n106\nCuatroVientos\n5748\n2909\n2839\n0.493911\n2019\n\n\n2019_194\nVicálvaro\n1 de enero de 2019\n28079\nMadrid\n19\n194\nElCañaveral\n1530\n785\n745\n0.486928\n2019\n\n\n2019_212\nBarajas\n1 de enero de 2019\n28079\nMadrid\n21\n212\nAeropuerto\n1851\n952\n899\n0.485683\n2019\n\n\n2018_1\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n1\nCentro\n132352\n66320\n66032\n0.498912\n2018\n\n\n2018_12\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n12\nEmbajadores\n44630\n23031\n21599\n0.483957\n2018\n\n\n2018_14\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n14\nJusticia\n16578\n8334\n8244\n0.497286\n2018\n\n\n2018_16\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n16\nSol\n7201\n3672\n3529\n0.490071\n2018\n\n\n2018_106\nLatina\n1 de enero de 2018\n28079\nMadrid\n10\n106\nCuatroVientos\n5662\n2870\n2792\n0.493112\n2018\n\n\n2018_194\nVicálvaro\n1 de enero de 2018\n28079\nMadrid\n19\n194\nElCañaveral\n945\n490\n455\n0.481481\n2018\n\n\n2018_212\nBarajas\n1 de enero de 2018\n28079\nMadrid\n21\n212\nAeropuerto\n1794\n922\n872\n0.486065\n2018\n\n\n\n\n\n\n\n… most populated area across all years:\n\nlargest_hood_num = madrid_pop[\"num_people\"].max()\nlargest_hood = madrid_pop[madrid_pop[\"num_people\"] == largest_hood_num]\nlargest_hood\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023_11\nCarabanchel\n1 de enero de 2023\n28079\nMadrid\n11\n11\nCarabanchel\n262339\n122632\n139707\n0.532544\n2023\n\n\n\n\n\n\n\n\nTip\nIf you are interested, more detail about query is available in the pandas documentation. This is another way of slicing Dataframes, but for now we will stay with the loc function.\n\n\n\nCombining queries\nNow, all of these queries can be combined with each other for further flexibility. For example, imagine you want to know the areas that have more than 100K inhabitants and have over 50% of women.\n\nwomen_power = madrid_pop.loc[\n    (madrid_pop[\"num_people_women\"] &gt; 100000) & (madrid_pop[\"ratio_women\"] &gt; 0.5)\n]\nwomen_power\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023_8\nFuencarral-El Pardo\n1 de enero de 2023\n28079\nMadrid\n8\n8\nFuencarral-El Pardo\n248443\n116944\n131499\n0.529292\n2023\n\n\n2023_10\nLatina\n1 de enero de 2023\n28079\nMadrid\n10\n10\nLatina\n241672\n112093\n129579\n0.536177\n2023\n\n\n2023_11\nCarabanchel\n1 de enero de 2023\n28079\nMadrid\n11\n11\nCarabanchel\n262339\n122632\n139707\n0.532544\n2023\n\n\n2023_13\nPuente de Vallecas\n1 de enero de 2023\n28079\nMadrid\n13\n13\nPuente de Vallecas\n241603\n114542\n127061\n0.525908\n2023\n\n\n2023_15\nCiudad Lineal\n1 de enero de 2023\n28079\nMadrid\n15\n15\nCiudad Lineal\n220345\n100759\n119586\n0.542722\n2023\n\n\n2023_16\nHortaleza\n1 de enero de 2023\n28079\nMadrid\n16\n16\nHortaleza\n198391\n94100\n104291\n0.525684\n2023\n\n\n2022_8\nFuencarral-El Pardo\n1 de enero de 2022\n28079\nMadrid\n8\n8\nFuencarral-El Pardo\n246281\n115955\n130326\n0.529176\n2022\n\n\n2022_10\nLatina\n1 de enero de 2022\n28079\nMadrid\n10\n10\nLatina\n237048\n109928\n127120\n0.536263\n2022\n\n\n2022_11\nCarabanchel\n1 de enero de 2022\n28079\nMadrid\n11\n11\nCarabanchel\n255514\n119381\n136133\n0.532781\n2022\n\n\n2022_13\nPuente de Vallecas\n1 de enero de 2022\n28079\nMadrid\n13\n13\nPuente de Vallecas\n235638\n111748\n123890\n0.525764\n2022\n\n\n2022_15\nCiudad Lineal\n1 de enero de 2022\n28079\nMadrid\n15\n15\nCiudad Lineal\n213905\n97357\n116548\n0.544859\n2022\n\n\n2022_16\nHortaleza\n1 de enero de 2022\n28079\nMadrid\n16\n16\nHortaleza\n195017\n92532\n102485\n0.525518\n2022\n\n\n2021_8\nFuencarral-El Pardo\n1 de enero de 2021\n28079\nMadrid\n8\n8\nFuencarral-ElPardo\n247692\n116520\n131172\n0.529577\n2021\n\n\n2021_10\nLatina\n1 de enero de 2021\n28079\nMadrid\n10\n10\nLatina\n240155\n111209\n128946\n0.536928\n2021\n\n\n2021_11\nCarabanchel\n1 de enero de 2021\n28079\nMadrid\n11\n11\nCarabanchel\n258633\n120600\n138033\n0.533702\n2021\n\n\n2021_13\nPuente de Vallecas\n1 de enero de 2021\n28079\nMadrid\n13\n13\nPuentedeVallecas\n239057\n113355\n125702\n0.525824\n2021\n\n\n2021_15\nCiudad Lineal\n1 de enero de 2021\n28079\nMadrid\n15\n15\nCiudadLineal\n216818\n98514\n118304\n0.545637\n2021\n\n\n2021_16\nHortaleza\n1 de enero de 2021\n28079\nMadrid\n16\n16\nHortaleza\n193228\n91585\n101643\n0.526026\n2021\n\n\n2020_8\nFuencarral-El Pardo\n1 de enero de 2020\n28079\nMadrid\n8\n8\nFuencarral-ElPardo\n249973\n117640\n132333\n0.529389\n2020\n\n\n2020_10\nLatina\n1 de enero de 2020\n28079\nMadrid\n10\n10\nLatina\n242139\n112282\n129857\n0.536291\n2020\n\n\n2020_11\nCarabanchel\n1 de enero de 2020\n28079\nMadrid\n11\n11\nCarabanchel\n260196\n121317\n138879\n0.533748\n2020\n\n\n2020_13\nPuente de Vallecas\n1 de enero de 2020\n28079\nMadrid\n13\n13\nPuentedeVallecas\n240867\n114235\n126632\n0.525734\n2020\n\n\n2020_15\nCiudad Lineal\n1 de enero de 2020\n28079\nMadrid\n15\n15\nCiudadLineal\n219867\n99966\n119901\n0.545334\n2020\n\n\n2020_16\nHortaleza\n1 de enero de 2020\n28079\nMadrid\n16\n16\nHortaleza\n193264\n91659\n101605\n0.525732\n2020\n\n\n2019_8\nFuencarral-El Pardo\n1 de enero de 2019\n28079\nMadrid\n8\n8\nFuencarral-ElPardo\n246021\n115797\n130224\n0.529321\n2019\n\n\n2019_10\nLatina\n1 de enero de 2019\n28079\nMadrid\n10\n10\nLatina\n238154\n110401\n127753\n0.53643\n2019\n\n\n2019_11\nCarabanchel\n1 de enero de 2019\n28079\nMadrid\n11\n11\nCarabanchel\n253040\n117802\n135238\n0.534453\n2019\n\n\n2019_13\nPuente de Vallecas\n1 de enero de 2019\n28079\nMadrid\n13\n13\nPuentedeVallecas\n234770\n111183\n123587\n0.526417\n2019\n\n\n2019_15\nCiudad Lineal\n1 de enero de 2019\n28079\nMadrid\n15\n15\nCiudadLineal\n216270\n98370\n117900\n0.545152\n2019\n\n\n2018_8\nFuencarral-El Pardo\n1 de enero de 2018\n28079\nMadrid\n8\n8\nFuencarral-ElPardo\n242928\n114433\n128495\n0.528943\n2018\n\n\n2018_10\nLatina\n1 de enero de 2018\n28079\nMadrid\n10\n10\nLatina\n235785\n109392\n126393\n0.536052\n2018\n\n\n2018_11\nCarabanchel\n1 de enero de 2018\n28079\nMadrid\n11\n11\nCarabanchel\n248220\n115525\n132695\n0.534586\n2018\n\n\n2018_13\nPuente de Vallecas\n1 de enero de 2018\n28079\nMadrid\n13\n13\nPuentedeVallecas\n230488\n109044\n121444\n0.526899\n2018\n\n\n2018_15\nCiudad Lineal\n1 de enero de 2018\n28079\nMadrid\n15\n15\nCiudadLineal\n214463\n97301\n117162\n0.546304\n2018\n\n\n\n\n\n\n\n\n\nSorting\nAmong the many operations DataFrame objects support, one of the most useful ones is to sort a table based on a given column. For example, imagine you want to sort the table by the influenza cases:\n\nmadrid_sorted = madrid_pop.sort_values(\"ratio_women\", ascending=False)\nmadrid_sorted\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2018_158\nCiudad Lineal\n1 de enero de 2018\n28079\nMadrid\n15\n158\nAtalaya\n1575\n656\n919\n0.583492\n2018\n\n\n2019_158\nCiudad Lineal\n1 de enero de 2019\n28079\nMadrid\n15\n158\nAtalaya\n1568\n654\n914\n0.582908\n2019\n\n\n2023_158\nCiudad Lineal\n1 de enero de 2023\n28079\nMadrid\n15\n158\nAtalaya\n1622\n691\n931\n0.573983\n2023\n\n\n2020_158\nCiudad Lineal\n1 de enero de 2020\n28079\nMadrid\n15\n158\nAtalaya\n1555\n667\n888\n0.571061\n2020\n\n\n2020_45\nSalamanca\n1 de enero de 2020\n28079\nMadrid\n4\n45\nLista\n21211\n9111\n12100\n0.570459\n2020\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2020_12\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n12\nEmbajadores\n47048\n24497\n22551\n0.479319\n2020\n\n\n2022_16\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n16\nSol\n8117\n4232\n3885\n0.478625\n2022\n\n\n2023_12\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n12\nEmbajadores\n46204\n24094\n22110\n0.47853\n2023\n\n\n2022_12\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n12\nEmbajadores\n46444\n24271\n22173\n0.477414\n2022\n\n\n2021_12\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n12\nEmbajadores\n47238\n24767\n22471\n0.475698\n2021\n\n\n\n\n912 rows × 12 columns\n\n\n\nGiven the rates differ, it may be better to sort by neighbourhood and then by year.\n\nsort_ls = [\"code_neighbourhood\", \"year\"]\nmadrid_sorted = madrid_pop.sort_values(sort_ls, ascending=True)\nmadrid_sorted\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2018_1\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n1\nCentro\n132352\n66320\n66032\n0.498912\n2018\n\n\n2019_1\nCentro\n1 de enero de 2019\n28079\nMadrid\n1\n1\nCentro\n134881\n67829\n67052\n0.49712\n2019\n\n\n2020_1\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n1\nCentro\n140473\n71127\n69346\n0.493661\n2020\n\n\n2021_1\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n1\nCentro\n141236\n71881\n69355\n0.491058\n2021\n\n\n2022_1\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n1\nCentro\n139682\n70986\n68696\n0.491803\n2022\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2019_97\nMoncloa-Aravaca\n1 de enero de 2019\n28079\nMadrid\n9\n97\nAravaca\n26823\n12619\n14204\n0.529546\n2019\n\n\n2020_97\nMoncloa-Aravaca\n1 de enero de 2020\n28079\nMadrid\n9\n97\nAravaca\n27503\n12899\n14604\n0.530997\n2020\n\n\n2021_97\nMoncloa-Aravaca\n1 de enero de 2021\n28079\nMadrid\n9\n97\nAravaca\n27568\n12896\n14672\n0.532211\n2021\n\n\n2022_97\nMoncloa-Aravaca\n1 de enero de 2022\n28079\nMadrid\n9\n97\nAravaca\n27323\n12779\n14544\n0.532299\n2022\n\n\n2023_97\nMoncloa-Aravaca\n1 de enero de 2023\n28079\nMadrid\n9\n97\nAravaca\n27445\n12836\n14609\n0.532301\n2023\n\n\n\n\n912 rows × 12 columns\n\n\n\nThis allows you to do so-called hierarchical sorting: sort first based on one column, if equal, then based on another column, etc."
  },
  {
    "objectID": "chapter_1/pandas.html#visual-exploration",
    "href": "chapter_1/pandas.html#visual-exploration",
    "title": "Pandas and Data",
    "section": "Visual Exploration",
    "text": "Visual Exploration\nThe next step to continue exploring a dataset is to get a feel for what it looks like, visually. You have already learnt how to unconver and inspect specific parts of the data, to check for particular cases you might be interested in. Now, you will see how to plot the data to get a sense of the overall distribution of values. For that, you can use the plotting capabilities of pandas.\n\nHistograms\nOne of the most common graphical devices to display the distribution of values in a variable is a histogram. Values are assigned into groups of equal intervals, and the groups are plotted as bars rising as high as the number of values into the group.\nA histogram is easily created with the following command. In this case, let us have a look at the shape of the overall numbers of people:\n\n_ = madrid_pop[\"num_people\"].plot.hist()\n\n\n\n\n\nAssigning to _\npandas returns an object with the drawing from its plotting methods. Since you are in Jupyter environment, and you don’t need to work further with that object; you can assign it to _, a convention for an unused variable.\n\nHowever, the default pandas plots can be a bit dull. A better option is to use another package, called seaborn.\n\nimport seaborn as sns\n\n\nWhy sns?\nseaborn is, by convention, imported as sns. That came as a joke after Samuel Normal Seaborn, a fictional character The West Wing show.\n\nThe same plot using seaborn has a more pleasant default style and more customisability.\n\nsns.distplot(madrid_pop[\"num_people\"])\n\n/var/folders/bs/sq3p18cs4dn1ws4kpmfyk_3c0000gn/T/ipykernel_35668/1382748844.py:1: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(madrid_pop[\"num_people\"])\n\n\n&lt;Axes: xlabel='num_people', ylabel='Density'&gt;\n\n\n\n\n\nNote you are using sns instead of pd, as the function belongs to seaborn instead of pandas.\nYou can quickly see most of the areas have seen somewhere between 0 and 50K people; and very few have more than 200K. However, remember that in this case we are visualizing all years together, which could lead to missinterpretations.\n\n\nLine and bar plots\nAnother very common way of visually displaying a variable is with a line or a bar chart. For example, if you want to generate a line plot of the (sorted) total population per year:\n\ntotal_people_per_year = madrid_pop.groupby(\"year\")[\"num_people\"].sum()\ntotal_people_per_year.plot()\n\n&lt;Axes: xlabel='year'&gt;\n\n\n\n\n\nWhat is evident is the impact of COVID on the total population. But understanding that the data is reported on the 1st of January of each year is crucial to understand why you see the offset on the dates.\nFor a bar plot all you need to do is to change from plot to plot.bar:\n\ntotal_people_per_year.plot.bar()\n\n&lt;Axes: xlabel='year'&gt;\n\n\n\n\n\nLet’s try to plot the ratio_women per neighbourhood, to see if we spot anything in particular.\n\nsns.lineplot(\n    x=\"year\",\n    y=\"ratio_women\",\n    hue=\"neighbourhood\",\n    data=madrid_pop.sort_values(\"year\", ascending=True),\n    legend=False,\n)\n\n&lt;Axes: xlabel='year', ylabel='ratio_women'&gt;\n\n\n\n\n\nWe can see some outliers, but the reality is that the data is hard to read so we probably would need some further analysis and visual considerations to efficiently communicate any possible trend.\n\nOne line or multiple lines?\nYou may have noticed that in some cases, the code is on a single line, but longer code is split into multiple lines. Python requires you to follow the indentation rules, but apart from that, there are not a lot of other limits."
  },
  {
    "objectID": "chapter_1/pandas.html#tidy-data",
    "href": "chapter_1/pandas.html#tidy-data",
    "title": "Pandas and Data",
    "section": "Tidy data",
    "text": "Tidy data\n\nClean vs. Tidy\nThis section is a bit more advanced and hence considered optional. Feel free to skip it, move to the next, and return later when you feel more confident.\n\nOnce you can read your data in, explore specific cases, and have a first visual approach to the entire set, the next step can be preparing it for more sophisticated analysis. Maybe you are thinking of modeling it through regression, or on creating subgroups in the dataset with particular characteristics, or maybe you simply need to present summary measures that relate to a slightly different arrangement of the data than you have been presented with.\nFor all these cases, you first need what statistician, and general R wizard, Hadley Wickham calls “tidy data”. The general idea to “tidy” your data is to convert them from whatever structure they were handed in to you into one that allows convenient and standardized manipulation, and that supports directly inputting the data into what he calls “tidy” analysis tools. But, at a more practical level, what is exactly “tidy data”? In Wickham’s own words:\n\nTidy data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types.\n\nHe then goes on to list the three fundamental characteristics of “tidy data”:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\nIf you are further interested in the concept of “tidy data”, I recommend you check out the original paper (open access) and the public repository associated with it."
  },
  {
    "objectID": "chapter_1/pandas.html#grouping-transforming-aggregating",
    "href": "chapter_1/pandas.html#grouping-transforming-aggregating",
    "title": "Pandas and Data",
    "section": "Grouping, transforming, aggregating",
    "text": "Grouping, transforming, aggregating\nOne of the advantage of tidy datasets is they allow to perform advanced transformations in a more direct way. One of the most common ones is what is called “group-by” operations. Originated in the world of databases, these operations allow you to group observations in a table by one of its labels, index, or category, and apply operations on the data group by group.\nFor example, given our dataframe, you might want to compute the total sum of the population by each district. This task can be split into two different ones:\n\nGroup the table in each of the different districts.\nCompute the sum of num_people for each of them.\n\nTo do this in pandas, meet one of its workhorses, and also one of the reasons why the library has become so popular: the groupby operator.\n\nmad_grouped = madrid_pop.groupby(\"year\")\nmad_grouped\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x145f03f10&gt;\n\n\nThe object mad_grouped still hasn’t computed anything. It is only a convenient way of specifying the grouping. But this allows us then to perform a multitude of operations on it. For our example, the sum is calculated as follows:\n\nmad_grouped.sum(numeric_only=True)\n\n\n\n\n\n\n\n\ncode_municipality\nnum_people\nnum_people_men\nnum_people_women\nratio_women\n\n\nyear\n\n\n\n\n\n\n\n\n\n2018\n4268008\n6443648\n3000680\n3442968\n81.100391\n\n\n2019\n4268008\n6532252\n3042356\n3489896\n81.108315\n\n\n2020\n4268008\n6669460\n3109464\n3559996\n81.047788\n\n\n2021\n4268008\n6624620\n3090314\n3534306\n81.013452\n\n\n2022\n4268008\n6573324\n3069648\n3503676\n80.939052\n\n\n2023\n4268008\n6679862\n3119732\n3560130\n80.94286\n\n\n\n\n\n\n\nSimilarly, you can also obtain a summary of each group:\n\nmad_grouped.describe()\n\n\n\n\n\n\n\n\ncode_municipality\nnum_people\n...\nnum_people_women\nratio_women\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ncount\nmean\n...\n75%\nmax\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nyear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2018\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n42392.421053\n...\n21771.75\n132695.0\n152.0\n0.533555\n0.01839\n0.481481\n0.521591\n0.534767\n0.54588\n0.583492\n\n\n2019\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n42975.342105\n...\n21987.0\n135238.0\n152.0\n0.533607\n0.018198\n0.483197\n0.522356\n0.534355\n0.54565\n0.582908\n\n\n2020\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n43878.026316\n...\n22623.75\n138879.0\n152.0\n0.533209\n0.017944\n0.479319\n0.522586\n0.533905\n0.545599\n0.571061\n\n\n2021\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n43583.026316\n...\n22530.75\n138033.0\n152.0\n0.532983\n0.017984\n0.475698\n0.522263\n0.533724\n0.545682\n0.568987\n\n\n2022\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n43245.552632\n...\n22296.5\n136133.0\n152.0\n0.532494\n0.017716\n0.477414\n0.521691\n0.533305\n0.54492\n0.56714\n\n\n2023\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n43946.460526\n...\n22742.75\n139707.0\n152.0\n0.532519\n0.017486\n0.47853\n0.522637\n0.533363\n0.544488\n0.573983\n\n\n\n\n6 rows × 40 columns\n\n\n\nYou will not get into it today as it goes beyond the basics this chapter wants to cover, but keep in mind that groupby allows you to not only call generic functions (like sum or describe), but also your own functions. This opens the door for virtually any kind of transformation and aggregation possible.\n\nAdditional reading\n\nA good introduction to data manipulation in Python is Wes McKinney’s “Python for Data Analysis” [@mckinney2012python].\nTo explore further some of the visualization capabilities in at your fingertips, the Python library seaborn is an excellent choice. Its online tutorial is a fantastic place to start.\nA good extension is Hadley Wickham’s “Tidy data” paper [@wickham2014tidy], which presents a very popular way of organising tabular data for efficient manipulation."
  },
  {
    "objectID": "chapter_1/pandas.html#acknowledgements",
    "href": "chapter_1/pandas.html#acknowledgements",
    "title": "Pandas and Data",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis section is derived from SDS by @martinfleis which in turn is based on A Course on Geographic Data Science by @darribas_gds_course, both licensed under CC-BY-SA 4.0. The text was slightly adapted, mostly to accommodate a different dataset used."
  },
  {
    "objectID": "chapter_2/qgis.html",
    "href": "chapter_2/qgis.html",
    "title": "Intro to GIS",
    "section": "",
    "text": "Vector and raster data are two fundamental types of spatial data used in Geographic Information Systems (GIS). Vector data represents geographical features as distinct shapes using points, lines, and polygons, each of which can carry detailed attribute information, making it ideal for precise mapping applications like boundaries, roads, and infrastructure networks. It excels in accuracy and detail for discrete data. In contrast, raster data consists of a grid of pixels, each holding a value, effectively representing continuous data. This format is well-suited for environmental analysis, land cover studies, and satellite imagery, as it captures variations over a wide area, such as elevation, temperature, or vegetation. While vector data is preferred for its precision in depicting specific geographic entities and managing related data, raster data is favored for its ability to represent complex, continuous phenomena over large spatial scales.\n\n\n\n\nVector data types are primarily categorized into Points, Lines, and Polygons, each with distinct characteristics and uses. Additionally, there are multi-geometry versions of these types, like Multi-Points, Multi-Lines, etc., which offer more complex representations.\n\nPoints: In vector GIS, Points are used to represent discrete locations on the earth’s surface. Each point is defined by a pair of coordinates (e.g. latitude and longitude) and can represent features like the location of a city, a well, or a landmark.\nLines: Lines are sequences of points connected by straight segments. They are used to represent linear features such as roads, rivers, or utility lines. Lines are characterized by length and direction and can also include attributes like road type or river name.\nPolygons: Polygons are closed shapes formed by a series of connected line segments. They are used to represent areas such as city boundaries, land parcels, or lakes. Polygons have attributes such as area and perimeter and can include additional information like population density or land use type.\nMulti-Geometry Versions:\n\nMulti-Points: This type represents a collection of points as a single entity. For instance, a Multi-Point could represent a chain of islands or a group of dispersed historical sites.\nMulti-Lines (or Multi-Polylines): These are collections of lines that are treated as a single feature. An example might be the total network of streets in a city or various segments of a long-distance hiking trail.\nMulti-Polygons: This type includes several polygons that are associated but not physically connected, like the different parcels of a dispersed national park or the territories of a country spread over several islands.\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtension\nFile Type\nDescription\n\n\n\n\nEsri Shapefile\n.SHP, .DBF, .SHX\nShapefile is a prevalent geospatial file type in GIS software, requiring three mandatory files: SHP (feature geometry), SHX (shape index position), and DBF (attribute data). Optional files like PRJ (coordinate reference system) are also common.\n\n\nGeoPackage\n.GPKG\nGeoPackage, a portable and compact format, is ideal for transferring geospatial information. Based on SQLite, it supports vector data, tile matrix sets, and extensions. It’s efficient for storing multiple data layers and a popular alternative to shapefiles for handling larger datasets.\n\n\nGeographic JavaScript Object Notation\n.GEOJSON, .JSON\nGeoJSON encodes geographic data structures using JSON. Common in online mapping applications, it represents geographic features like points, lines, and polygons in an open-standard format.\n\n\nGeography Markup Language\n.GML\nGML, an XML extension, stores geographic entities as text, including properties, geometry, and spatial reference. It’s similar to GeoJSON but more verbose, offering a human-readable and machine-readable format.\n\n\nGoogle Keyhole Markup Language\n.KML, .KMZ\nKML, developed for Google Earth, is an XML-based GIS format. KMZ, a compressed version, is the standard Google Earth format. KML/KMZ files include geospatial coordinates (WGS84) and altitude (meters). Adopted as an international standard by the Open Geospatial Consortium in 2008.\n\n\nGPS eXchange Format\n.GPX\nGPX, an XML schema, describes waypoints, tracks, and routes from GPS receivers. It’s an exchange format, facilitating data transfer across programs. GPX files store coordinates, with optional properties like time and elevation.\n\n\nOpenStreetMap\n.OSM\nOSM files, OpenStreetMap’s native format, come from the world’s largest crowdsourced GIS project. They include vector features and are XML-based. The PBF Format is a smaller, more efficient alternative. OSM files are compatible with QGIS for data interoperability.\n\n\n\nReference: https://gisgeography.com/gis-formats/\n\n\n\n\nRasters contain a grid of pixels, where the value used for each grid cell can represent a characteristic such as temperature or landcover. Geographic rasters typically include Geospatial information necessary to correctly project and locate the information in GIS software.\n\n\n\n\n\n\n\n\n\n\nExtension\nFile Type\nDescription\n\n\n\n\nAmerican Standard Code for Information Interchange ASCII Grid\n.ASC\nASCII Grid files (ASC) store raster data as text in a grid format. Each cell in the grid is represented by a numeric value (which can be a float), corresponding to the geographic attribute being mapped, such as elevation. ASCII Grid files include a header specifying metadata like cell size, number of rows and columns, and coordinates of the lower left corner. The format is simple and can be generated or edited with a text editor, making it versatile for GIS data storage and exchange. Commonly used as an interchange format, it can be space, comma, or tab-delimited.\n\n\nGeoTIFF\n.TIF, .TIFF\nGeoTIFF is a widely used file format in GIS and remote sensing applications, known for its ability to embed georeferencing information within a standard TIFF (Tagged Image File Format). It stores raster images and allows embedding of metadata such as map projections, coordinate systems, ellipsoids, and datums. This embedded geospatial data enables the precise positioning of the image in the correct geographic space. GeoTIFFs are commonly used for storing satellite imagery, aerial photography, and GIS data. They can also be accompanied by additional files, like world files (TFW), for georeferencing.\n\n\n\nReference: https://gisgeography.com/gis-formats/\n\n\n\n\n\nGeographic data is intrinsically tied to the earth’s surface and is represented using coordinates within defined Coordinate Reference Systems (CRS). Broadly, these systems fall into two categories: Geographic Coordinate Systems and Projected Coordinate Systems.\nGeographic CRSs use a three-dimensional spherical surface to define locations on the earth. They are based on latitude and longitude measurements and are most commonly referenced to the WGS84 datum. While they accurately represent the earth’s curved surface and preserve the true shape and angular relationships of features, distances and areas may not be as accurate, especially over larger areas.\nProjected CRSs translate the earth’s three-dimensional surface onto a two-dimensional plane, making it easier to analyze and visualize spatial data. They use a mathematical projection to flatten the curved surface of the earth, which can lead to distortions in shape, area, distance, or direction. Different types of projections are designed to minimize these distortions for specific regions or purposes. For example, some are optimized for preserving area (equal-area projections), while others preserve distances or shapes.\n\n\n\n\n\n\nTip\n\n\n\nA useful reference website for CRS is EPSG.io: Coordinate Systems Worldwide\n\n\n\n\nGeographic Coordinate Reference Systems (CRS) use a three-dimensional spherical or ellipsoidal surface to represent locations on the Earth’s curved surface. Geographic CRSs are based on latitude and longitude coordinates and are useful for global or regional mapping.\n\nUnits: Degrees\nCommon Examples:\n\nWGS 84 (World Geodetic System 1984) EPSG:4326: A widely used CRS for global mapping and GPS navigation.\nNAD83 (North American Datum 1983) EPSG:4269: A CRS commonly used in North America.\n\n\n\n\n\nProjected Coordinate Reference Systems (CRS) use a two-dimensional Cartesian coordinate system to represent locations on a flat surface. These CRSs are created by “projecting” the curved Earth onto a flat surface, which introduces distortions, which inevitably requires tradeoffs in distances, areas, or shapes, with different projection systems involving different trade-offs. Projected CRSs are commonly used for local or small-scale mapping appropriate to specific countries or regions of the globe, where locally accurate measurements and comparisons are required.\n\nUnits: Meters (most commonly)\nCommon Examples:\n\nUTM Zone 30N (ETRS89) EPSG:3035: This CRS is commonly used in Europe (around latitude 30° North).\nBritish National Grid (BNG) EPSG:27700: This CRS is commonly used in the United Kingdom.\nState Plane Coordinate System (SPCS): This CRS is widely used in the United States for local or regional mapping.\n\n\n\n\n\nhttps://gisgeography.com/map-projection-types/\n\n\n\n\n\nGeneral layout\nThe toolbox\nExtensions / plugins\nWeb tiles\n\n\n\n\n\n\n\nNote\n\n\n\n💻 Install Quick Map Services (QMS) → load OSM standard\n\n\n\n\n\n\n\n\nTip\n\n\n\n💻 Exercise → Load/Import data from DATOS MADRID Neighbourhoods (.shp); Population (.csv) → Explore data attributes : filter based on date (fecha)\n\n\nBarrios municipales de Madrid\nPoblación por distrito y barrio a 1 de enero\n\n\n\nDiscovering sources for GIS (Geographic Information System) data is essential for spatial analysis and mapping. There are various official and non-official sources, each offering different types of data suited to specific needs and applications.\n\n\n\nGovernment Agencies: Agencies such as the US Geological Survey (USGS) and the Environmental Protection Agency (EPA) provide official and reliable GIS data. However, these datasets may reflect governmental priorities and perspectives.\nInternational Organizations: Organizations like the United Nations (UN) and the World Bank offer global GIS datasets, ideal for international research. They may not always capture local nuances due to their broad scope.\nLocal Authorities: City and regional councils supply local GIS data, which is often tailored to specific local government projects and priorities.\n\n\n\n\n\nPrivate Sector: Companies such as ArcGIS Living Atlas, Esri, and CARTO Datawarehouse offer comprehensive, commercially-oriented GIS datasets.\nSpecialized GIS Data Portals: Portals like the Harvard Geospatial Library provide access to specialized datasets across various fields.\nOpen Data Platforms: Platforms such as OpenStreetMap offer freely accessible GIS data, contributed by a community of users. These datasets might require additional validation due to their open-source nature.\nSpace Agencies: Agencies like NASA and the European Space Agency (ESA) provide official satellite imagery, particularly useful for environmental and spatial studies.\nAcademic Institutions: Universities often share GIS data derived from research projects. While not officially sanctioned, this data can be valuable but might need adaptation for specific uses.\nSocial Media and Crowdsourced Data: Platforms like Twitter and Facebook can be sources of GIS data, reflecting public behavior and trends. This data is not official and requires careful adaptation to account for inherent biases.\nWeb Scraping: Involves the automated collection of data from websites. This method is not an official source and the quality of data can vary. It’s useful for obtaining unique, real-time data. Web scraping demands significant technical skills for data extraction and processing. Legal and ethical considerations are important, as scraping can sometimes violate terms of service or data privacy laws.\n\n\n\n\nPurposely collected data is specifically gathered to meet the objectives of a defined research question or project goal. This type of data is obtained through structured methods such as surveys, experiments, or systematic observations.\n\n\n\nTailored Collection: The data is methodically gathered to align closely with the specific needs and aims of a particular study or project.\nControl Over Variables: Efforts are made to control or monitor variables, enhancing the relevance and accuracy of the data.\nHighly Relevant: The data’s targeted collection ensures its direct applicability and relevance to the specific research goals.\n\n\n\n\n\nSelection Bias: There’s a possibility of preferentially selecting data that confirms a pre-existing hypothesis, while disregarding contradicting data.\nResearcher Bias: The inclinations or hypotheses of the researcher may inadvertently influence the data collection methods and interpretation.\nSampling Bias: The methodology for selecting samples could impact the representativeness of the data, potentially leading to skewed outcomes.\n\n\n\n\n\nAdapted data is information that was initially collected for purposes other than the current research project but is repurposed or reinterpreted to suit new research needs. This category includes data from existing public databases, prior studies, or information obtained through web scraping.\n\n\n\nBroadly Collected: Originally gathered for various general or distinct objectives, often from pre-existing sources.\nVaried Sources: Derived from multiple sources, each with unique data collection methods.\nVersatility: Provides the opportunity to apply pre-existing data to new or unrelated research questions.\n\n\n\n\n\nBias of Original Purpose: The initial intent behind the data collection may affect the type and nature of the data, potentially reducing its applicability to new research questions.\nRisk of Contextual Misinterpretation: Lacking the original context, there is a danger of misinterpreting the data when adapting it for different uses.\nConcerns about Data Quality and Completeness: The original data may not fulfill the specific quality or completeness standards required for the new research purpose.\n\n\n\n\n\nWhen dealing with data, licenses refer to legal agreements or permissions that determine how the data can be used, distributed, or modified. Different datasets may have different licensing requirements, so it’s important to understand and comply with the specific license terms associated with the data you are using.\nExamples:\n\nCreative Commons (CC) licenses: Standardized licenses that allow creators to choose permissions for their work. Used for creative works like images, music, videos, and written content.\nOpen Database License (ODbL): Specifically for databases. Allows users to freely use, share, and modify the database. Modifications and derivative works must be under the same license. Used for open data projects and community-driven data initiatives.\nGNU General Public License (GPL): Copyleft license for open-source software. Allows modification and distribution. Modifications and derivative works must be under the same license. Used for promoting open-source principles and collaboration.\n\n\n\n\nAttribution or citation refers to giving proper credit to the original source or creator of the data. Just like when citing sources in academic papers, it’s important to acknowledge the origin of the data you are using. This helps provide transparency, accountability, and recognition to the creators of the dataset.\n\n\n\nPrivacy considerations are important when working with data, especially if the data contains personally identifiable information or sensitive information. It’s very important to handle and store data in a way that respects privacy laws and regulations. This may involve anonymizing or de-identifying data, implementing security measures, and obtaining consent from individuals whose data is being used.\n\n\n\n\n\n\nNote\n\n\n\nPlease avoid using any and all data containing private or sensitive data unless you first review this and seek approval from faculty. Handling these forms of data comes with numerous steps and safeguards and will generally be strongly discouraged.\n\n\n\n\n\n\nOften times, data from several datasources (or even the same one) comes with different projection systems. Other times it’s a matter of software support or visual preference, but what’s really important is to aim for consolidating all your data layers into the same projection system. This will help with accurate calculations and ‘real’ overlap of layers.\n\n\n\n\n\n\nNote\n\n\n\n💻 Using Madrid’s neighbourhoods (above) and the toolbox → Search for ’reproject layer’ → Reproject layer to EPSG: 4326 (and others) and save as a different files (or try Geopackage layers) → Restart a new project and load the new layer → See the layer properties of each and how they overlap or change shapes.\n\n\n\n\n\nAttributes in GIS layers provide additional information about the spatial features in the data. They include text, numbers, dates, or images. Attributes help add context to the geographic elements, enabling better analysis and visualization. They can be used to filter, classify, and symbolize features based on their characteristics. Attributes play a crucial role in data analysis, allowing us to gain insights, make comparisons, and create meaningful visual representations.\n\n\n\n\n\n\nNote\n\n\n\nInputting and editing data\n💻 → calculate the area of all polygons and save in new field\n\n\n\n\n\n\n\n\nNote\n\n\n\nAttribute based joins 💻 → Identify the common attributes between the two layers (population and polygons) → Create a Join based on attributes → Calculate the density of population and the density of men and women separately. → delete unnecessary attributes → Save and export (selected features only)\n\n\n\n\n\n\n\n\nNote\n\n\n\nStyling and visualising data 💻 → Create a Cloropeth map with the new density measure\n\n\n\n\n\n\n\n\n⚙\nCONNECTION SETTINGS\n\n\n\n\nIP Address\n(will be provided during class)\n\n\nDatabase names\nnfi / urban_analytics\n\n\nPort\n5432\n\n\nUsername\nscholar\n\n\nPassword\n(will be provided during class)\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote that the connections to the Databases are restricted by the source IP address.\n\n\n\n\n\nPoint Maps: Represent specific locations.\nHeat Maps: Visualise the density or intensity of a phenomenon.  \nFlow Maps: Represent flows or strengths of connections. \nCartograms: Distort the geographic shape of regions to represent a specific variable. \nCloropleth: Use color to represent a particular attribute or variable. \n\n\n\n\n\nThe Modifiable Areal Unit Problem (MAUP) is a common issue in geography and maps. It refers to the fact that the results of statistical analysis or visual representation can be influenced by the choice of spatial unit or scale used to aggregate data. The MAUP arises because geographic phenomena can be measured and analysed at different levels of spatial granularity, such as census tracts, counties, or regions. When data is aggregated into different spatial units, it can lead to different patterns, relationships, and interpretations.\nhttps://gisgeography.com/maup-modifiable-areal-unit-problem/\nCrime Rates: Imagine you have crime data recorded at the neighborhoud level. If you aggregate the data into larger units, such as police precincts, the crime rates may appear different due to the varying sizes and characteristics of neighborhouds within each precinct. This can result in different spatial patterns and potentially lead to incorrect conclusions about crime rates in different areas.\nhttps://www.youtube.com/watch?v=7NTw1v7DuAA&t=115s\nThese examples demonstrate how the choice of spatial unit can significantly impact the analysis and interpretation of geographic data, highlighting the need to carefully consider the scale and boundaries of spatial units when conducting spatial analysis or creating maps."
  },
  {
    "objectID": "chapter_2/qgis.html#file-types",
    "href": "chapter_2/qgis.html#file-types",
    "title": "Intro to GIS",
    "section": "",
    "text": "Vector and raster data are two fundamental types of spatial data used in Geographic Information Systems (GIS). Vector data represents geographical features as distinct shapes using points, lines, and polygons, each of which can carry detailed attribute information, making it ideal for precise mapping applications like boundaries, roads, and infrastructure networks. It excels in accuracy and detail for discrete data. In contrast, raster data consists of a grid of pixels, each holding a value, effectively representing continuous data. This format is well-suited for environmental analysis, land cover studies, and satellite imagery, as it captures variations over a wide area, such as elevation, temperature, or vegetation. While vector data is preferred for its precision in depicting specific geographic entities and managing related data, raster data is favored for its ability to represent complex, continuous phenomena over large spatial scales.\n\n\n\n\nVector data types are primarily categorized into Points, Lines, and Polygons, each with distinct characteristics and uses. Additionally, there are multi-geometry versions of these types, like Multi-Points, Multi-Lines, etc., which offer more complex representations.\n\nPoints: In vector GIS, Points are used to represent discrete locations on the earth’s surface. Each point is defined by a pair of coordinates (e.g. latitude and longitude) and can represent features like the location of a city, a well, or a landmark.\nLines: Lines are sequences of points connected by straight segments. They are used to represent linear features such as roads, rivers, or utility lines. Lines are characterized by length and direction and can also include attributes like road type or river name.\nPolygons: Polygons are closed shapes formed by a series of connected line segments. They are used to represent areas such as city boundaries, land parcels, or lakes. Polygons have attributes such as area and perimeter and can include additional information like population density or land use type.\nMulti-Geometry Versions:\n\nMulti-Points: This type represents a collection of points as a single entity. For instance, a Multi-Point could represent a chain of islands or a group of dispersed historical sites.\nMulti-Lines (or Multi-Polylines): These are collections of lines that are treated as a single feature. An example might be the total network of streets in a city or various segments of a long-distance hiking trail.\nMulti-Polygons: This type includes several polygons that are associated but not physically connected, like the different parcels of a dispersed national park or the territories of a country spread over several islands.\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtension\nFile Type\nDescription\n\n\n\n\nEsri Shapefile\n.SHP, .DBF, .SHX\nShapefile is a prevalent geospatial file type in GIS software, requiring three mandatory files: SHP (feature geometry), SHX (shape index position), and DBF (attribute data). Optional files like PRJ (coordinate reference system) are also common.\n\n\nGeoPackage\n.GPKG\nGeoPackage, a portable and compact format, is ideal for transferring geospatial information. Based on SQLite, it supports vector data, tile matrix sets, and extensions. It’s efficient for storing multiple data layers and a popular alternative to shapefiles for handling larger datasets.\n\n\nGeographic JavaScript Object Notation\n.GEOJSON, .JSON\nGeoJSON encodes geographic data structures using JSON. Common in online mapping applications, it represents geographic features like points, lines, and polygons in an open-standard format.\n\n\nGeography Markup Language\n.GML\nGML, an XML extension, stores geographic entities as text, including properties, geometry, and spatial reference. It’s similar to GeoJSON but more verbose, offering a human-readable and machine-readable format.\n\n\nGoogle Keyhole Markup Language\n.KML, .KMZ\nKML, developed for Google Earth, is an XML-based GIS format. KMZ, a compressed version, is the standard Google Earth format. KML/KMZ files include geospatial coordinates (WGS84) and altitude (meters). Adopted as an international standard by the Open Geospatial Consortium in 2008.\n\n\nGPS eXchange Format\n.GPX\nGPX, an XML schema, describes waypoints, tracks, and routes from GPS receivers. It’s an exchange format, facilitating data transfer across programs. GPX files store coordinates, with optional properties like time and elevation.\n\n\nOpenStreetMap\n.OSM\nOSM files, OpenStreetMap’s native format, come from the world’s largest crowdsourced GIS project. They include vector features and are XML-based. The PBF Format is a smaller, more efficient alternative. OSM files are compatible with QGIS for data interoperability.\n\n\n\nReference: https://gisgeography.com/gis-formats/\n\n\n\n\nRasters contain a grid of pixels, where the value used for each grid cell can represent a characteristic such as temperature or landcover. Geographic rasters typically include Geospatial information necessary to correctly project and locate the information in GIS software.\n\n\n\n\n\n\n\n\n\n\nExtension\nFile Type\nDescription\n\n\n\n\nAmerican Standard Code for Information Interchange ASCII Grid\n.ASC\nASCII Grid files (ASC) store raster data as text in a grid format. Each cell in the grid is represented by a numeric value (which can be a float), corresponding to the geographic attribute being mapped, such as elevation. ASCII Grid files include a header specifying metadata like cell size, number of rows and columns, and coordinates of the lower left corner. The format is simple and can be generated or edited with a text editor, making it versatile for GIS data storage and exchange. Commonly used as an interchange format, it can be space, comma, or tab-delimited.\n\n\nGeoTIFF\n.TIF, .TIFF\nGeoTIFF is a widely used file format in GIS and remote sensing applications, known for its ability to embed georeferencing information within a standard TIFF (Tagged Image File Format). It stores raster images and allows embedding of metadata such as map projections, coordinate systems, ellipsoids, and datums. This embedded geospatial data enables the precise positioning of the image in the correct geographic space. GeoTIFFs are commonly used for storing satellite imagery, aerial photography, and GIS data. They can also be accompanied by additional files, like world files (TFW), for georeferencing.\n\n\n\nReference: https://gisgeography.com/gis-formats/"
  },
  {
    "objectID": "chapter_2/qgis.html#coordinate-reference-systems",
    "href": "chapter_2/qgis.html#coordinate-reference-systems",
    "title": "Intro to GIS",
    "section": "",
    "text": "Geographic data is intrinsically tied to the earth’s surface and is represented using coordinates within defined Coordinate Reference Systems (CRS). Broadly, these systems fall into two categories: Geographic Coordinate Systems and Projected Coordinate Systems.\nGeographic CRSs use a three-dimensional spherical surface to define locations on the earth. They are based on latitude and longitude measurements and are most commonly referenced to the WGS84 datum. While they accurately represent the earth’s curved surface and preserve the true shape and angular relationships of features, distances and areas may not be as accurate, especially over larger areas.\nProjected CRSs translate the earth’s three-dimensional surface onto a two-dimensional plane, making it easier to analyze and visualize spatial data. They use a mathematical projection to flatten the curved surface of the earth, which can lead to distortions in shape, area, distance, or direction. Different types of projections are designed to minimize these distortions for specific regions or purposes. For example, some are optimized for preserving area (equal-area projections), while others preserve distances or shapes.\n\n\n\n\n\n\nTip\n\n\n\nA useful reference website for CRS is EPSG.io: Coordinate Systems Worldwide\n\n\n\n\nGeographic Coordinate Reference Systems (CRS) use a three-dimensional spherical or ellipsoidal surface to represent locations on the Earth’s curved surface. Geographic CRSs are based on latitude and longitude coordinates and are useful for global or regional mapping.\n\nUnits: Degrees\nCommon Examples:\n\nWGS 84 (World Geodetic System 1984) EPSG:4326: A widely used CRS for global mapping and GPS navigation.\nNAD83 (North American Datum 1983) EPSG:4269: A CRS commonly used in North America.\n\n\n\n\n\nProjected Coordinate Reference Systems (CRS) use a two-dimensional Cartesian coordinate system to represent locations on a flat surface. These CRSs are created by “projecting” the curved Earth onto a flat surface, which introduces distortions, which inevitably requires tradeoffs in distances, areas, or shapes, with different projection systems involving different trade-offs. Projected CRSs are commonly used for local or small-scale mapping appropriate to specific countries or regions of the globe, where locally accurate measurements and comparisons are required.\n\nUnits: Meters (most commonly)\nCommon Examples:\n\nUTM Zone 30N (ETRS89) EPSG:3035: This CRS is commonly used in Europe (around latitude 30° North).\nBritish National Grid (BNG) EPSG:27700: This CRS is commonly used in the United Kingdom.\nState Plane Coordinate System (SPCS): This CRS is widely used in the United States for local or regional mapping.\n\n\n\n\n\nhttps://gisgeography.com/map-projection-types/"
  },
  {
    "objectID": "chapter_2/qgis.html#qgis-ui",
    "href": "chapter_2/qgis.html#qgis-ui",
    "title": "Intro to GIS",
    "section": "",
    "text": "General layout\nThe toolbox\nExtensions / plugins\nWeb tiles\n\n\n\n\n\n\n\nNote\n\n\n\n💻 Install Quick Map Services (QMS) → load OSM standard\n\n\n\n\n\n\n\n\nTip\n\n\n\n💻 Exercise → Load/Import data from DATOS MADRID Neighbourhoods (.shp); Population (.csv) → Explore data attributes : filter based on date (fecha)\n\n\nBarrios municipales de Madrid\nPoblación por distrito y barrio a 1 de enero"
  },
  {
    "objectID": "chapter_2/qgis.html#gis-data-sources",
    "href": "chapter_2/qgis.html#gis-data-sources",
    "title": "Intro to GIS",
    "section": "",
    "text": "Discovering sources for GIS (Geographic Information System) data is essential for spatial analysis and mapping. There are various official and non-official sources, each offering different types of data suited to specific needs and applications.\n\n\n\nGovernment Agencies: Agencies such as the US Geological Survey (USGS) and the Environmental Protection Agency (EPA) provide official and reliable GIS data. However, these datasets may reflect governmental priorities and perspectives.\nInternational Organizations: Organizations like the United Nations (UN) and the World Bank offer global GIS datasets, ideal for international research. They may not always capture local nuances due to their broad scope.\nLocal Authorities: City and regional councils supply local GIS data, which is often tailored to specific local government projects and priorities.\n\n\n\n\n\nPrivate Sector: Companies such as ArcGIS Living Atlas, Esri, and CARTO Datawarehouse offer comprehensive, commercially-oriented GIS datasets.\nSpecialized GIS Data Portals: Portals like the Harvard Geospatial Library provide access to specialized datasets across various fields.\nOpen Data Platforms: Platforms such as OpenStreetMap offer freely accessible GIS data, contributed by a community of users. These datasets might require additional validation due to their open-source nature.\nSpace Agencies: Agencies like NASA and the European Space Agency (ESA) provide official satellite imagery, particularly useful for environmental and spatial studies.\nAcademic Institutions: Universities often share GIS data derived from research projects. While not officially sanctioned, this data can be valuable but might need adaptation for specific uses.\nSocial Media and Crowdsourced Data: Platforms like Twitter and Facebook can be sources of GIS data, reflecting public behavior and trends. This data is not official and requires careful adaptation to account for inherent biases.\nWeb Scraping: Involves the automated collection of data from websites. This method is not an official source and the quality of data can vary. It’s useful for obtaining unique, real-time data. Web scraping demands significant technical skills for data extraction and processing. Legal and ethical considerations are important, as scraping can sometimes violate terms of service or data privacy laws.\n\n\n\n\nPurposely collected data is specifically gathered to meet the objectives of a defined research question or project goal. This type of data is obtained through structured methods such as surveys, experiments, or systematic observations.\n\n\n\nTailored Collection: The data is methodically gathered to align closely with the specific needs and aims of a particular study or project.\nControl Over Variables: Efforts are made to control or monitor variables, enhancing the relevance and accuracy of the data.\nHighly Relevant: The data’s targeted collection ensures its direct applicability and relevance to the specific research goals.\n\n\n\n\n\nSelection Bias: There’s a possibility of preferentially selecting data that confirms a pre-existing hypothesis, while disregarding contradicting data.\nResearcher Bias: The inclinations or hypotheses of the researcher may inadvertently influence the data collection methods and interpretation.\nSampling Bias: The methodology for selecting samples could impact the representativeness of the data, potentially leading to skewed outcomes.\n\n\n\n\n\nAdapted data is information that was initially collected for purposes other than the current research project but is repurposed or reinterpreted to suit new research needs. This category includes data from existing public databases, prior studies, or information obtained through web scraping.\n\n\n\nBroadly Collected: Originally gathered for various general or distinct objectives, often from pre-existing sources.\nVaried Sources: Derived from multiple sources, each with unique data collection methods.\nVersatility: Provides the opportunity to apply pre-existing data to new or unrelated research questions.\n\n\n\n\n\nBias of Original Purpose: The initial intent behind the data collection may affect the type and nature of the data, potentially reducing its applicability to new research questions.\nRisk of Contextual Misinterpretation: Lacking the original context, there is a danger of misinterpreting the data when adapting it for different uses.\nConcerns about Data Quality and Completeness: The original data may not fulfill the specific quality or completeness standards required for the new research purpose.\n\n\n\n\n\nWhen dealing with data, licenses refer to legal agreements or permissions that determine how the data can be used, distributed, or modified. Different datasets may have different licensing requirements, so it’s important to understand and comply with the specific license terms associated with the data you are using.\nExamples:\n\nCreative Commons (CC) licenses: Standardized licenses that allow creators to choose permissions for their work. Used for creative works like images, music, videos, and written content.\nOpen Database License (ODbL): Specifically for databases. Allows users to freely use, share, and modify the database. Modifications and derivative works must be under the same license. Used for open data projects and community-driven data initiatives.\nGNU General Public License (GPL): Copyleft license for open-source software. Allows modification and distribution. Modifications and derivative works must be under the same license. Used for promoting open-source principles and collaboration.\n\n\n\n\nAttribution or citation refers to giving proper credit to the original source or creator of the data. Just like when citing sources in academic papers, it’s important to acknowledge the origin of the data you are using. This helps provide transparency, accountability, and recognition to the creators of the dataset.\n\n\n\nPrivacy considerations are important when working with data, especially if the data contains personally identifiable information or sensitive information. It’s very important to handle and store data in a way that respects privacy laws and regulations. This may involve anonymizing or de-identifying data, implementing security measures, and obtaining consent from individuals whose data is being used.\n\n\n\n\n\n\nNote\n\n\n\nPlease avoid using any and all data containing private or sensitive data unless you first review this and seek approval from faculty. Handling these forms of data comes with numerous steps and safeguards and will generally be strongly discouraged."
  },
  {
    "objectID": "chapter_2/qgis.html#reprojections-in-qgis",
    "href": "chapter_2/qgis.html#reprojections-in-qgis",
    "title": "Intro to GIS",
    "section": "",
    "text": "Often times, data from several datasources (or even the same one) comes with different projection systems. Other times it’s a matter of software support or visual preference, but what’s really important is to aim for consolidating all your data layers into the same projection system. This will help with accurate calculations and ‘real’ overlap of layers.\n\n\n\n\n\n\nNote\n\n\n\n💻 Using Madrid’s neighbourhoods (above) and the toolbox → Search for ’reproject layer’ → Reproject layer to EPSG: 4326 (and others) and save as a different files (or try Geopackage layers) → Restart a new project and load the new layer → See the layer properties of each and how they overlap or change shapes."
  },
  {
    "objectID": "chapter_2/qgis.html#attributes",
    "href": "chapter_2/qgis.html#attributes",
    "title": "Intro to GIS",
    "section": "",
    "text": "Attributes in GIS layers provide additional information about the spatial features in the data. They include text, numbers, dates, or images. Attributes help add context to the geographic elements, enabling better analysis and visualization. They can be used to filter, classify, and symbolize features based on their characteristics. Attributes play a crucial role in data analysis, allowing us to gain insights, make comparisons, and create meaningful visual representations.\n\n\n\n\n\n\nNote\n\n\n\nInputting and editing data\n💻 → calculate the area of all polygons and save in new field\n\n\n\n\n\n\n\n\nNote\n\n\n\nAttribute based joins 💻 → Identify the common attributes between the two layers (population and polygons) → Create a Join based on attributes → Calculate the density of population and the density of men and women separately. → delete unnecessary attributes → Save and export (selected features only)\n\n\n\n\n\n\n\n\nNote\n\n\n\nStyling and visualising data 💻 → Create a Cloropeth map with the new density measure"
  },
  {
    "objectID": "chapter_2/qgis.html#connecting-to-data-on-postgis",
    "href": "chapter_2/qgis.html#connecting-to-data-on-postgis",
    "title": "Intro to GIS",
    "section": "",
    "text": "⚙\nCONNECTION SETTINGS\n\n\n\n\nIP Address\n(will be provided during class)\n\n\nDatabase names\nnfi / urban_analytics\n\n\nPort\n5432\n\n\nUsername\nscholar\n\n\nPassword\n(will be provided during class)\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote that the connections to the Databases are restricted by the source IP address."
  },
  {
    "objectID": "chapter_2/qgis.html#map-types",
    "href": "chapter_2/qgis.html#map-types",
    "title": "Intro to GIS",
    "section": "",
    "text": "Point Maps: Represent specific locations.\nHeat Maps: Visualise the density or intensity of a phenomenon.  \nFlow Maps: Represent flows or strengths of connections. \nCartograms: Distort the geographic shape of regions to represent a specific variable. \nCloropleth: Use color to represent a particular attribute or variable."
  },
  {
    "objectID": "chapter_2/qgis.html#quirks-part-1",
    "href": "chapter_2/qgis.html#quirks-part-1",
    "title": "Intro to GIS",
    "section": "",
    "text": "The Modifiable Areal Unit Problem (MAUP) is a common issue in geography and maps. It refers to the fact that the results of statistical analysis or visual representation can be influenced by the choice of spatial unit or scale used to aggregate data. The MAUP arises because geographic phenomena can be measured and analysed at different levels of spatial granularity, such as census tracts, counties, or regions. When data is aggregated into different spatial units, it can lead to different patterns, relationships, and interpretations.\nhttps://gisgeography.com/maup-modifiable-areal-unit-problem/\nCrime Rates: Imagine you have crime data recorded at the neighborhoud level. If you aggregate the data into larger units, such as police precincts, the crime rates may appear different due to the varying sizes and characteristics of neighborhouds within each precinct. This can result in different spatial patterns and potentially lead to incorrect conclusions about crime rates in different areas.\nhttps://www.youtube.com/watch?v=7NTw1v7DuAA&t=115s\nThese examples demonstrate how the choice of spatial unit can significantly impact the analysis and interpretation of geographic data, highlighting the need to carefully consider the scale and boundaries of spatial units when conducting spatial analysis or creating maps."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NFI Urban Analytics 2024",
    "section": "",
    "text": "Welcome to the Python Urban Analytics course at the Norman Foster Institute. This course is designed to empower scholars with the skills and knowledge to harness the power of urban analytics and tools such as QGIS and Python in analyzing and interpreting urban data.\nSee the sidebar for the Sections which contain readings, data, methods, and lab work.\nThis content will be further elaborated and refined as the year progresses.\nWe hope you enjoy the course!\n\n\n\nYou have been added to a Google Spaces for online discussions. This platform is your primary hub for:\n\nSeeking peer assistance.\nDiscussing course materials and urban analytics topics.\nCollaborating on projects and assignments.\nAnnouncements and updates from the course team.\n\nThe learning process is enriched through collaborative effort. Your contributions make a difference!\n\n\n\nThis course is will touch on a number of broad topics. High-level guidance will be provided and scholars will then be encouraged to further explore these approaches and develop their skillsets based on methods of interest or relevance to their work:\n\nSpatial Analysis: Exploring Geographic Information Systems, spatial data handling, and mapping urban data.\nPython: Introduction to Python and its usage for geospatial analysis.\nData Handling: Techniques for handling datasets and an introduction to data science methods.\nProject Work: Lab work tasks and workflows."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "NFI Urban Analytics 2024",
    "section": "",
    "text": "Welcome to the Python Urban Analytics course at the Norman Foster Institute. This course is designed to empower scholars with the skills and knowledge to harness the power of urban analytics and tools such as QGIS and Python in analyzing and interpreting urban data.\nSee the sidebar for the Sections which contain readings, data, methods, and lab work.\nThis content will be further elaborated and refined as the year progresses.\nWe hope you enjoy the course!"
  },
  {
    "objectID": "index.html#communication",
    "href": "index.html#communication",
    "title": "NFI Urban Analytics 2024",
    "section": "",
    "text": "You have been added to a Google Spaces for online discussions. This platform is your primary hub for:\n\nSeeking peer assistance.\nDiscussing course materials and urban analytics topics.\nCollaborating on projects and assignments.\nAnnouncements and updates from the course team.\n\nThe learning process is enriched through collaborative effort. Your contributions make a difference!"
  },
  {
    "objectID": "index.html#topics",
    "href": "index.html#topics",
    "title": "NFI Urban Analytics 2024",
    "section": "",
    "text": "This course is will touch on a number of broad topics. High-level guidance will be provided and scholars will then be encouraged to further explore these approaches and develop their skillsets based on methods of interest or relevance to their work:\n\nSpatial Analysis: Exploring Geographic Information Systems, spatial data handling, and mapping urban data.\nPython: Introduction to Python and its usage for geospatial analysis.\nData Handling: Techniques for handling datasets and an introduction to data science methods.\nProject Work: Lab work tasks and workflows."
  },
  {
    "objectID": "chapter_0/setup.html",
    "href": "chapter_0/setup.html",
    "title": "Setup",
    "section": "",
    "text": "Instructions for how to setup your Github account and how to sign-up for the Github Student Developer Pack will be given in class. Please don’t try to activate the Student Developer Pack ahead of class. This is so that it can be configured with the correct procedure.\n\n\n\nSetup for the CARTO account will be detailed in class after activating the Github Student Developer Pack.\n\n\n\nDownload and install QGIS form the official download links.\n\n\n\nDownload and install Anaconda from the official download links."
  },
  {
    "objectID": "chapter_0/setup.html#github-and-the-github-student-developer-pack",
    "href": "chapter_0/setup.html#github-and-the-github-student-developer-pack",
    "title": "Setup",
    "section": "",
    "text": "Instructions for how to setup your Github account and how to sign-up for the Github Student Developer Pack will be given in class. Please don’t try to activate the Student Developer Pack ahead of class. This is so that it can be configured with the correct procedure."
  },
  {
    "objectID": "chapter_0/setup.html#carto-account",
    "href": "chapter_0/setup.html#carto-account",
    "title": "Setup",
    "section": "",
    "text": "Setup for the CARTO account will be detailed in class after activating the Github Student Developer Pack."
  },
  {
    "objectID": "chapter_0/setup.html#qgis",
    "href": "chapter_0/setup.html#qgis",
    "title": "Setup",
    "section": "",
    "text": "Download and install QGIS form the official download links."
  },
  {
    "objectID": "chapter_0/setup.html#anaconda",
    "href": "chapter_0/setup.html#anaconda",
    "title": "Setup",
    "section": "",
    "text": "Download and install Anaconda from the official download links."
  },
  {
    "objectID": "chapter_0/anaconda.html",
    "href": "chapter_0/anaconda.html",
    "title": "Python Notebooks",
    "section": "",
    "text": "Welcome to the first hands-on section of the course where you will familiarise yourself with Jupyter Lab for running Python ‘Notebooks’.\nThis guide assumes the prior installation of Anaconda as detailed previously.\n\n\nCreate a new “environment” the first time you use Anaconda. Environments ensure that packages remain self-contained and make it easier to synchronise packages without unnecessary contamination from unrelated projects.\n\nOpen Anaconda Navigator.\nClick on Environments\nClick to create a new environment called nfi.\nSelect the newly created nfi environment from the list of environments.\nIn the right pane, change the dropdown view mode to “Not Installed”.\nSearch for jupyterlab, then select it and click to install.\nSelect the Home button to return to the main view.\n\n\n\n\nTo launch Juptyer Lab:\n\nOpen Anaconda Navigator.\nUsing the top dropdown menus, select “All applications” and select the newly created “nfi” environment.\nClick to launch “Jupyter Lab” from the list of applications. This will launch a new Jupyter Lab instance in a web browser.\nUse the “Python 3” kernel if prompted.\n\n\n\n\n\n\n\nTip\n\n\n\nDon’t confuse the “Jupyer Lab” with the “Jupyter Notebook” apps. The former is a newer replacement for the latter.\n\n\n\n\n\nYou can now create a new Notebook or you otherwise open an existing Python Notebook. Notebooks are composed of cells.\n\n\n\nJupyter Notebool cell\n\n\nCells can be either code (Python) or text (Markdown). A typical notebook is then a series of cells where some include text describing what is happening while others contain the code, either waiting for execution or already executed. The cells with the executed code may also contain outputs. The individual code cells can be “played” to see how the code actions unfold from step to step.\nYou can try creating your own cell content and experimenting with running the cells. Remember that the cells are only aware of previously run cells. We can start with simple math that Python can do natively. Run the following code cell in your notebook. After inserting the text, you can either click the “play” button or press the Shift + Enter keys.\n\n1 + 1\n# this will return 2 if once the cell is run\n\n2\n\n\n\n\n\nShut down Jupyter Lab from the menubar in the web interface using File &gt; Shut Down.\n\n\n\nShut down Jupyter Lab\n\n\n\n\n\nIf you are unable to install an environment using the instructions above, you can follow the course using Google Colab. You will need to install any required packages to your Colab environment. Reach out in class if you need help getting set up."
  },
  {
    "objectID": "chapter_0/anaconda.html#overview",
    "href": "chapter_0/anaconda.html#overview",
    "title": "Python Notebooks",
    "section": "",
    "text": "Welcome to the first hands-on section of the course where you will familiarise yourself with Jupyter Lab for running Python ‘Notebooks’.\nThis guide assumes the prior installation of Anaconda as detailed previously.\n\n\nCreate a new “environment” the first time you use Anaconda. Environments ensure that packages remain self-contained and make it easier to synchronise packages without unnecessary contamination from unrelated projects.\n\nOpen Anaconda Navigator.\nClick on Environments\nClick to create a new environment called nfi.\nSelect the newly created nfi environment from the list of environments.\nIn the right pane, change the dropdown view mode to “Not Installed”.\nSearch for jupyterlab, then select it and click to install.\nSelect the Home button to return to the main view.\n\n\n\n\nTo launch Juptyer Lab:\n\nOpen Anaconda Navigator.\nUsing the top dropdown menus, select “All applications” and select the newly created “nfi” environment.\nClick to launch “Jupyter Lab” from the list of applications. This will launch a new Jupyter Lab instance in a web browser.\nUse the “Python 3” kernel if prompted.\n\n\n\n\n\n\n\nTip\n\n\n\nDon’t confuse the “Jupyer Lab” with the “Jupyter Notebook” apps. The former is a newer replacement for the latter.\n\n\n\n\n\nYou can now create a new Notebook or you otherwise open an existing Python Notebook. Notebooks are composed of cells.\n\n\n\nJupyter Notebool cell\n\n\nCells can be either code (Python) or text (Markdown). A typical notebook is then a series of cells where some include text describing what is happening while others contain the code, either waiting for execution or already executed. The cells with the executed code may also contain outputs. The individual code cells can be “played” to see how the code actions unfold from step to step.\nYou can try creating your own cell content and experimenting with running the cells. Remember that the cells are only aware of previously run cells. We can start with simple math that Python can do natively. Run the following code cell in your notebook. After inserting the text, you can either click the “play” button or press the Shift + Enter keys.\n\n1 + 1\n# this will return 2 if once the cell is run\n\n2\n\n\n\n\n\nShut down Jupyter Lab from the menubar in the web interface using File &gt; Shut Down.\n\n\n\nShut down Jupyter Lab\n\n\n\n\n\nIf you are unable to install an environment using the instructions above, you can follow the course using Google Colab. You will need to install any required packages to your Colab environment. Reach out in class if you need help getting set up."
  },
  {
    "objectID": "chapter_2/sql.html",
    "href": "chapter_2/sql.html",
    "title": "SQL - A cross platform language",
    "section": "",
    "text": "SQL and PostgreSQL Basics:\n\n\nUnderstanding SQL (Structured Query Language)\nIntroduction to PostgreSQL as a Relational Database Management System (RDBMS)\nWorking with Schemas in PostgreSQL\n\n\nData Manipulation with SQL and PostgreSQL:\n\n\nCREATE, SELECT, INSERT, DELETE Statements\nPrimary Keys (PK) and Foreign Keys (FK)\nCreating and Managing Indexes for Performance Optimization\n\n\nSQL Concepts:\n\n\nAggregations: COUNT, SUM, AVG, MIN, MAX\nComplex Queries and Subqueries WITH, WHERE\nConditional Statements: CASE Statements\n\n\nIntroduction to GIS Integration with PostgreSQL:\n\n\nSpatial Data Types in PostgreSQL\nSpatial Indexing for Efficient Spatial Queries\nSpatial Functions and Operators in PostgreSQL\n\n\nSpatial and Non-Spatial Joins:\n\n\nPerforming Spatial Joins for GIS Analysis\nNon-Spatial Joins for Relational Data Analysis\nCombining Spatial and Non-Spatial Data in Queries\n\n\nHow to ask ChatGPT\n\n\nIterating query development\nDebugging"
  },
  {
    "objectID": "chapter_2/sql.html#key-topics",
    "href": "chapter_2/sql.html#key-topics",
    "title": "SQL - A cross platform language",
    "section": "Key Topics",
    "text": "Key Topics\n\nIntroduction to Data Analysis and Databases:\n\n\nImportance of Data Analysis in Decision Making\nOverview of Relational Databases\n\n\nConnecting to Google Cloud Platform (GCP):\n\n\nConnecting to NFI’s database(s)\nSetting Up a GCP Account\n\n\nSQL and PostgreSQL Basics:\n\n\nUnderstanding SQL (Structured Query Language)\nIntroduction to PostgreSQL as a Relational Database Management System (RDBMS)\nWorking with Schemas in PostgreSQL\n\n\nData Manipulation with SQL and PostgreSQL:\n\n\nCREATE, SELECT, INSERT, DELETE Statements\nPrimary Keys (PK) and Foreign Keys (FK)\nCreating and Managing Indexes for Performance Optimization\n\n\nSQL Concepts:\n\n\nAggregations: COUNT, SUM, AVG, MIN, MAX\nComplex Queries and Subqueries WITH, WHERE\nConditional Statements: CASE Statements\n\n\nIntroduction to GIS Integration with PostgreSQL:\n\n\nSpatial Data Types in PostgreSQL\nSpatial Indexing for Efficient Spatial Queries\nSpatial Functions and Operators in PostgreSQL\n\n\nSpatial and Non-Spatial Joins:\n\n\nPerforming Spatial Joins for GIS Analysis\nNon-Spatial Joins for Relational Data Analysis\nCombining Spatial and Non-Spatial Data in Queries"
  },
  {
    "objectID": "chapter_2/sql.html#software-needed",
    "href": "chapter_2/sql.html#software-needed",
    "title": "SQL - A cross platform language",
    "section": "Software needed",
    "text": "Software needed\n\nFor this session please make sure you have downloaded the following software(s):\n\nTablePlus | Modern, Native Tool for Database Management."
  },
  {
    "objectID": "chapter_2/sql.html#to-dos",
    "href": "chapter_2/sql.html#to-dos",
    "title": "SQL - A cross platform language",
    "section": "To-Dos",
    "text": "To-Dos\n\nCreate a table with the following characteristics.\nUse schema. ua0\nTable name : [yourinitials] + “_prof_exp”\nFields required: (assign data types as you think would be best)\n\nid\nscholar_id\nindustry_id\ninstitution\nyears_of_experience\neducation\nlocation (crs: 4326)\nend_year\nnotes\n\n** If you think more fields are necessary, feel free to add them\nCreate a GitHub account\n\nadd link to public.scholars.github_link (varchar)\n\n\n\nOptional\nCreate a local instance of PostGIS for use in the upcoming months\nLearn PostgreSQL Tutorial - Full Course for Beginners"
  },
  {
    "objectID": "chapter_2/sql.html#useful-resources",
    "href": "chapter_2/sql.html#useful-resources",
    "title": "SQL - A cross platform language",
    "section": "Useful Resources",
    "text": "Useful Resources\n\n**********POSTGRESQL & POSTGIS CHEATSHEETS**********\n\nPostgreSQL & PostGIS cheatsheet (a work in progress)\nPostgres Cheatsheet"
  },
  {
    "objectID": "chapter_1/python.html",
    "href": "chapter_1/python.html",
    "title": "Python and Notebooks",
    "section": "",
    "text": "Python can be used as a simple calculator. Remember, you can press Shift + Enter to execute the code in the cells below. Try it out by typing some simple math into new cells and see what you get.\n\n42 * 12\n\n504\n\n\n\n12 / 3\n\n4.0\n\n\nIf you want to edit and re-run some code, change the cell and press Shift + Enter to execute the modified code."
  },
  {
    "objectID": "chapter_1/python.html#simple-python-math",
    "href": "chapter_1/python.html#simple-python-math",
    "title": "Python and Notebooks",
    "section": "",
    "text": "Python can be used as a simple calculator. Remember, you can press Shift + Enter to execute the code in the cells below. Try it out by typing some simple math into new cells and see what you get.\n\n42 * 12\n\n504\n\n\n\n12 / 3\n\n4.0\n\n\nIf you want to edit and re-run some code, change the cell and press Shift + Enter to execute the modified code."
  },
  {
    "objectID": "chapter_1/python.html#functions",
    "href": "chapter_1/python.html#functions",
    "title": "Python and Notebooks",
    "section": "Functions",
    "text": "Functions\nYou can use Python for more advanced math by using a function. Functions are pieces of code that perform a single action, such as printing information to the screen (e.g., the print() function). Functions exist for a huge number of operations in Python.\nLet’s try out a few simple examples using functions to find the sin or square root of a value. You can type sin(3) or sqrt(4) into the cells below to test this out.\n\nsin(3)\n\nNameError: name 'sin' is not defined\n\n\n\nsqrt(4)\n\nNameError: name 'sqrt' is not defined\n\n\nWell, that didn’t work. Python can calculate square roots or do basic trigonometry, but we need one more step.\n\nMath operations\nThe table below shows the list of basic arithmetic operations that can be done by default in Python.\n\n\n\nOperation\nSymbol\nExample syntax\nReturned value\n\n\n\n\nAddition\n+\n2 + 2\n4\n\n\nSubtraction\n-\n4 - 2\n2\n\n\nMultiplication\n*\n2 * 3\n6\n\n\nDivision\n/\n4 / 2\n2\n\n\nExponentiation\n**\n2 ** 3\n8\n\n\n\nFor anything more advanced, we need to load a module or a package. For math operations, this module is called math and can be loaded by typing import math.\n\nimport math\n\nNow that we have access to functions in the math module, we can use it by typing the module name, a period (dot), and the name of the function we want to use. For example, math.sin(3). Try this with the sine and square root examples from above.\n\nmath.sin(3)\n\n0.1411200080598672\n\n\n\nmath.sqrt(4)\n\n2.0\n\n\nLet’s summarise what you’ve just done with modules:\n\nA module is a group of code items, such as functions, related to one another. Individual modules are often in a group called a package.\nModules can be loaded using import. Functions that are part of the module modulename can then be used by typing modulename.functionname(). For example, sin() is a function that is part of the math module and is used by typing math.sin() with some number between the parentheses.\nWithin a Jupyter Notebook, the variables you defined earlier in the notebook will be available for use in the following cells as long as you have executed the cells.\nModules may also contain constants such as math.pi (notice no parentheses at the end). Type this in the cell below to see the constant’s math.pi value.\n\n\nmath.pi\n\n3.141592653589793\n\n\n\n\nCombining functions\nFunctions can also be combined. The print() function returns values within the parentheses as text on the screen. Below, try printing the value of the square root of four.\n\nprint(math.sqrt(4))\n\n2.0\n\n\nYou can also combine text with other calculated values using the print() function. For example, print('Two plus two is', 2+2) would generate the text reading 'Two plus two is 4'. Combine the print() function with the math.sqrt() function in the cell below to produce text that reads 'The square root of 4 is 2.0'.\n\nprint(\"The square root of 4 is\", math.sqrt(4))\n\nThe square root of 4 is 2.0"
  },
  {
    "objectID": "chapter_1/python.html#variables",
    "href": "chapter_1/python.html#variables",
    "title": "Python and Notebooks",
    "section": "Variables",
    "text": "Variables\nA variable can store values calculated in expressions and used for other calculations. Assigning value to variables is straightforward. To assign a value, you type variable_name = value, where variable_name is the name of the variable you wish to define. In the cell below, define a variable called temp_celsius, assign it a value of 10.0, and then print that variable value using the print() function. Note that you should do this on two separate lines.\n\ntemp_celsius = 10.0\nprint(temp_celsius)\n\n10.0\n\n\nAs we did above, you can combine text and even use some math when printing out variable values. The idea is similar to adding 2+2 or calculating the square root of four from the previous section. In the cell below, print out the value of temp_celsius in degrees Fahrenheit by multiplying temp_celsius by 9/5 and adding 32. This should be done within the print() function to produce output that reads 'Temperature in Fahrenheit: 50.0'.\n\nprint(\"Temperature in Fahrenheit:\", 9 / 5 * temp_celsius + 32)\n\nTemperature in Fahrenheit: 50.0\n\n\n\nUpdating variables\nValues stored in variables can also be updated. Let’s redefine the value of temp_celsius to be equal to 15.0 and print its value in the cells below.\n\ntemp_celsius = 15.0\n\n\nprint(\"temperature in Celsius is now:\", temp_celsius)\n\ntemperature in Celsius is now: 15.0\n\n\n\nWarning\nIf you try to run some code that accesses a variable that has not yet been defined, you will get a NameError message. Try printing out the value of the variable temp_fahrenheit using the print() function in the cell below.\n\n\nprint(\"Temperature in Celsius:\", 5 / 9 * (temp_fahrenheit - 32))\n\nTemperature in Celsius: 15.0\n\n\n\nNote\nOne of the interesting things here is that if we define the undefined variable in a cell lower down in the notebook and execute that cell, we can return to the earlier cell, and the code should now work. That was a bit of a complicated sentence, so let’s test this all out. First, let’s define a variable called temp_fahrenheit in the cell below and assign it to be equal to 9/5 * temp_celsius + 32, the conversion factor from temperatures in Celsius to Fahrenheit. Then, return to the cell above this text and run that cell again. See how the error message has gone away? temp_fahrenheit has now been defined, and thus, the cell above no longer generates a NameError when the code is executed.\nAlso, the number beside the cell, for example, In [2], tells you the order in which the Python cells have been executed. This way, you can see a history of the order in which you have run the cells.\n\n\ntemp_fahrenheit = 9 / 5 * temp_celsius + 32\n\nTo check their current values, print out the values of temp_celsius and temp_fahrenheit in the cell below.\n\nprint(\"temperature in Celsius:\", temp_celsius, \"and in Fahrenheit:\", temp_fahrenheit)\n\ntemperature in Celsius: 15.0 and in Fahrenheit: 59.0"
  },
  {
    "objectID": "chapter_1/python.html#data-types",
    "href": "chapter_1/python.html#data-types",
    "title": "Python and Notebooks",
    "section": "Data types",
    "text": "Data types\nA data type determines the characteristics of data in a program. There are four basic data types in Python, as shown in the table below.\n\n\n\nData type name\nData type\nExample\n\n\n\n\nint\nWhole integer values\n4\n\n\nfloat\nDecimal values\n3.1415\n\n\nstr\nCharacter strings\n'Hot'\n\n\nbool\nTrue/false values\nTrue\n\n\n\nThe data type can be found using the type() function. As you will see, the data types are essential because some are incompatible.\nLet’s define a variable weather_forecast and assign it the value 'Hot'. After this, we can check its data type using the type() function.\n\nweather_forecast = \"Hot\"\ntype(weather_forecast)\n\nstr\n\n\nLet’s also check the type of temp_fahrenheit. What happens if you try to combine temp_fahrenheit and weather_forecast in a single math equation such as temp_fahrenheit = temp_fahrenheit + 5.0 * weather_forecast?\n\ntype(temp_fahrenheit)\n\nfloat\n\n\n\ntemp_fahrenheit = temp_fahrenheit + 5.0 * weather_forecast\n\nTypeError: can't multiply sequence by non-int of type 'float'\n\n\nIn this case, we get at TypeError because we are trying to execute a math operation with data types that are not compatible. There is no way in Python to multiply numbers with a character string."
  },
  {
    "objectID": "chapter_1/python.html#acknowledgements",
    "href": "chapter_1/python.html#acknowledgements",
    "title": "Python and Notebooks",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe ‘Let the snake in’ section is derived from A taste of Python section of the Geo-Python course 2022 by D. Whipp, H. Tenkanen, V. Heikinheimo, H. Aagesen, and C. Fink from the Department of Geosciences and Geography, University of Helsinki, licensed under CC-BY-SA 4.0."
  }
]