[
  {
    "objectID": "chapter_0/installation.html",
    "href": "chapter_0/installation.html",
    "title": "Software & Accounts",
    "section": "",
    "text": "Instructions for how to setup your Github account and how to sign-up for the Github Student Developer Pack will be given in class. Please don’t try to activate the Student Developer Pack ahead of class. This is so that it can be configured with the correct accounts and procedure.\n\n\n\nSetup for the CARTO account will be detailed in class after activating the Github Student Developer Pack.\n\n\n\nDownload and install QGIS form the official download links.\n\n\n\nDownload and install Anaconda from the official download links.\n\n\n\nDownload and install TablePlus.. This is a tool for interacting with databases.",
    "crumbs": [
      "Setup",
      "Software & Accounts"
    ]
  },
  {
    "objectID": "chapter_0/installation.html#github-student-developer-pack",
    "href": "chapter_0/installation.html#github-student-developer-pack",
    "title": "Software & Accounts",
    "section": "",
    "text": "Instructions for how to setup your Github account and how to sign-up for the Github Student Developer Pack will be given in class. Please don’t try to activate the Student Developer Pack ahead of class. This is so that it can be configured with the correct accounts and procedure.",
    "crumbs": [
      "Setup",
      "Software & Accounts"
    ]
  },
  {
    "objectID": "chapter_0/installation.html#carto-account",
    "href": "chapter_0/installation.html#carto-account",
    "title": "Software & Accounts",
    "section": "",
    "text": "Setup for the CARTO account will be detailed in class after activating the Github Student Developer Pack.",
    "crumbs": [
      "Setup",
      "Software & Accounts"
    ]
  },
  {
    "objectID": "chapter_0/installation.html#qgis",
    "href": "chapter_0/installation.html#qgis",
    "title": "Software & Accounts",
    "section": "",
    "text": "Download and install QGIS form the official download links.",
    "crumbs": [
      "Setup",
      "Software & Accounts"
    ]
  },
  {
    "objectID": "chapter_0/installation.html#anaconda",
    "href": "chapter_0/installation.html#anaconda",
    "title": "Software & Accounts",
    "section": "",
    "text": "Download and install Anaconda from the official download links.",
    "crumbs": [
      "Setup",
      "Software & Accounts"
    ]
  },
  {
    "objectID": "chapter_0/installation.html#table-plus",
    "href": "chapter_0/installation.html#table-plus",
    "title": "Software & Accounts",
    "section": "",
    "text": "Download and install TablePlus.. This is a tool for interacting with databases.",
    "crumbs": [
      "Setup",
      "Software & Accounts"
    ]
  },
  {
    "objectID": "chapter_0/index.html",
    "href": "chapter_0/index.html",
    "title": "Overview",
    "section": "",
    "text": "Overview\nThis section details installation and steps for setting up your Github account, signing up for the Github Student Developer Pack, software downloads, and getting Python notebooks running on your system.",
    "crumbs": [
      "Setup",
      "Overview"
    ]
  },
  {
    "objectID": "chapter_2/open_source.html",
    "href": "chapter_2/open_source.html",
    "title": "Open Source",
    "section": "",
    "text": "Use open-source code wherever possible! Open source code consists of a dynamic ecosystem of constantly co-evolving code packages. This collaborative development has permitted a remarkable progression in the capabilities and sophistication of freely accessible code. This has benefitted geospatial and urban analysis immensely, making these tools more widely accessible to those who wish to use them.\nTo further cement some basic Python skills, we’ll be making use of the open-source learn-python3 repository, created and maintained by Github user jerry-git.",
    "crumbs": [
      "Basic Coding",
      "Open Source"
    ]
  },
  {
    "objectID": "chapter_2/open_source.html#fetching-the-notebooks",
    "href": "chapter_2/open_source.html#fetching-the-notebooks",
    "title": "Open Source",
    "section": "Fetching the notebooks",
    "text": "Fetching the notebooks\n\nGo to the learn-python3 repository.\nBefore using code from online code repositories, double-check the license type. In this case, it is MIT licensed which is an open-source license.\nDownload the repository to a local folder by clicking on the green “code” dropdown button, and select “Download ZIP”.\nUnzip if necessary and move the folder to a working location of your choosing.",
    "crumbs": [
      "Basic Coding",
      "Open Source"
    ]
  },
  {
    "objectID": "chapter_2/open_source.html#using-the-notebooks",
    "href": "chapter_2/open_source.html#using-the-notebooks",
    "title": "Open Source",
    "section": "Using the notebooks",
    "text": "Using the notebooks\n\nOpen your Anaconda Navigator and launch Jupyter Lab as previously.\nOnce Jupyter Lab has launched, navigate to the downloaded folder and open the notebooks &gt; beginner &gt; notebooks folder.\nSequentially work through notebooks from 1 to 7. Feel free to experiment with the notebook content.\nEmerging AI tools such as ChatGPT are incredibly useful for understanding and adapting code. These tools are largely replacing previously used online resources such as Stack Overflow (though these can still be useful).",
    "crumbs": [
      "Basic Coding",
      "Open Source"
    ]
  },
  {
    "objectID": "chapter_1/gis_tips.html",
    "href": "chapter_1/gis_tips.html",
    "title": "Tips",
    "section": "",
    "text": "In multiple scenarios you may find information in one table/layer that you might want to connect to information from another. JOINS are the way to do this, and work by connecting two different datasources based on a shared attribute. So doing, the attributes from one table can be connected to another for analysis or visualision purposes.\n\n\n\n\nInner Join: Returns only rows that have been matched across both tables.\nLeft (Outer) Join: Returns all rows from the left table, and matched rows from the right table where found.\nRight (Outer) Join: Returns all rows from the right table, and matched rows from the left table where found.\nFull (Outer) Join: Returns all rows from both tables, regardless of whether matches were found in the other table.\nCross Join: Returns a Cartesian product of the two tables, i.e. each row connected to each other row from the other table. Use with caution!\n\n\n\n\n\nJoin types\n\n\n\n\n\n\n\n\n\nOccurs when each record in one table corresponds to exactly one record in another table.\n\n\n\n\n\n\n\nOccurs when a single record in one table can be associated with one or more records in another table.\n\n\n\n\n\n\n\nOccurrs when multiple records from one table correspond with one or more in another table.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nWhen using joins you are not modifying the original tables.\nYour resulting table can be much smaller or (exponentially) larger than any of the original tables.\n\n\n\n\n\n\nSimilar to joining tables based on their attributes, it is possible to join them based on their location. This is a very common technique in spatial databases (e.g. PostGIS) where you’ll commonly use a feature such as a city boundary to filter and join data from another table (e.g. streets).\nTo achieve the expected results, make sure that both origin tables are in the same CRS.\n\n\nIn QGIS and spatial databases, spatial predicates are used to define spatial relationships between features. These are used for joining or selecting data based on the relationships between the features. Some of the common predicates are:\n\nContains: Determines if one geometry contains another.\nWithin: Checks if one geometry is within another geometry.\nIntersects: Tests if two geometries intersect at any point.\nDisjoint: Determines if two geometries do not intersect.\nTouches: Checks if two geometries have at least one point in common but their interiors do not intersect.\nCrosses: Determines if the geometries have some, but not all, interior points in common.\nEquals: Checks if two geometries are exactly the same in terms of spatial coordinates.\nOverlaps: Determines if two geometries overlap, meaning they share some but not all points, and the intersection has the same dimension as the geometries themselves.\n\n\n\n\n\n\n\nCaution\n\n\n\nPlease note that depending on the size and complexity of your joins some operations can be computationally expensive and some advanced techniques might need to be implemented to achieve best performance.",
    "crumbs": [
      "GIS Tools",
      "Tips"
    ]
  },
  {
    "objectID": "chapter_1/gis_tips.html#join-types",
    "href": "chapter_1/gis_tips.html#join-types",
    "title": "Tips",
    "section": "",
    "text": "Inner Join: Returns only rows that have been matched across both tables.\nLeft (Outer) Join: Returns all rows from the left table, and matched rows from the right table where found.\nRight (Outer) Join: Returns all rows from the right table, and matched rows from the left table where found.\nFull (Outer) Join: Returns all rows from both tables, regardless of whether matches were found in the other table.\nCross Join: Returns a Cartesian product of the two tables, i.e. each row connected to each other row from the other table. Use with caution!\n\n\n\n\n\nJoin types",
    "crumbs": [
      "GIS Tools",
      "Tips"
    ]
  },
  {
    "objectID": "chapter_1/gis_tips.html#cardinality",
    "href": "chapter_1/gis_tips.html#cardinality",
    "title": "Tips",
    "section": "",
    "text": "Occurs when each record in one table corresponds to exactly one record in another table.\n\n\n\n\n\n\n\nOccurs when a single record in one table can be associated with one or more records in another table.\n\n\n\n\n\n\n\nOccurrs when multiple records from one table correspond with one or more in another table.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nWhen using joins you are not modifying the original tables.\nYour resulting table can be much smaller or (exponentially) larger than any of the original tables.",
    "crumbs": [
      "GIS Tools",
      "Tips"
    ]
  },
  {
    "objectID": "chapter_1/gis_tips.html#introduction-to-spatial-joins",
    "href": "chapter_1/gis_tips.html#introduction-to-spatial-joins",
    "title": "Tips",
    "section": "",
    "text": "Similar to joining tables based on their attributes, it is possible to join them based on their location. This is a very common technique in spatial databases (e.g. PostGIS) where you’ll commonly use a feature such as a city boundary to filter and join data from another table (e.g. streets).\nTo achieve the expected results, make sure that both origin tables are in the same CRS.\n\n\nIn QGIS and spatial databases, spatial predicates are used to define spatial relationships between features. These are used for joining or selecting data based on the relationships between the features. Some of the common predicates are:\n\nContains: Determines if one geometry contains another.\nWithin: Checks if one geometry is within another geometry.\nIntersects: Tests if two geometries intersect at any point.\nDisjoint: Determines if two geometries do not intersect.\nTouches: Checks if two geometries have at least one point in common but their interiors do not intersect.\nCrosses: Determines if the geometries have some, but not all, interior points in common.\nEquals: Checks if two geometries are exactly the same in terms of spatial coordinates.\nOverlaps: Determines if two geometries overlap, meaning they share some but not all points, and the intersection has the same dimension as the geometries themselves.\n\n\n\n\n\n\n\nCaution\n\n\n\nPlease note that depending on the size and complexity of your joins some operations can be computationally expensive and some advanced techniques might need to be implemented to achieve best performance.",
    "crumbs": [
      "GIS Tools",
      "Tips"
    ]
  },
  {
    "objectID": "chapter_1/gis_tips.html#tools-and-plugins",
    "href": "chapter_1/gis_tips.html#tools-and-plugins",
    "title": "Tips",
    "section": "Tools and plugins",
    "text": "Tools and plugins\nIn QGIS, georeferencing can be carried out using the built-in Georeferencer tool, which can be found under the menu Layer -&gt; Georeferencer.... This tool allows you to manually add control points that link the location on the image with geographic coordinates on the map, adjusting the image to fit the spatial data.\n\n\n\n\n\n\nTip\n\n\n\nThe more nodes you add and the further away from each other, the more refences the algorithm has to accomodate the original file. Minimum of 5 points should be considered to have an descent baseline.\n\n\n\nPlugins: For more advanced or specific georeferencing needs, there are additional plugins like Freehand Raster Georeferencer, allowing for more flexible manipulation of images directly within the map canvas.",
    "crumbs": [
      "GIS Tools",
      "Tips"
    ]
  },
  {
    "objectID": "chapter_1/gis_tips.html#use-cases-of-georeferenced-images",
    "href": "chapter_1/gis_tips.html#use-cases-of-georeferenced-images",
    "title": "Tips",
    "section": "Use cases of georeferenced images",
    "text": "Use cases of georeferenced images\n\nHistorical Maps: Bringing old maps into modern GIS projects, helping to study changes over time.\nTracing Exercises: Overlaying aerial or satellite imagery to digitize features such as roads, buildings, or land use.\nReshaping sketches: Planning and ideation sketches usually are our of scale and conveniently reshaping elements of the environment. Georefereincing them helps to better transition ideas into reality.\n\n\n\n\n\n\n\nIt’s important to consider potential distortions when georeferencing images, as the process involves stretching or rotating the image to align with real-world coordinates. Understanding and managing these distortions is key to ensuring the accuracy of the geospatial analysis.",
    "crumbs": [
      "GIS Tools",
      "Tips"
    ]
  },
  {
    "objectID": "chapter_1/gis_tips.html#demo",
    "href": "chapter_1/gis_tips.html#demo",
    "title": "Tips",
    "section": "Demo …",
    "text": "Demo …",
    "crumbs": [
      "GIS Tools",
      "Tips"
    ]
  },
  {
    "objectID": "chapter_1/sql.html",
    "href": "chapter_1/sql.html",
    "title": "SQL for GIS",
    "section": "",
    "text": "This is a draft outline and will be fleshed-out closer to class. To make visible, edit _quarto.yml to include in directory.\n\n\nSQL and PostgreSQL Basics:\n\n\nUnderstanding SQL (Structured Query Language)\nIntroduction to PostgreSQL as a Relational Database Management System (RDBMS)\nWorking with Schemas in PostgreSQL\n\n\nData Manipulation with SQL and PostgreSQL:\n\n\nCREATE, SELECT, INSERT, DELETE Statements\nPrimary Keys (PK) and Foreign Keys (FK)\nCreating and Managing Indexes for Performance Optimization\n\n\nSQL Concepts:\n\n\nAggregations: COUNT, SUM, AVG, MIN, MAX\nComplex Queries and Subqueries WITH, WHERE\nConditional Statements: CASE Statements\n\n\nIntroduction to GIS Integration with PostgreSQL:\n\n\nSpatial Data Types in PostgreSQL\nSpatial Indexing for Efficient Spatial Queries\nSpatial Functions and Operators in PostgreSQL\n\n\nSpatial and Non-Spatial Joins:\n\n\nPerforming Spatial Joins for GIS Analysis\nNon-Spatial Joins for Relational Data Analysis\nCombining Spatial and Non-Spatial Data in Queries\n\n\nHow to ask ChatGPT\n\n\nIterating query development\nDebugging"
  },
  {
    "objectID": "chapter_1/sql.html#key-topics",
    "href": "chapter_1/sql.html#key-topics",
    "title": "SQL for GIS",
    "section": "Key Topics",
    "text": "Key Topics\n\nIntroduction to Data Analysis and Databases:\n\n\nImportance of Data Analysis in Decision Making\nOverview of Relational Databases\n\n\nConnecting to Google Cloud Platform (GCP):\n\n\nConnecting to NFI’s database(s)\nSetting Up a GCP Account\n\n\nSQL and PostgreSQL Basics:\n\n\nUnderstanding SQL (Structured Query Language)\nIntroduction to PostgreSQL as a Relational Database Management System (RDBMS)\nWorking with Schemas in PostgreSQL\n\n\nData Manipulation with SQL and PostgreSQL:\n\n\nCREATE, SELECT, INSERT, DELETE Statements\nPrimary Keys (PK) and Foreign Keys (FK)\nCreating and Managing Indexes for Performance Optimization\n\n\nSQL Concepts:\n\n\nAggregations: COUNT, SUM, AVG, MIN, MAX\nComplex Queries and Subqueries WITH, WHERE\nConditional Statements: CASE Statements\n\n\nIntroduction to GIS Integration with PostgreSQL:\n\n\nSpatial Data Types in PostgreSQL\nSpatial Indexing for Efficient Spatial Queries\nSpatial Functions and Operators in PostgreSQL\n\n\nSpatial and Non-Spatial Joins:\n\n\nPerforming Spatial Joins for GIS Analysis\nNon-Spatial Joins for Relational Data Analysis\nCombining Spatial and Non-Spatial Data in Queries"
  },
  {
    "objectID": "chapter_1/sql.html#software-needed",
    "href": "chapter_1/sql.html#software-needed",
    "title": "SQL for GIS",
    "section": "Software needed",
    "text": "Software needed\n\nFor this session please make sure you have downloaded the following software(s):\n\nTablePlus | Modern, Native Tool for Database Management."
  },
  {
    "objectID": "chapter_1/sql.html#connecting-to-data-on-postgis",
    "href": "chapter_1/sql.html#connecting-to-data-on-postgis",
    "title": "SQL for GIS",
    "section": "Connecting to data on PostGIS 👨🏽‍💻",
    "text": "Connecting to data on PostGIS 👨🏽‍💻\n\n\n\n⚙\nCONNECTION SETTINGS\n\n\n\n\nIP Address\n(will be provided during class)\n\n\nDatabase names\nnfi / urban_analytics\n\n\nPort\n5432\n\n\nUsername\nscholar\n\n\nPassword\n(will be provided during class)\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote that the connections to the Databases are restricted by the source IP address."
  },
  {
    "objectID": "chapter_1/sql.html#to-dos",
    "href": "chapter_1/sql.html#to-dos",
    "title": "SQL for GIS",
    "section": "To-Dos",
    "text": "To-Dos\n\nCreate a table with the following characteristics.\nUse schema. ua0\nTable name : [yourinitials] + “_prof_exp”\nFields required: (assign data types as you think would be best)\n\nid\nscholar_id\nindustry_id\ninstitution\nyears_of_experience\neducation\nlocation (crs: 4326)\nend_year\nnotes\n\n** If you think more fields are necessary, feel free to add them\nCreate a GitHub account\n\nadd link to public.scholars.github_link (varchar)\n\n\n\nOptional\nCreate a local instance of PostGIS for use in the upcoming months\nLearn PostgreSQL Tutorial - Full Course for Beginners"
  },
  {
    "objectID": "chapter_1/sql.html#useful-resources",
    "href": "chapter_1/sql.html#useful-resources",
    "title": "SQL for GIS",
    "section": "Useful Resources",
    "text": "Useful Resources\n\nPOSTGRESQL & POSTGIS CHEATSHEETS\n\nPostgreSQL & PostGIS cheatsheet (a work in progress)\nPostgres Cheatsheet"
  },
  {
    "objectID": "chapter_1/gis.html",
    "href": "chapter_1/gis.html",
    "title": "Intro to GIS",
    "section": "",
    "text": "Vector and raster data are two fundamental types of spatial data used in Geographic Information Systems (GIS). Vector data represents geographical features as distinct shapes using points, lines, and polygons, each of which can carry detailed attribute information, making it ideal for precise mapping applications like boundaries, roads, and infrastructure networks. It excels in accuracy and detail for discrete data. In contrast, raster data consists of a grid of pixels, each holding a value, effectively representing continuous data. This format is well-suited for environmental analysis, land cover studies, and satellite imagery, as it captures variations over a wide area, such as elevation, temperature, or vegetation. While vector data is preferred for its precision in depicting specific geographic entities and managing related data, raster data is favored for its ability to represent complex, continuous phenomena over large spatial scales.\n\n\n\n\nVector data types are primarily categorized into Points, Linestrings, and Polygons, each with distinct characteristics and uses. Additionally, there are multi-geometry versions of these types, such as MultiPoints, MultiLinestrings, and MultiPolygons, which offer more complex feature groupings.\n\nPoints: Points are used to represent discrete locations on the earth’s surface. Each point is defined by a pair of coordinates (e.g. latitude and longitude, or an easting and northing) and can represent features such as the location of a tree or a landmark. They can have attributes further describing characteristics of interest, such as tree species or landmark name.\nLinestring: Lines connect a series of points to describe linear features such as roads or rivers. Linestring geometries inherently have length and may include attributes such as road type or river name.\nPolygons: Polygons are closed shapes used to represent areas such as city boundaries or land parcels. Polygon geometries inherently have an area and perimeter and may have attributes such as population counts or land use types.\n\n\n\n\nGeometry types\n\n\n\nMulti-Geometry Versions: Multi geometries provide a way to associated a grouping of Features which can share the same feature attributes.\n\nMultiPoint: For example, a grouping of trees.\nMultiLinestring: For example, a grouping of related segments describing a long-distance hiking trail.\nMultiPolygon: For example, a grouping of buildings which together represent a larger entity such as a hospital or university.\n\n\n\n\n\nEsri Shapefile\nShapefile is a prevalent geospatial file type in GIS software, requiring three mandatory files:\n\nSHP (feature geometry)\nSHX (shape index position)\nDBF (attribute data)\nShapefiles often also include optional files such as PRJ (coordinate reference system).\n\nGeoPackage\nGeoPackage provides a modern and simpler alternative to shapefiles and is rapidly gaining acceptance. It can store multiple data layers and it’s use of SQLite means that you can use SQL query data from the file.\nGeoJSON\nGeoJSON encodes geographic data structures using JSON, a format commonly used for web development.\n\nReference: https://gisgeography.com/gis-formats/\n\n\n\n\n\nRasters contain a grid of pixels, where the value used for each grid cell can represent a characteristic such as temperature or landcover. Geographic rasters typically include Geospatial information necessary to correctly project and locate the information in GIS software.\n\n\n\nRaster image grid cells\n\n\n\n\n\nASCII Grid\nASCII Grid files (ASC) store raster data as text in a grid format. Each cell in the grid is represented by a numeric value corresponding to the geographic attribute being mapped, such as elevation. ASCII Grid files include a header specifying metadata like cell size, number of rows and columns, and coordinates of the lower left corner. The format is simple and can be generated or edited with a text editor, and may use space, comma, or tab-delimiters.\nGeoTIFF\nGeoTIFF is widely becoming the defacto raster format in GIS. It embeds coordinate reference system information within a TIFF (Tagged Image File Format) file. GeoTIFFs are commonly used for storing satellite imagery, elevation data, and land surface temperatures.\n\nReference: https://gisgeography.com/gis-formats/",
    "crumbs": [
      "GIS Tools",
      "Intro to GIS"
    ]
  },
  {
    "objectID": "chapter_1/gis.html#file-types",
    "href": "chapter_1/gis.html#file-types",
    "title": "Intro to GIS",
    "section": "",
    "text": "Vector and raster data are two fundamental types of spatial data used in Geographic Information Systems (GIS). Vector data represents geographical features as distinct shapes using points, lines, and polygons, each of which can carry detailed attribute information, making it ideal for precise mapping applications like boundaries, roads, and infrastructure networks. It excels in accuracy and detail for discrete data. In contrast, raster data consists of a grid of pixels, each holding a value, effectively representing continuous data. This format is well-suited for environmental analysis, land cover studies, and satellite imagery, as it captures variations over a wide area, such as elevation, temperature, or vegetation. While vector data is preferred for its precision in depicting specific geographic entities and managing related data, raster data is favored for its ability to represent complex, continuous phenomena over large spatial scales.\n\n\n\n\nVector data types are primarily categorized into Points, Linestrings, and Polygons, each with distinct characteristics and uses. Additionally, there are multi-geometry versions of these types, such as MultiPoints, MultiLinestrings, and MultiPolygons, which offer more complex feature groupings.\n\nPoints: Points are used to represent discrete locations on the earth’s surface. Each point is defined by a pair of coordinates (e.g. latitude and longitude, or an easting and northing) and can represent features such as the location of a tree or a landmark. They can have attributes further describing characteristics of interest, such as tree species or landmark name.\nLinestring: Lines connect a series of points to describe linear features such as roads or rivers. Linestring geometries inherently have length and may include attributes such as road type or river name.\nPolygons: Polygons are closed shapes used to represent areas such as city boundaries or land parcels. Polygon geometries inherently have an area and perimeter and may have attributes such as population counts or land use types.\n\n\n\n\nGeometry types\n\n\n\nMulti-Geometry Versions: Multi geometries provide a way to associated a grouping of Features which can share the same feature attributes.\n\nMultiPoint: For example, a grouping of trees.\nMultiLinestring: For example, a grouping of related segments describing a long-distance hiking trail.\nMultiPolygon: For example, a grouping of buildings which together represent a larger entity such as a hospital or university.\n\n\n\n\n\nEsri Shapefile\nShapefile is a prevalent geospatial file type in GIS software, requiring three mandatory files:\n\nSHP (feature geometry)\nSHX (shape index position)\nDBF (attribute data)\nShapefiles often also include optional files such as PRJ (coordinate reference system).\n\nGeoPackage\nGeoPackage provides a modern and simpler alternative to shapefiles and is rapidly gaining acceptance. It can store multiple data layers and it’s use of SQLite means that you can use SQL query data from the file.\nGeoJSON\nGeoJSON encodes geographic data structures using JSON, a format commonly used for web development.\n\nReference: https://gisgeography.com/gis-formats/\n\n\n\n\n\nRasters contain a grid of pixels, where the value used for each grid cell can represent a characteristic such as temperature or landcover. Geographic rasters typically include Geospatial information necessary to correctly project and locate the information in GIS software.\n\n\n\nRaster image grid cells\n\n\n\n\n\nASCII Grid\nASCII Grid files (ASC) store raster data as text in a grid format. Each cell in the grid is represented by a numeric value corresponding to the geographic attribute being mapped, such as elevation. ASCII Grid files include a header specifying metadata like cell size, number of rows and columns, and coordinates of the lower left corner. The format is simple and can be generated or edited with a text editor, and may use space, comma, or tab-delimiters.\nGeoTIFF\nGeoTIFF is widely becoming the defacto raster format in GIS. It embeds coordinate reference system information within a TIFF (Tagged Image File Format) file. GeoTIFFs are commonly used for storing satellite imagery, elevation data, and land surface temperatures.\n\nReference: https://gisgeography.com/gis-formats/",
    "crumbs": [
      "GIS Tools",
      "Intro to GIS"
    ]
  },
  {
    "objectID": "chapter_1/gis.html#coordinate-reference-systems",
    "href": "chapter_1/gis.html#coordinate-reference-systems",
    "title": "Intro to GIS",
    "section": "Coordinate Reference Systems",
    "text": "Coordinate Reference Systems\nGeographic data is intrinsically tied to the earth’s surface and is represented using coordinates within defined Coordinate Reference Systems (CRS). Broadly, these systems fall into two categories: Geographic Coordinate Systems and Projected Coordinate Systems.\nGeographic CRSs use a spherical surface to define locations on the earth’s surface. They are based on degrees in latitude and longitude and are most commonly referenced to the WGS84 datum. While they accurately represent the earth’s curved surface and preserve the true shape and angular relationships of features, distances and areas may not be as accurate, especially over larger distances.\nProjected CRSs translate the earth’s three-dimensional surface onto a two-dimensional plane, making it easier to analyze and visualize spatial data. They use a mathematical projection to flatten the curved surface of the earth, which can lead to distortions in shape, area, distance, or direction. Different types of projections are designed to minimize these distortions for specific regions or purposes. For example, projections can be optimized for preserving areas (equal-area projections), angles and shapes (conformal), or distances (equidistant).\n\n\n\nDymaxion Fuller projection. NFF image rights.\n\n\n\n\n\n\n\n\nTip\n\n\n\nA useful reference website for different CRS is EPSG.io: Coordinate Systems Worldwide\n\n\n\nGeographic\nGeographic Coordinate Reference Systems (CRS) use a spherical or ellipsoidal surface to represent locations on the Earth’s curved surface. Geographic CRSs are based on latitude and longitude coordinates and are useful for global or regional mapping.\n\nUnits: Degrees\nCommon Examples:\n\nWGS 84 (World Geodetic System 1984) EPSG:4326: A widely used CRS for global mapping and GPS navigation.\nNAD83 (North American Datum 1983) EPSG:4269: A CRS commonly used in North America.\n\n\n\n\nProjected\nProjected Coordinate Reference Systems (CRS) use a Cartesian coordinate system to represent locations on a flat surface. These CRSs are created by “projecting” the curved Earth onto a flat surface, which introduces distortions. This inevitably requires tradeoffs in distances, areas, or shapes, with different projection systems involving different trade-offs. Projected CRSs are commonly used for local or small-scale mapping appropriate to specific countries or regions of the globe, where locally accurate measurements and comparisons are required.\n\nUnits: Meters (most commonly)\nCommon Examples:\n\nLAEA (Lambert Azimuthal Equal Area) Europe projection EPSG:3035: This CRS is commonly used for Europe, though note that countries will typically also have even more localised projected CRS.\nBritish National Grid (BNG) EPSG:27700: This CRS is commonly used in the United Kingdom.\nState Plane Coordinate System (SPCS): This CRS is widely used in the United States, and divides the country into zones, each with their own projection.\n\n\n\n\nSome visual examples\n\n\nhttps://gisgeography.com",
    "crumbs": [
      "GIS Tools",
      "Intro to GIS"
    ]
  },
  {
    "objectID": "chapter_1/gis.html#gis-data",
    "href": "chapter_1/gis.html#gis-data",
    "title": "Intro to GIS",
    "section": "GIS Data",
    "text": "GIS Data\n\nSources\nDiscovering sources for GIS (Geographic Information System) data is a handy skill for spatial analysis and mapping! There are various official and non-official sources, each offering different types of data suited to specific needs and applications. When searching for and using different data sources, try to remain cognisant of the merits and limitations of different sources, which may impact considerations such as data accuracy or bias.\n\nGovernment Agencies: National agencies often provide official and reliable forms of GIS data. However, these datasets may reflect governmental priorities and perspectives or they may be reluctant to make these datasets openly available.\nInternational Organizations: Organizations such as the United Nations (UN) and the World Bank offer global GIS datasets, but these may have limited local detail due to their broad scope.\nLocal Authorities: City and regional councils often create and maintain local GIS datasets, often tailored to specific local government projects and priorities. Their local specificity may make it tricky to compare to different datasets sourced from other cities.\nPrivate Sector: There are many private sector organisations which generate, curate, or distribute data, however, these datasets are less likely to be openly available and may incur substantial monetary cost.\nOpen Data Platforms: Platforms such as OpenStreetMap offer freely accessible GIS data, contributed by a community of users. These datasets may require additional validation if used for mission-critical purposes, though can otherwise be extremely useful, allowing for generalisable workflows and providing coverage in some instances where no other options exist.\nAPIs and Web Scraping: Involves the automated collection of data from online APIs or websites. The quality of data can vary but may otherwise be useful for obtaining interesting data. These techniques may involve significant technical skills for data extraction and processing. Legal and ethical considerations are important, as scraping can sometimes violate terms of service or data privacy laws.\n\n\n\nPurposely Collected Data\nPurposely collected data is gathered specifically to address a certain research question or project goal, using structured approaches such as surveys, experiments, or observations. It’s tailored to meet the specific needs of a study, with careful control over variables to ensure the data’s relevance and accuracy. This type of data is highly pertinent to the project it’s collected for, due to its focused nature and methodical collection process.\n\n\nAdapted Data\nOn the other hand, data can also be adapted to new research needs from information originally collected for different purposes. This may include repurposing data from various sources such as public databases, existing research studies, or web scraping.",
    "crumbs": [
      "GIS Tools",
      "Intro to GIS"
    ]
  },
  {
    "objectID": "chapter_1/gis.html#attribution",
    "href": "chapter_1/gis.html#attribution",
    "title": "Intro to GIS",
    "section": "Attribution",
    "text": "Attribution\nAttribution or citation refers to giving proper credit to the original source or creator of the data. Just as with citing sources in academic papers, it’s important to acknowledge the origin of the data you are using. This helps provide transparency, accountability, and recognition to the creators of the dataset.\n\nLicenses:\nWhen dealing with data, licenses refer to legal agreements or permissions that determine how the data can be used, distributed, or modified. Different datasets may have different licensing requirements, so it’s important to understand and comply with the specific license terms associated with the data you are using.\nExamples:\n\nCreative Commons (CC) licenses: These are standardized licenses that allow creators to specify permissions for their work. They are commonly used for a variety of creative works, including images, music, videos, and written content.\nOpen Government Licenses: These licenses have been adopted by governments and may allow individuals and organizations permissive use, reuse, and sharing of information.\nGNU General Public License (GPL): This is a copyleft license for open-source software. It permits users to modify and distribute the software, but any modifications or derivative works must also be licensed under the GPL. This license is popular for promoting open-source principles and collaborative software development.\nMIT / Apache / BSD Licenses: Permissive open-source software licenses. These may allow great flexibility such as using, copying, modifying, and distributing derivative work, even for commercial purposes. The primary requirement is to include the original copyright and license notice in any significant portions of the software.",
    "crumbs": [
      "GIS Tools",
      "Intro to GIS"
    ]
  },
  {
    "objectID": "chapter_1/gis.html#privacy",
    "href": "chapter_1/gis.html#privacy",
    "title": "Intro to GIS",
    "section": "Privacy",
    "text": "Privacy\nPrivacy considerations are important when working with data, especially if the data contains personally identifiable information or sensitive information. It’s very important to handle and store data in a way that respects privacy laws and regulations. This may involve anonymizing or de-identifying data, implementing security measures, and obtaining consent from individuals whose data is being used.\n\n\n\n\n\n\nNote\n\n\n\nPlease avoid using any and all data containing private or sensitive data unless you first review this and seek approval from faculty. Handling these forms of data comes with numerous steps and safeguards and will generally be strongly discouraged.",
    "crumbs": [
      "GIS Tools",
      "Intro to GIS"
    ]
  },
  {
    "objectID": "chapter_1/gis.html#quirks---part-1",
    "href": "chapter_1/gis.html#quirks---part-1",
    "title": "Intro to GIS",
    "section": "Quirks - Part 1",
    "text": "Quirks - Part 1\n\nMAUP\nThe Modifiable Areal Unit Problem (MAUP) is encountered when using different scales or zones of analysis to group data. This can cause significant changes in the balance and aggregation of data based on how it is grouped. This can have a potentially significant impact on visualisation or analysis. MAUP is exploited for political purposes through the process of gerrymandering.\n\nhttps://gisgeography.com\n\n\n\nYouTube",
    "crumbs": [
      "GIS Tools",
      "Intro to GIS"
    ]
  },
  {
    "objectID": "chapter_1/gis.html#resources",
    "href": "chapter_1/gis.html#resources",
    "title": "Intro to GIS",
    "section": "Resources",
    "text": "Resources\nThe following resources provide further information on common types of map visualisations and best practices:\n\nCARTO - 5 Popular Thematic map Types\nFundamentals of Data Visualisation\n\nColor scales\nVisualizing geospatial data\nCommon pitfalls of color use\n\nColor Brewer",
    "crumbs": [
      "GIS Tools",
      "Intro to GIS"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NFI Urban Analytics 2024",
    "section": "",
    "text": "Welcome to Part 1 of the Urban Analytics course at NFI. This course is designed to empower scholars with the skills and knowledge to harness the power of urban analytics for analyzing and interpreting urban data.\nThis is a practical course and is best approached with a hands-on and explorative mindset. It consists of three parts:\n\nPart 1 includes information on general setup and provides an introduction to basic concepts. See the sidebar for the Sections.\nPart 2 will be released separately and delves into targeted themes and skills development.\nPart 3 will be framed around pilot cities specific workflows.\n\nWe hope you enjoy the course!\n\n\n\nThis course is will touch on a number of broad topics. High-level guidance will be provided and scholars will then be encouraged to further explore these approaches and develop their skillsets based on methods of interest or relevance to their work:\n\nSpatial Analysis: Exploring Geographic Information Systems, spatial data handling, and mapping urban data.\nPython: Introduction to Python and its usage for geospatial analysis.\nData Handling: Techniques for handling datasets and an introduction to data science methods.\nProject Work: Lab work tasks and workflows.\n\n\n\n\nYou have been added to a Google Spaces for online discussions. This platform is your primary hub for:\n\nSeeking peer assistance.\nDiscussing course materials and urban analytics topics.\nCollaborating on projects and assignments.\nAnnouncements and updates from the course team.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#ua-part-1",
    "href": "index.html#ua-part-1",
    "title": "NFI Urban Analytics 2024",
    "section": "",
    "text": "Welcome to Part 1 of the Urban Analytics course at NFI. This course is designed to empower scholars with the skills and knowledge to harness the power of urban analytics for analyzing and interpreting urban data.\nThis is a practical course and is best approached with a hands-on and explorative mindset. It consists of three parts:\n\nPart 1 includes information on general setup and provides an introduction to basic concepts. See the sidebar for the Sections.\nPart 2 will be released separately and delves into targeted themes and skills development.\nPart 3 will be framed around pilot cities specific workflows.\n\nWe hope you enjoy the course!",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#topics",
    "href": "index.html#topics",
    "title": "NFI Urban Analytics 2024",
    "section": "",
    "text": "This course is will touch on a number of broad topics. High-level guidance will be provided and scholars will then be encouraged to further explore these approaches and develop their skillsets based on methods of interest or relevance to their work:\n\nSpatial Analysis: Exploring Geographic Information Systems, spatial data handling, and mapping urban data.\nPython: Introduction to Python and its usage for geospatial analysis.\nData Handling: Techniques for handling datasets and an introduction to data science methods.\nProject Work: Lab work tasks and workflows.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#communication",
    "href": "index.html#communication",
    "title": "NFI Urban Analytics 2024",
    "section": "",
    "text": "You have been added to a Google Spaces for online discussions. This platform is your primary hub for:\n\nSeeking peer assistance.\nDiscussing course materials and urban analytics topics.\nCollaborating on projects and assignments.\nAnnouncements and updates from the course team.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "chapter_1/qgis.html",
    "href": "chapter_1/qgis.html",
    "title": "Intro to QGIS",
    "section": "",
    "text": "We’ll first explore the QGIS user interface, taking some time to explore:\n\nMain Interface: The map area, toolbars, layers panel, status bar, dock panels.\nToolbars: These are collections of icons and tools that provide quick access to commonly used features and functions in QGIS. These can be customised based on your preferences. Some of the common toolbars provide functionality such as zooming, panning, selecting features, and editing features.\nMap View: This is the central area where your maps and layers are displayed. We’ll explore how to navigate the map view, including zooming, panning, and identifying features.\nBrowser Panel: For browsing and loading data from your file system, databases, and web services.\nLayers Panel: Here, you can see a list of all the layers loaded in your project. Layers can be organised, styled, and otherwise configured.\nAttribute Table: This panel lets you view and edit the attribute data associated with your vector layers.\nStatus Bar: Located at the bottom, the status bar shows information about the map and your project, such as the current map scale, the coordinate reference system, and messaging about ongoing processes.\nPlugins: QGIS can be extended with plugins. We’ll briefly touch on how to find, install, and use plugins to enhance the functionality of QGIS.",
    "crumbs": [
      "GIS Tools",
      "Intro to QGIS"
    ]
  },
  {
    "objectID": "chapter_1/qgis.html#qgis-ui",
    "href": "chapter_1/qgis.html#qgis-ui",
    "title": "Intro to QGIS",
    "section": "",
    "text": "We’ll first explore the QGIS user interface, taking some time to explore:\n\nMain Interface: The map area, toolbars, layers panel, status bar, dock panels.\nToolbars: These are collections of icons and tools that provide quick access to commonly used features and functions in QGIS. These can be customised based on your preferences. Some of the common toolbars provide functionality such as zooming, panning, selecting features, and editing features.\nMap View: This is the central area where your maps and layers are displayed. We’ll explore how to navigate the map view, including zooming, panning, and identifying features.\nBrowser Panel: For browsing and loading data from your file system, databases, and web services.\nLayers Panel: Here, you can see a list of all the layers loaded in your project. Layers can be organised, styled, and otherwise configured.\nAttribute Table: This panel lets you view and edit the attribute data associated with your vector layers.\nStatus Bar: Located at the bottom, the status bar shows information about the map and your project, such as the current map scale, the coordinate reference system, and messaging about ongoing processes.\nPlugins: QGIS can be extended with plugins. We’ll briefly touch on how to find, install, and use plugins to enhance the functionality of QGIS.",
    "crumbs": [
      "GIS Tools",
      "Intro to QGIS"
    ]
  },
  {
    "objectID": "chapter_1/qgis.html#plugins",
    "href": "chapter_1/qgis.html#plugins",
    "title": "Intro to QGIS",
    "section": "Plugins",
    "text": "Plugins\nInstall the Quick Map Services (QMS) Plugin:\n\nFrom the Plugins menu, select Manage and Install Plugins\nSearch for QMS\nInstall QuickMapServices\nInstall and Close the window.\n\nThis plugin will add a new menu entry under the Web menu.\n\nFrom the Web menu, select QuickMapServices, then OSM and OSM Standard.\nThis will add a map to your map view.",
    "crumbs": [
      "GIS Tools",
      "Intro to QGIS"
    ]
  },
  {
    "objectID": "chapter_1/qgis.html#xyz-tiles",
    "href": "chapter_1/qgis.html#xyz-tiles",
    "title": "Intro to QGIS",
    "section": "XYZ Tiles",
    "text": "XYZ Tiles\nXYZ tiles are a popular way to represent map tile data in a simple and standardized format, allowing for efficient storage, retrieval, and display of map imagery at various scales. The “XYZ” refers to the coordinate system used to locate the tiles. “X” and “Y” represent the tile’s column and row, respectively, in a grid layout, with the origin typically at the top-left corner of the map. The “Z” parameter stands for zoom level, where higher zoom levels correspond to more detailed views (i.e., a higher resolution). This system enables GIS applications to quickly load the appropriate level of detail for a given view, making it a cornerstone of web mapping services like Google Maps and OpenStreetMap. It allows for seamless and efficient navigation across various scales and locations on a digital map.\nTo establish a new connection:\n\nOn the Browser panel, right-click on XYZ Tiles -&gt; New Connection...\nAsign a name to the connection (eg. OpenStreetMap)\nPaste the respective URL\n\nDepending on the connection, some of other connection attributes might be required (eg. Authentication details)\n\nWe reccomend you establish at least the following connections: - OpenStreetMap : https://tile.openstreetmap.org/{z}/{x}/{y}.png - Google Imagery Hybrid : http://mt0.google.com/vt/lyrs=y&hl=en&x={x}&y={y}&z={z}&s=Ga\n\n\n\n\n\n\n\nTip\n\n\n\nUseful list of many others Tiles. https://qms.nextgis.com Note, make sure you navigate to the TMS section and the reported status = works (symbolised with the green dot).",
    "crumbs": [
      "GIS Tools",
      "Intro to QGIS"
    ]
  },
  {
    "objectID": "chapter_1/qgis.html#datasets",
    "href": "chapter_1/qgis.html#datasets",
    "title": "Intro to QGIS",
    "section": "Datasets",
    "text": "Datasets\n\nDownload the municipal neighbourhoods as a SHP file: Barrios municipales de Madrid\nDownload the population by district and neighbourhood dataset as a CSV file: Población por distrito y barrio a 1 de enero",
    "crumbs": [
      "GIS Tools",
      "Intro to QGIS"
    ]
  },
  {
    "objectID": "chapter_1/qgis.html#coordinate-reference-systems",
    "href": "chapter_1/qgis.html#coordinate-reference-systems",
    "title": "Intro to QGIS",
    "section": "Coordinate Reference Systems",
    "text": "Coordinate Reference Systems\nGeospatial data is represented using a coordinate reference system (CRS). QGIS will ordinarily detect this information automatically. However, it can display information in any selected CRS, so you can open multiple files in different CRSs and these will display correctly. This is because QGIS will reproject each respective file into the map’s currently selected CRS system.\nIn the lower right corner of the status bar at the bottom of the screen:\n\nSelect the CRS button to open the project’s CRS configuration.\nSearch for CRS code 3857, then select it, Apply, and OK.\n\n\nHint: When using OSM base maps, the base maps will render more performantly in the Web Mercator projection (3857). This is because QGIS then doesn’t need to dynamically reproject the map tiles out of their native 3857 projection.",
    "crumbs": [
      "GIS Tools",
      "Intro to QGIS"
    ]
  },
  {
    "objectID": "chapter_1/qgis.html#loading-data",
    "href": "chapter_1/qgis.html#loading-data",
    "title": "Intro to QGIS",
    "section": "Loading Data",
    "text": "Loading Data\nLoad your first dataset by dragging and dropping the shapefile (the downloaded file ending with .shp) into your map view.\n\nYou can toggle layer visibility and order using the Layers Panel.\nUse the Select Features button to select a feature and see it highlighted.\nClick the Pan Map to Selection button to see the map zoom to the currently selected feature\nClick the Zoom Full button to return to a view of the full dataset.\nUse the Identify Features button to see attribute information for a particular feature.\nClick the Toggle Editing button -&gt; this will let you delete existing features or add new features - use with caution! Don’t make any changes for now, just click the Toggle Editing button again to exit edit mode.",
    "crumbs": [
      "GIS Tools",
      "Intro to QGIS"
    ]
  },
  {
    "objectID": "chapter_1/qgis.html#saving-and-exporting-data",
    "href": "chapter_1/qgis.html#saving-and-exporting-data",
    "title": "Intro to QGIS",
    "section": "Saving and exporting data",
    "text": "Saving and exporting data\nWe don’t want to edit the original dataset, so let’s make a copy and work with that instead:\n\nRight click the layer in the Layer Panel and select export -&gt; Save Features as\nSelect the GeoPackage format.\nSelect a file location on disk and give it a distinctive name such as my_gpkg.\nSelect 3035 for the CRS projection\nCheck that Add saved file to map at the bottom of the window is selected\nSelect OK\nThe file will be saved and will automatically load into your map.\nRemove the original SHP file from your Layers Panel since it is no longer needed. This can be done with the Remove Layer button.",
    "crumbs": [
      "GIS Tools",
      "Intro to QGIS"
    ]
  },
  {
    "objectID": "chapter_1/qgis.html#attributes",
    "href": "chapter_1/qgis.html#attributes",
    "title": "Intro to QGIS",
    "section": "Attributes",
    "text": "Attributes\nAttributes in GIS layers provide additional information about the spatial features in the data. They include text, numbers, dates, or images. Attributes help add context to the geographic elements, enabling better analysis and visualization. They can be used to filter, classify, and symbolize features based on their characteristics. Attributes play a crucial role in data analysis, allowing us to gain insights, make comparisons, and create meaningful visual representations.\nUsing the Municipal Boundaries my_gpkg dataset:\n\nOpen the attribute table for the neighbourhood boundaries by right clicking on the layer in the Layers Panel and selecting Attribute Table. From the Attribute table you can try the same steps you tried for the map view (selecting, zooming to, editing).\nToggle Editing Mode by pressing the Toggle Editing Mode button.\nClick on the New Field button in the toolbar\nSet the Name as area and the type as Decimal number, then OK.\nCheck that no rows are selected (otherwise the calculation only applies to the selected rows).\nOpen the Field Calculator by clicking its icon (abacus symbol) in the toolbar.\nSelect Update existing field and select the area column.\nIn the Expression pane, enter $area, then press the blue forward button below to preview the results of the calculation.\nPress OK and check that your area column has been updated.\nPress the Save edits button to save your calculation.\nTurn off editing mode.\n\nWe’ll now add another column, this time we’ll use another column as input and we will transform the data from a text to an integer.\n\nTurn editing mode on\nOpen the Field Calculator\nThis time select Create a new field and call it cod_barrio\nCheck that it is an Integer type\nIn the expression, enter to_int(COD_BAR) -&gt; this will take the text from the COD_BAR column and will cast this to an integer. In the process, the data will drop the leading zero.\nApply the change, save to disk, and toggle editing to off.",
    "crumbs": [
      "GIS Tools",
      "Intro to QGIS"
    ]
  },
  {
    "objectID": "chapter_1/qgis.html#loading-csv-files",
    "href": "chapter_1/qgis.html#loading-csv-files",
    "title": "Intro to QGIS",
    "section": "Loading CSV files",
    "text": "Loading CSV files\n\nTo load the CSV file we’ll go to the Layer menu bar, Add Layer, then Add Delimited Text Layer.\nFor File name, navigate to your CSV file and open it. If the load window dissapears (on a Mac), it might be hiding behind the main application window.\nThis file uses semi-colon delimeters, so select Custom delimiters and check that Semicolon is selected\nCheck that the option for First record has field names is selected. This will automatically use the first line of the CSV to set the attribute names.\nSelect No geometry. This will load the dataset without visual geometric information. Note that if you have longitude and latitude information in CSV columns (or Eastings and Northings for projected CRS), then you can specify these columns and QGIS will generate the point geometries accordingly.\nSelect Add then Close.\nNote that this layer looks different in the Layers Panel because it doesn’t have associated geometry. However, you can still view the Attribute Table.\nRight click on the layer and select export -&gt; Save Features as. Then save the layer as a new CSV file called my_csv. As before, select the option to add the layer to the map, save, then remove the original input CSV from the Layers Panel.",
    "crumbs": [
      "GIS Tools",
      "Intro to QGIS"
    ]
  },
  {
    "objectID": "chapter_1/qgis.html#selecting-and-deleting-features",
    "href": "chapter_1/qgis.html#selecting-and-deleting-features",
    "title": "Intro to QGIS",
    "section": "Selecting and deleting features",
    "text": "Selecting and deleting features\n\nOpen the attribute table for the my_csv layer by right clicking on the data table in the Layers Panel and selecting Attribute Table.\nOrder the data by clicking the fecha column header, order in descending order by clicking again.\nClick on the first entry (which should be 1 de enero de 2023) then hold down shift and click on the last entry with a fecha value of 1 de enero de 2023.\nClick the Invert selection button, this will invert the selection so you only have entries selected that are not 1 de enero de 2023.\nClick the Delete selected features button.\nManually select the row with a cod_distrito value of Todos, and delete it, since this is an aggregated row containing aggregated information for all districts.\nOpen the Field Calculator, and this time select to create a new field called num_people as a Decimal type.\nUse the following expression to populate the field: to_real(\"num_personas\") - this will take the original num_personas column and will convert it from text format to numeric format, and writes it to a new column called num_people. We can then use this numeric column for map styling.\nSave and turn off editing.",
    "crumbs": [
      "GIS Tools",
      "Intro to QGIS"
    ]
  },
  {
    "objectID": "chapter_1/qgis.html#joins",
    "href": "chapter_1/qgis.html#joins",
    "title": "Intro to QGIS",
    "section": "Joins",
    "text": "Joins\n\nOpen the Attribute Panels for both the my_gpkg and the my_csv layers.\nWe want to “Join” information from the CSV dataset to the neighbourhood boundaries using the matching identifiers in the respective cod_barrio / cod_barrio columns. We are now just visually inspecting the data, so you can go ahead and close the Attribute Panels when done.\nDouble click on the my_gpkg layer to open the properties panel.\nClick Joins in the left sidebar.\nClick the plus button to create a new join.\nSelect the my_csv table for the Join layer -&gt; the layer from which attributes will be joined to my_gpkg.\nSelect the cod_barrio column for the Join field - this refers to the cod_barrio column in the my_csv file.\nSelect the cod_barrio column for the Target field – this refers to the cod_barrio column in the my_gpkg file.\nClick OK and close the layer properties. This will “weld” the my_csv columns onto the matching my_gpkg geometries so that we can view the census information using their geographic boundaries.\nReopen the Attribute Panel for the my_gpkg dataset, you’ll see extra columns which have now joined the data from the my_csv table into their corresponding boundaries based on the matching identifiers.\nLet’s export this to a new file to my_joined_gpkg, which will now include the joined data.\nRemove my_csv and my_gpkg from the Layers Panel.",
    "crumbs": [
      "GIS Tools",
      "Intro to QGIS"
    ]
  },
  {
    "objectID": "chapter_1/qgis.html#visualisation",
    "href": "chapter_1/qgis.html#visualisation",
    "title": "Intro to QGIS",
    "section": "Visualisation",
    "text": "Visualisation\n\nOpen the my_joined_gpkg layer properties by double clicking on the layer name in the Layers Panel.\nSelect Labels in the left pane, select Single Labels, then select the NOMBRE column, then apply. This will add the Barrio names to the map. You can format labels to your heart’s content.\nSelect Symbology in the left pane. You can try use the “graduated” view option (select from the top drop-down) to view the area column information (select from the “Value” drop-down) or the csv_num_people column information on your map.\nYou can use different “Mode” options - press the Classify button to see how they detect different thresholds for the colour bands.\nPress Apply to see the changes on the map.\nThere are almost limitless options so feel free to be creative.\n\n\n\n\nMadrid’s 2023 population distribution - Greg Maya",
    "crumbs": [
      "GIS Tools",
      "Intro to QGIS"
    ]
  },
  {
    "objectID": "chapter_1/qgis.html#resources",
    "href": "chapter_1/qgis.html#resources",
    "title": "Intro to QGIS",
    "section": "Resources",
    "text": "Resources\nIf you are feeling like doing a little more you can look at the QGIS mapping section and/or any of the following online materials. There is a lot that can be done with QGIS and we have just skimmed the surface!\nThe QGIS Tutorials and Tips is a great resource for QGIS.\nFor lab-work, please do the following tutorials from QGIS Tutorials and Tips in your own time to build some general familiarity with QGIS:\n\nMaking a Map\nBasic Vector Styling\nDigitizing Map Data\nWorking with Projections\nCreating Heatmaps",
    "crumbs": [
      "GIS Tools",
      "Intro to QGIS"
    ]
  },
  {
    "objectID": "chapter_1/qgis_mapping.html",
    "href": "chapter_1/qgis_mapping.html",
    "title": "QGIS Mapping",
    "section": "",
    "text": "We’ll first start with another statistical quirk!\n\n\nAn interesting conundrum arises in statistical analysis when relationships between variables for grouped data are assumed to be valid for the individuals within those groupings. Individual level information is lost in the process of grouping information into larger aggregations, and statistical observations computed for these larger groupings may therefore be different, or potentially misleading in the context of individuals. As a general rule, use individual level (unaggregated) data or the smallest groupings of data (e.g. census statistics) available.\nHere is some background reading, with some examples:\n\nWikipedia: Ecological Correlations\nBritannica: Ecological Fallacy\nThe ecological correlations fallacy",
    "crumbs": [
      "GIS Tools",
      "QGIS Mapping"
    ]
  },
  {
    "objectID": "chapter_1/qgis_mapping.html#quirks---part-2",
    "href": "chapter_1/qgis_mapping.html#quirks---part-2",
    "title": "QGIS Mapping",
    "section": "",
    "text": "We’ll first start with another statistical quirk!\n\n\nAn interesting conundrum arises in statistical analysis when relationships between variables for grouped data are assumed to be valid for the individuals within those groupings. Individual level information is lost in the process of grouping information into larger aggregations, and statistical observations computed for these larger groupings may therefore be different, or potentially misleading in the context of individuals. As a general rule, use individual level (unaggregated) data or the smallest groupings of data (e.g. census statistics) available.\nHere is some background reading, with some examples:\n\nWikipedia: Ecological Correlations\nBritannica: Ecological Fallacy\nThe ecological correlations fallacy",
    "crumbs": [
      "GIS Tools",
      "QGIS Mapping"
    ]
  },
  {
    "objectID": "chapter_1/qgis_mapping.html#mapping",
    "href": "chapter_1/qgis_mapping.html#mapping",
    "title": "QGIS Mapping",
    "section": "Mapping",
    "text": "Mapping\nAs part of our practical exercises in class, we will focus on the essential skill of drawing street networks using vector data mapping. This exercise is centered on the historic center of San Marino and is designed to enhance your proficiency in inputting geometry. The main task will involve creating a GeoPackage Layer:\n\nStreet Network layer: representing the pedestrian routes through the assigned tiles.\n\nFor those interested in exploring further and applying additional GIS skills, we offer the optional task of creating a Buildings layer:\n\nBuildings layer (Optional): representing all the buildings within the assigned tiles.\n\nThis approach allows you to focus on the critical aspect of mapping the street network, with the option to delve into building mapping as an additional challenge.",
    "crumbs": [
      "GIS Tools",
      "QGIS Mapping"
    ]
  },
  {
    "objectID": "chapter_1/qgis_mapping.html#connecting-to-db",
    "href": "chapter_1/qgis_mapping.html#connecting-to-db",
    "title": "QGIS Mapping",
    "section": "Connecting to DB",
    "text": "Connecting to DB\nAlthough the exercise is primarily based on local data manipulation, connecting to NFI’s database is recommended to access the reference grid and familiarize yourself with its structure. The database is read-only for scholars, ensuring no accidental modifications.\n\n\n\n\n\n\nTip\n\n\n\nConnection details will be provided in class. All connections to the database are IP protected.",
    "crumbs": [
      "GIS Tools",
      "QGIS Mapping"
    ]
  },
  {
    "objectID": "chapter_1/qgis_mapping.html#mapping-vector-data",
    "href": "chapter_1/qgis_mapping.html#mapping-vector-data",
    "title": "QGIS Mapping",
    "section": "Mapping Vector Data",
    "text": "Mapping Vector Data\nWe will use the historic center of San Marino for this exercise, aiming to familiarize you with inputting geometry. Attributes will be considered at later stages. Each scholar can produce individual GeoPackage Layers (locally):\n\nStreet Network layer (Linestring, CRS: 3035) is the primary focus, emphasizing the mapping of pedestrian routes.\n\nSuggested name: network_[scholarname]\n\nBuildings layer (Polygons, CRS: 3035) for those who wish to further challenge themselves by mapping the area’s buildings.\n\nSuggested name: buildings_[scholarname]\n\n\nScholars will refer to specific areas of the republic found in the nfi database path: - nfi.sanmarino.base_mapping\nExplore the street_network attribute to see your areas and collaborate with neighboring tiles.",
    "crumbs": [
      "GIS Tools",
      "QGIS Mapping"
    ]
  },
  {
    "objectID": "chapter_1/qgis_mapping.html#contextual-resources-for-mapping",
    "href": "chapter_1/qgis_mapping.html#contextual-resources-for-mapping",
    "title": "QGIS Mapping",
    "section": "Contextual Resources for Mapping",
    "text": "Contextual Resources for Mapping\nUse OSM or Bing maps XYZ Tiles, as detailed in our previous lesson, for a better understanding of your mapping area. The 3D CESIUM Story offers a 3D view of the 2D polygons you’ll be mapping, enhancing your perspective and accuracy.\n\nOSM tile: https://tile.openstreetmap.org/{z}/{x}/{y}.png\nBing tile URL : https://t0.tiles.virtualearth.net/tiles/a{q}.jpeg?g=685&mkt=en-us&n=z\n\n\n\n\n\n\n\nTip\n\n\n\nActivate the Snapping Toolbar for precision in mapping. Experiment with different pixel tolerance values and options like Topological Editing and Intersection Snapping.",
    "crumbs": [
      "GIS Tools",
      "QGIS Mapping"
    ]
  },
  {
    "objectID": "chapter_1/qgis_mapping.html#contextual-resources-for-mapping-1",
    "href": "chapter_1/qgis_mapping.html#contextual-resources-for-mapping-1",
    "title": "QGIS Mapping",
    "section": "Contextual Resources for Mapping",
    "text": "Contextual Resources for Mapping\nFor a better understanding of your mapping area, utilize OSM or Bing maps XYZ Tiles as detailed in our previous lesson. Additionally, explore the 3D CESIUM Story for an immersive 3D view of the 2D polygons and lines you will be mapping. These resources will enhance your perspective and accuracy in representing San Marino’s historic center.",
    "crumbs": [
      "GIS Tools",
      "QGIS Mapping"
    ]
  },
  {
    "objectID": "chapter_1/qgis_mapping.html#rules-for-correctly-drawing-the-street-network",
    "href": "chapter_1/qgis_mapping.html#rules-for-correctly-drawing-the-street-network",
    "title": "QGIS Mapping",
    "section": "Rules for Correctly Drawing the Street Network",
    "text": "Rules for Correctly Drawing the Street Network\nTo ensure the accuracy and efficiency of the street network layer you create, follow these guidelines:\n\nIntersection to Intersection: A ‘street’ should be mapped from one intersection to another. If the street continues beyond an intersection, it should be added as an additional feature. A single street can have several intermediate points, but the fewer, the better.\nSnapping: All beginnings and ends of streets must be snapped to each other to enable correct analysis and connectivity within the network.\nBoundary Responsibility: For the edges of your assigned tiles, you are responsible for mapping features that cross the bottom and right boundaries. This means if a street or building extends beyond these edges, you should include it in your drawing. Conversely, features that extend beyond the top or left boundaries of your tile are the responsibility of your neighboring scholars. Therefore, you can omit such streets or buildings, assuming they will be covered by your colleagues’ efforts.\nUnique ID Column: Ideally, each table should have at least one ID column with no duplicates.\n\nIn QGIS, you can ensure uniqueness by using the “Field Calculator” to create a new field. Use an expression like @row_number to automatically assign a unique number to each feature.\n\nMinimizing Elements: The most effective street network is one described using the fewest number of elements while still accurately representing all possible connections. Strive for simplicity and completeness in your mapping.\n\n\nExtra Mile\nEngage further by combining tiles with your adjacent neighbors and/or adding attributes to your layers, especially if you choose to work on the optional Buildings layer.\nWhen merging files, ensure that all Linestrings (from the Network layer) and Polygon edges are appropriately snapped.",
    "crumbs": [
      "GIS Tools",
      "QGIS Mapping"
    ]
  },
  {
    "objectID": "chapter_1/cloud_gis.html",
    "href": "chapter_1/cloud_gis.html",
    "title": "Cloud GIS",
    "section": "",
    "text": "This will be an in-class hands-on session providing an introduction to the use of CARTO for publishing and styling interactive maps.\nThe are two excercises that will help us explore different components of the platform:\nEach excersice will be lead by a live demo of the main steps to better control the visualization of the datasets. Then, each student will have time to explore on their own time the tool and come up with a shereable map.",
    "crumbs": [
      "GIS Tools",
      "Cloud GIS"
    ]
  },
  {
    "objectID": "chapter_1/cloud_gis.html#connecting-to-carto",
    "href": "chapter_1/cloud_gis.html#connecting-to-carto",
    "title": "Cloud GIS",
    "section": "Connecting to CARTO",
    "text": "Connecting to CARTO\nBecause this will be the first time you log in, you will have to log in into www.carto.com using your GitHub account (you will need the Student Development Pack approved - see the Software & Accounts section).\n\nSearch for Are you a student? and Access using GitHub\n\n\n\nUse GitHub login\n\n\n\nFollow the stepts to create your account and note that each scholar will create a separate ‘organization’. This means all your files and maps will be yours and you will not be joining NFI’s organization. You can name ir however you like!",
    "crumbs": [
      "GIS Tools",
      "Cloud GIS"
    ]
  },
  {
    "objectID": "chapter_1/cloud_gis.html#datasets",
    "href": "chapter_1/cloud_gis.html#datasets",
    "title": "Cloud GIS",
    "section": "Datasets",
    "text": "Datasets\nFor this excercise we will use two official data sources. I\n\nCycling routes (LINESTRINGS, 4326):\n\nDownload geopackage file\nConsult official source : for references to the .gpkg file above\n\nCycling docks (POINTS, 4326):\n\nDownload geopackage file\nConsult official source : for references to the .gpkg file above\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that both file have been reprojected to EPSG: 4326 as this is the prefered projection system for CARTO\n\n\n\nLoading Data\nThere are several ways to use the data in CARTO. But this time we are going to upload it directly from CARTO BUILDER\n\nUsing the main menu, navigate to Maps –&gt; + New map button. This takes you to a new tab with an instance of CARTO’s Builder.\nAdd source from... and upload the gpkg in a folder of your preference.\n\n\n\n\n\n\n\nNote that multiple layers can be added from the same\n\n\n\nAdd style\nDepending on the geometry types, you will have different option for styling your Points (docking stations) and the Linestrings (cycling routes).\n\n\n\n\n\n\nNote\n\n\n\nAlways familirise yourself with the data before jumping into modifying its style \n\n\nFor now we encourage you to explore the following three tabs:\n\nLayers\nLegends\nBasemaps\n\n\n\n\n\n\n\n\nPublish\nWhen you are happy with the map make sure you SHARE your results by publishing them to allow for public access\n\nAdd your resulting URL to this notion card\n\n\n\nYour example\n\nThe best maps will be added to complete this section!",
    "crumbs": [
      "GIS Tools",
      "Cloud GIS"
    ]
  },
  {
    "objectID": "chapter_1/cloud_gis.html#datasets-1",
    "href": "chapter_1/cloud_gis.html#datasets-1",
    "title": "Cloud GIS",
    "section": "Datasets",
    "text": "Datasets\nFor this excercise we are going to use the same datasets than we did in our previous session. However, this time we are going to use the full dataset containing the population data.\n\nDownload the municipal neighbourhoods as a SHP file: Barrios municipales de Madrid\nDownload the population by district and neighbourhood dataset as a CSV file: Población por distrito y barrio a 1 de enero",
    "crumbs": [
      "GIS Tools",
      "Cloud GIS"
    ]
  },
  {
    "objectID": "chapter_1/cloud_gis.html#workflows",
    "href": "chapter_1/cloud_gis.html#workflows",
    "title": "Cloud GIS",
    "section": "Workflows",
    "text": "Workflows\nFrom our previous experience, we know that in order to join the datasets, we first had to clean and filter the data. This time we are going to make use of CARTO’s WORKFLOWS: a no-code visual interface for replicating processes like cleaning, filtering, joining (and much more). Visit for more.",
    "crumbs": [
      "GIS Tools",
      "Cloud GIS"
    ]
  },
  {
    "objectID": "chapter_1/cloud_gis.html#hands-on",
    "href": "chapter_1/cloud_gis.html#hands-on",
    "title": "Cloud GIS",
    "section": "Hands-on",
    "text": "Hands-on\nThe following are the general steps we encourage you to follow to get familiar with other functions of CARTO.\n\nData prep\nFor this excerse we just want you to import the file below and update the inputs and output parameters\n\n\n\nCARTO’s Workflow\n\n\n\nUnder menu Data Explorer –&gt; Import both datasets (CSV and SHP)\nUnder menu Workflows –&gt; Import this SQL procedure to give you a head start\nUpdate workflow :\n\nInputs: data sources according to the location and name of your files\nOutput: select where you want to store the results\n\nRun the workflow to identify potential issues\n\nRememeber to read the Error messages (if at all), to debug your workflow\n\n\n\n\nData viz\nNow we are going to move into another builder instance and load the newly created table (from the steps above). This time we encourage you to look into the other two tabs :\n\nWidgets : allow you to interact and filter the data plotted on the map\nInteractions : helps you surface propreties of individual elements in the map\n\n\n\n\n\n\n\n\nPublish\nWhen you are happy with the map make sure you SHARE your results by publishing them to allow for public access\n\nAdd your resulting URL to this notion card\n\n\n\nOutput Example\n    \n\nMadrid’s Population Dashboard - Greg Maya",
    "crumbs": [
      "GIS Tools",
      "Cloud GIS"
    ]
  },
  {
    "objectID": "chapter_2/intro_to_python.html",
    "href": "chapter_2/intro_to_python.html",
    "title": "Welcome to Python",
    "section": "",
    "text": "Python is a great language for general purpose data wrangling. If you’re working with geospatial data and general data science then it is arguably THE language. (Though “R” fans will probably debate this!)",
    "crumbs": [
      "Basic Coding",
      "Welcome to Python"
    ]
  },
  {
    "objectID": "chapter_2/intro_to_python.html#some-basics",
    "href": "chapter_2/intro_to_python.html#some-basics",
    "title": "Welcome to Python",
    "section": "Some basics",
    "text": "Some basics\nFire up your JupyterLab and follow along by typing and executing these examples.\n\na_variable = 'boo'\nprint(a_variable)\n\nboo\n\n\nPython works with variables which store references to information. These variables can be named so that they are easy to remember and should be descriptive so that they make their purpose clear. By convention, variables in Python use lower caps with underscores and should not start with a number.\nNotice that these variable names don’t tell us very much:\n\na = 21\nb = 34\nd = b - a\n\nBut these ones do:\n\ntemp_day_1 = 21\ntemp_day_2 = 34\ntemp_diff = temp_day_2 - temp_day_1\n\nYou should also use comments to explain your code, this becomes important as the code grows in complexity. Use a hash symbol for a line comment and triple quotes for block comments.\n\n\"\"\"\nThe following lines detail the procedure for\ncomputing a temperature difference from day 1 to 2.\n\"\"\"\n# the temperature on day 1\ntemp_day_1 = 21\n# the temperature on day 2\ntemp_day_2 = 34\n# the temperature difference from day 1 to 2\ntemp_diff = temp_day_2 - temp_day_1\n\nAnother concept is the use of functions. Functions take parameters (arguments) and return a result according to a defined sequence of steps. In certain contexts they are also referred to as “methods”, but don’t worry too much about that for now. A commonly used function in Python is the print() method. Notice that a function has parentheses, and this is where you pass your function arguments.\n\n# you can pass a parameter to print()\n# it will then take the parameter and print it to the screen\nprint('This will print boo!')\n# the same for a variable\nprint(temp_diff)\n# and in the case of the print() method\n# multiple arguments can be passed at once\n# function arguments are separated with commas\nprint('The temp diff is: ', temp_diff)\n\nThis will print boo!\n13\nThe temp diff is:  13\n\n\nYou can define a function using the def keyword. Let’s define a function which takes a number and multiplies it by a specified multiplier.\n\n# you must use \"def\"\n# define your parameters\ndef num_multiply(a_number, multiple):\n  # note the use of indented lines\n  multiplied_number = a_number * multiple\n  # you must return a value if you want to use\n  # it outside of the function\n  return multiplied_number\n\n# let's try it out\n# assign the returned value to a variable\nout_num_a = num_multiply(2, 2)\nprint('2x2=', out_num_a)\n# functions save us from rewriting the\n# same bits of code over and over...\n# we can use the same logic with new arguments\nout_num_b = num_multiply(3, 10)\nprint('3x10=', out_num_b)\n\n2x2= 4\n3x10= 30\n\n\n\nIf you find yourself writing bits of code which seem repetitive, then you probably want to be using functions…!\n\nVariables can store different kinds of data, including str, int, float (decimal), and other types such as “booleans” (True and False) and None types. If you want to find out what type a variable is, you can use the built-in type() method, which will tell you what type a given variable is.\n\n# here we are nesting methods\n# the type() method will run first and returns a result\n# this result is then passed to the print() method\nprint(type(a_variable))\nprint(type(temp_diff))\nprint(type(1))\nprint(type(1.0))\nprint(type(True))\nprint(type(None))\n\n&lt;class 'str'&gt;\n&lt;class 'int'&gt;\n&lt;class 'int'&gt;\n&lt;class 'float'&gt;\n&lt;class 'bool'&gt;\n&lt;class 'NoneType'&gt;\n\n\nOnce you’ve assigned a value to a variable, you can use this variable to do things like maths (in the case of int or float types).\n\ntemp_day_1 = 21\ntemp_day_2 = 34\nprint('added:', temp_day_1 + temp_day_2)\nprint('subtracted:', temp_day_2 - temp_day_1)\nprint('divided:', temp_day_2 / 2)\n# modulo gives the remainder after division\nprint('modulo:', temp_day_1 % 2)\n\nadded: 55\nsubtracted: 13\ndivided: 17.0\nmodulo: 1\n\n\nYou can do surprising things with strings as well.\n\npart_1 = 'Hello'\npart_2 = 'there!'\n# here we'll combine the parts using an empty space\ntogether = part_1 + ' ' + part_2\nprint(together)\n\nHello there!\n\n\nPython has some other types that can be useful. One of these is lists, which use square brackets to collect several items into the same variable.\n\nmy_list = [1, 2, 3, 4]\n# gets the length of a list\nprint('number of items:', len(my_list))\n# indexes into the first item in a list\n# in Python, lists are zero indexed\n# note the use of square brackets for indexing\nget_something = my_list[0]\nprint('first item:', get_something)\n# gets the last item\nget_something_else = my_list[-1]\nprint('last item:', get_something_else)\n# gets list entries from index 0 to before index 2\nget_several_things = my_list[0:2]\nprint('several items:', get_several_things)\n\nnumber of items: 4\nfirst item: 1\nlast item: 4\nseveral items: [1, 2]\n\n\nNote that you can iterate over lists using for and in syntax. We use a variable, which we are here calling num, to store the current result from iterating over the list. We can then use that variable to do something.\n\n# this will print each number in my_list\nfor num in my_list:\n    # note the indented line\n    # indentation has to be used consistently\n    print('num is currently:', num)\n\nnum is currently: 1\nnum is currently: 2\nnum is currently: 3\nnum is currently: 4\n\n\nPython has a number of built-in convenience methods, which can simplify common tasks:\n\nprint('min:', min(my_list))\nprint('max:', max(my_list))\n# note the extra step to casts the output\n# from reversed to a list type\nprint('reversed:', list(reversed(my_list)))\nprint('absolute:', abs(-100))\n\nmin: 1\nmax: 4\nreversed: [4, 3, 2, 1]\nabsolute: 100",
    "crumbs": [
      "Basic Coding",
      "Welcome to Python"
    ]
  },
  {
    "objectID": "chapter_2/intro_to_python.html#importing-packages",
    "href": "chapter_2/intro_to_python.html#importing-packages",
    "title": "Welcome to Python",
    "section": "Importing packages",
    "text": "Importing packages\nPython can import packages which contain reusable code.\nPackages have to be installed before they are used, unless they are part of the default Python library. Let’s install the shapely package. Note that this method of installing a package with a leading exclamation mark is particular to JupyterLab Notebooks, and only has to be done if the package is not yet installed.\n\n# checks that the shapely package\n# is installed for this notebook\n# uncomment the next line to run\n# !pip install shapely\n\nOnce installed and available, we can use modules provided by the package. Here we are using the from and import syntax to import the geometry module which is available from the shapely package.\n\nfrom shapely import geometry",
    "crumbs": [
      "Basic Coding",
      "Welcome to Python"
    ]
  },
  {
    "objectID": "chapter_2/intro_to_python.html#shapely",
    "href": "chapter_2/intro_to_python.html#shapely",
    "title": "Welcome to Python",
    "section": "Shapely",
    "text": "Shapely\nshapely let’s us do some neat things with spatial information. Let’s define a Point type using the geometry module. The geometry module has some already defined methods which we can call. Let’s use the Point constructor to generate a new point. Note the use of dot-notation.\n\n# this creates a point using the Point constructor\n# defined by the geometry module\npoint_a = geometry.Point(0, 0)\nprint(point_a)\nprint(type(point_a))\n\nPOINT (0 0)\n&lt;class 'shapely.geometry.point.Point'&gt;\n\n\nDifferent packages and modules define set patterns of behaviour. To understand what these are and what you can do with them, refer to the package documentation and ask ChatGPT to help you along the way.\nFor example, a shapely Point type has built-in methods which allow you to easily produce certain forms of behaviour. You can access these using dot-notation. Let’s buffer the point object using the provided buffer() method.\n\nbuff = point_a.buffer(10)\nprint(buff)\nprint(type(buff))\n\nPOLYGON ((10 0, 9.95184726672197 -0.980171403295606, 9.807852804032304 -1.9509032201612824, 9.569403357322088 -2.902846772544623, 9.238795325112868 -3.826834323650898, 8.819212643483551 -4.7139673682599765, 8.314696123025453 -5.555702330196022, 7.73010453362737 -6.343932841636455, 7.0710678118654755 -7.071067811865475, 6.343932841636455 -7.73010453362737, 5.555702330196023 -8.314696123025453, 4.713967368259978 -8.81921264348355, 3.8268343236508984 -9.238795325112868, 2.902846772544623 -9.56940335732209, 1.9509032201612833 -9.807852804032304, 0.9801714032956077 -9.951847266721968, 0.0000000000000006 -10, -0.9801714032956065 -9.95184726672197, -1.950903220161282 -9.807852804032304, -2.9028467725446214 -9.56940335732209, -3.826834323650897 -9.238795325112868, -4.713967368259977 -8.819212643483551, -5.55570233019602 -8.314696123025454, -6.3439328416364535 -7.730104533627371, -7.071067811865475 -7.0710678118654755, -7.73010453362737 -6.343932841636455, -8.314696123025453 -5.555702330196022, -8.81921264348355 -4.713967368259978, -9.238795325112868 -3.826834323650899, -9.569403357322088 -2.902846772544624, -9.807852804032304 -1.9509032201612861, -9.951847266721968 -0.9801714032956083, -10 -0.0000000000000012, -9.95184726672197 0.9801714032956059, -9.807852804032304 1.9509032201612837, -9.56940335732209 2.902846772544621, -9.238795325112868 3.8268343236508966, -8.819212643483551 4.7139673682599765, -8.314696123025454 5.55570233019602, -7.730104533627371 6.343932841636453, -7.071067811865477 7.071067811865475, -6.343932841636459 7.730104533627367, -5.555702330196022 8.314696123025453, -4.713967368259978 8.81921264348355, -3.8268343236509033 9.238795325112864, -2.9028467725446245 9.569403357322088, -1.9509032201612866 9.807852804032303, -0.9801714032956045 9.95184726672197, -0.0000000000000018 10, 0.9801714032956009 9.95184726672197, 1.950903220161283 9.807852804032304, 2.9028467725446205 9.56940335732209, 3.8268343236509 9.238795325112866, 4.713967368259976 8.819212643483551, 5.555702330196018 8.314696123025454, 6.343932841636456 7.730104533627369, 7.071067811865474 7.071067811865477, 7.730104533627367 6.343932841636459, 8.314696123025453 5.555702330196022, 8.819212643483548 4.713967368259979, 9.238795325112864 3.826834323650904, 9.569403357322088 2.902846772544625, 9.807852804032303 1.9509032201612873, 9.95184726672197 0.980171403295605, 10 0))\n&lt;class 'shapely.geometry.polygon.Polygon'&gt;\n\n\nNotice that we now have a Polygon type since the returned type is no longer a Point. Types (which are a form of Python class) can have properties which are accessed using dot-notation. Unlike methods, properties do not require parentheses because they do not accept arguments.\n\n# here we are using the area property to retrieve the area\nprint('Point area:', point_a.area)\nprint('Polygon area:', buff.area)\n# properties can be nested via dot-notation\n# for example, polygons have an exterior property\n# which returns a LinearRing\n# let's get the LinearRing's length property\nprint(type(buff.exterior))\nprint('Polygon circumference:', buff.exterior.length)\n\nPoint area: 0.0\nPolygon area: 313.6548490545941\n&lt;class 'shapely.geometry.polygon.LinearRing'&gt;\nPolygon circumference: 62.80662313909506\n\n\nLet’s create a line. The LineString constructor requires a list of points.\n\npoint_b = geometry.Point(1, 1)\nline = geometry.LineString([point_a, point_b])\nprint(line)\nprint(list(line.coords))\nprint(line.length)\n\nLINESTRING (0 0, 1 1)\n[(0.0, 0.0), (1.0, 1.0)]\n1.4142135623730951",
    "crumbs": [
      "Basic Coding",
      "Welcome to Python"
    ]
  },
  {
    "objectID": "chapter_2/intro_to_python.html#putting-things-together",
    "href": "chapter_2/intro_to_python.html#putting-things-together",
    "title": "Welcome to Python",
    "section": "Putting things together",
    "text": "Putting things together\nLet’s try putting some things together. If you’re wondering what a certain line is doing, then try pasting the code block into ChatGPT, which will typically provide a detailed description.\n\nx_coords = [0, 1, 2, 3]\ny_coords = [0, 1, 2, 3]\n# this is where we'll store the points\ncollect_points = []\n# let's do a nested iteration over the \nfor x in x_coords:\n    for y in y_coords:\n        p = geometry.Point(x, y)\n        # this adds the p variable\n        # to the collect_points list\n        collect_points.append(p)\nprint('num points:', len(collect_points))\n# let's turn the list of points into a MultiPoint\nmulti_point = geometry.MultiPoint(collect_points)\nprint(type(multi_point))\n\nnum points: 16\n&lt;class 'shapely.geometry.multipoint.MultiPoint'&gt;",
    "crumbs": [
      "Basic Coding",
      "Welcome to Python"
    ]
  },
  {
    "objectID": "chapter_2/intro_to_python.html#geopandas",
    "href": "chapter_2/intro_to_python.html#geopandas",
    "title": "Welcome to Python",
    "section": "GeoPandas",
    "text": "GeoPandas\nThere is an easier way to keep track of multiple geometries: GeoPandas! GeoPandas is a geospatial enabled version of Pandas DataFrames. Think of it as a CSV spreadsheet with the ability to handle geometry data types.\n\n# install if necessary by uncommenting the next line\n# !pip install geopandas\n\nLet’s create a GeoPandas DataFrame from our collected points\n\n# we'll import the whole geoopandas package\n# and we'll give it a shorter alias using \"as\"\nimport geopandas as gpd\n# let's create a dataframe from the points\ndf = gpd.GeoDataFrame(geometry=collect_points, crs=None)\n# the head() method returns the first (typically 5) rows\nprint(df.head())\n\n                  geometry\n0  POINT (0.00000 0.00000)\n1  POINT (0.00000 1.00000)\n2  POINT (0.00000 2.00000)\n3  POINT (0.00000 3.00000)\n4  POINT (1.00000 0.00000)\n\n\nThere we go, that’s a lot neater. You can add columns to DataFrames and you can also export (or import) from common geospatial file types such as gpkg. GeoDataFrames are also much easier to visualise.\n\ndf.plot()\n\n\n\n\n\n\n\n\nYou can run the typical shapely operations on a GeoDataFrame’s geometry column. Let’s see what happens if we buffer the GeoDataFrame.\n\nbuff_series = df.buffer(0.6)\n# this returns a GeoSeries\n# essentially a single column of a DataFrame\nprint(type(buff_series))\n# the geometry type is now POLYGON\nprint(buff_series.head())\nbuff_series.plot()\n\n&lt;class 'geopandas.geoseries.GeoSeries'&gt;\n0    POLYGON ((0.60000 0.00000, 0.59711 -0.05881, 0...\n1    POLYGON ((0.60000 1.00000, 0.59711 0.94119, 0....\n2    POLYGON ((0.60000 2.00000, 0.59711 1.94119, 0....\n3    POLYGON ((0.60000 3.00000, 0.59711 2.94119, 0....\n4    POLYGON ((1.60000 0.00000, 1.59711 -0.05881, 1...\ndtype: geometry\n\n\n\n\n\n\n\n\n\nDataFrames are really useful for handling data. Let’s add a column called area and assign it the area value for the geometry.\n\n# create a new GeoDataFrame from the polygon GeoSeries column\ndf_buff = gpd.GeoDataFrame(geometry=buff_series, crs=None)\n# adds a new column called area\n# sets it to the value of the respective geometry areas\ndf_buff['area'] = df_buff.geometry.area\n# let's do the same for x and y\ndf_buff['x'] = df_buff.geometry.centroid.x\ndf_buff['y'] = df_buff.geometry.centroid.y\n# and let's filter the columns for printing\n# we can do this by passing a list of column names\n# to GeoPandas indexer - which uses square brackets\nprint(df_buff[['area', 'x', 'y']].head())\n\n       area             x             y\n0  1.129157 -1.808352e-18 -3.430668e-18\n1  1.129157 -5.745118e-18  1.000000e+00\n2  1.129157  2.928570e-18  2.000000e+00\n3  1.129157  1.931576e-17  3.000000e+00\n4  1.129157  1.000000e+00  3.075098e-17\n\n\nDataFrames are useful for filtering data.\n\n# here we are filtering rows based on their x values\n# we are then assigning this to a new variable\n# the part \"df_buff.x &lt; 2\" is what does the magic\nfiltered_df_buff = df_buff[df_buff.x &lt; 2]\nfiltered_df_buff.plot()\n\n\n\n\n\n\n\n\nThere are almost limitless ways to manipulate data in DataFrames and these are well documented on the web. If you’re ever stuck, remember that ChatGPT is quite good at finding bugs and tends to have an intuitive grasp of what you’re trying to do if you explain your objective well.",
    "crumbs": [
      "Basic Coding",
      "Welcome to Python"
    ]
  },
  {
    "objectID": "chapter_2/pandas.html",
    "href": "chapter_2/pandas.html",
    "title": "Pandas & Data",
    "section": "",
    "text": "Now that you know some of the basics of Python Notebooks, it’s time to start using them for data science (no, that simple math you did the last time doesn’t count as data science!).\nYou are about to enter the Python data ecosystem. This course does not explicitly cover the fundamentals of programming; if you’re stuck, confused, or need further explanation, the answers are more often than not easily accessible through online resources such as your favourite search engine, Stack Overflow, and the amazingly capable explanatory power of emerging AI tools such as ChatGPT. The internet is a friend of every programmer and you’re encouraged to use it effectively from the get-go. There has never been an easier time to learn how to code!\nLet’s dig in!\n\nThis notebook can be downloaded from here.\n\n\n\nThis section is adapted from the Spatial Data Science for Social Geography by Martin Fleischman, which in turn is based on A Course on Geographic Data Science by Dani Arribas-Bel, both licensed under CC-BY-SA 4.0. The content and datasets were reworked to accommodate the Madrid neighbourhood population statistics dataset.\n\n\n\nReal-world datasets are messy. There is no way around it: datasets have “holes” (missing data), the number of formats in which data can be stored can be endless, and the best structure to share data is not always the optimal for analysis. Hence the need to “munge” or “wrangle” data. Much of the time spent in what is called Data Science is related not only to modelling and insight but more often than not has to do with much more basic and less exotic tasks such as obtaining the data, data cleaning, and other preparation which makes analysis possible.\nSurprisingly, very little has been published on patterns, techniques, and best practices for quick and efficient data cleaning, manipulation, and transformation because of how labour-intensive this aspect is. In this session, we will use real-world datasets and learn how to transform and manipulate these, if necessary, prior to analysis. For this, we will introduce some of the bread-and-butter of data analysis and scientific computing in Python. These are fundamental tools that are widely and consistently used on almost any tasks relating to data analysis.\nThis notebook covers the basics and the content that needs to be grasped, and discusses several patterns to clean and structure data properly, including tidying, subsetting, and aggregating. We will conclude with some basic visualisation. An additional extension presents more advanced tricks to manipulate tabular data.\n\n\n\nWe will be exploring demographic characteristics of Madrid. The data has been aggregated to a neighbourhood level by the statistic’s office of Madrid’s City Hall. It contains information per year (from 2018 to 2023) and theme.\nAs with many datasets that will be used during this course, the data was originally found in an online data portal at the following link\nThe main tool we will use for this task is the pandas package. As with any Python module or package, the Pandas package has to be imported into the Notebook prior to usage.\n\nimport pandas as pd\n\nPandas let’s us work with datasets, and stores the information in a DataFrame consisting of rows and columns. Here, we will read an online csv dataset into a Pandas DataFrame. Pandas can open local files directly, but also has the nifty ability to download files directly from the web:\n\n# here we will directly fetch the CSV dataset and save it as a Pandas DataFrame\nmadrid_pop = pd.read_csv(\n    \"https://datos.madrid.es/egob/catalogo/300557-0-poblacion-distrito-barrio.csv\",\n    sep=\";\",\n)\n\n\nDelimiters\nIn this case we are using the Pandas read_csv method because the source file is a csv. By default, csv files are assumed to use commas for data delimitation, but this file uses semi-colons instead. This is why we are passing the optional sep parameter to specify a ; delimeter.\n\n\nFormats\nNote that pandas allows for many more formats to be read and written. A full list of formats supported may be found in the documentation.\n\nIt is also possible to download the file and read it locally. In this case, you would download the file by clicking on this link. Then place it in the same folder as the notebook where you intend to read it from, and run the below cell.\n\n# remember, this cell will only work if you have downloaded the file\n# and, if the filepath is correct!\n# uncomment the below lines to run!\n# madrid_pop = pd.read_csv(\n#     \"poblacion_1_enero.csv\",\n#     sep=\";\",\n# )\n\n\n\n\nNow, we are ready to start playing and interrogating the dataset! What you have at your fingertips is a table summarising, for each of the districts in Madrid, how many people lived there by gender. These tables are called DataFrame objects, and they have a lot of functionality built-in to explore and manipulate the data they contain. Let’s explore a few of those cool tricks!\n\n\nThe first aspect worth spending a bit of time on is the structure of a DataFrame. You can print it by simply typing its name:\n\nmadrid_pop\n\n\n\n\n\n\n\n\nfecha\ncod_municipio\nmunicipio\ncod_distrito\ndistrito\ncod_barrio\nbarrio\nnum_personas\nnum_personas_hombres\nnum_personas_mujeres\n\n\n\n\n0\n1 de enero de 2023\n28079\nMadrid\n1\nCentro\n1\nCentro\n139.687\n70.770\n68.917\n\n\n1\n1 de enero de 2023\n28079\nMadrid\n2\nArganzuela\n2\nArganzuela\n153.304\n71.754\n81.550\n\n\n2\n1 de enero de 2023\n28079\nMadrid\n3\nRetiro\n3\nRetiro\n117.918\n53.458\n64.460\n\n\n3\n1 de enero de 2023\n28079\nMadrid\n4\nSalamanca\n4\nSalamanca\n145.702\n64.452\n81.250\n\n\n4\n1 de enero de 2023\n28079\nMadrid\n5\nChamartín\n5\nChamartín\n144.796\n65.245\n79.551\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n913\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n212\nAeropuerto\n1.794\n922\n872\n\n\n914\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n213\nCascoHistóricodeBarajas\n7.336\n3.550\n3.786\n\n\n915\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n214\nTimón\n11.750\n5.651\n6.099\n\n\n916\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n215\nCorralejos\n7.510\n3.657\n3.853\n\n\n917\n1 de enero de 2018\n28079\nMadrid\nTodos\nTodos\nTodos\nTodos\n3.221.824\n1.500.340\n1.721.484\n\n\n\n\n918 rows × 10 columns\n\n\n\nAs you can expect, the dataset was provided in the original local language, so to make things a little easier, we are going to translate the column names using one list of texts. for this we will reassign the column names corresponding to their English equivalents.\n\nNote that there are multiple ways of arriving at the same output, the below is just one of them\n\n\nen_cols = [\n    \"date\",\n    \"code_municipality\",\n    \"municipality\",\n    \"code_district\",\n    \"district\",\n    \"code_neighbourhood\",\n    \"neighbourhood\",\n    \"num_people\",\n    \"num_people_men\",\n    \"num_people_women\",\n]\nmadrid_pop.columns = en_cols\nmadrid_pop\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ndistrict\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\n\n\n\n\n0\n1 de enero de 2023\n28079\nMadrid\n1\nCentro\n1\nCentro\n139.687\n70.770\n68.917\n\n\n1\n1 de enero de 2023\n28079\nMadrid\n2\nArganzuela\n2\nArganzuela\n153.304\n71.754\n81.550\n\n\n2\n1 de enero de 2023\n28079\nMadrid\n3\nRetiro\n3\nRetiro\n117.918\n53.458\n64.460\n\n\n3\n1 de enero de 2023\n28079\nMadrid\n4\nSalamanca\n4\nSalamanca\n145.702\n64.452\n81.250\n\n\n4\n1 de enero de 2023\n28079\nMadrid\n5\nChamartín\n5\nChamartín\n144.796\n65.245\n79.551\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n913\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n212\nAeropuerto\n1.794\n922\n872\n\n\n914\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n213\nCascoHistóricodeBarajas\n7.336\n3.550\n3.786\n\n\n915\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n214\nTimón\n11.750\n5.651\n6.099\n\n\n916\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n215\nCorralejos\n7.510\n3.657\n3.853\n\n\n917\n1 de enero de 2018\n28079\nMadrid\nTodos\nTodos\nTodos\nTodos\n3.221.824\n1.500.340\n1.721.484\n\n\n\n\n918 rows × 10 columns\n\n\n\nThe default printing is truncated to keep a compact view, but is enough to convey its structure. DataFrame objects have two dimensions: rows and columns. Rows are typically identified with an index column and columns are typically identified with names describing their content. The above example shows how the column names were automatically lifted from the csv file’s column headers.\nWe can set the row index using the set_index() method. Let’s set the index to use the district column.\n\n# Remove leading and trailing spaces from 'distrito'\nmadrid_pop[\"district\"] = madrid_pop[\"district\"].str.strip()\n# set the index based on the district column\nmadrid_pop.set_index(\"district\", inplace=True)\n\nColumns can be assigned as one of various forms of data such as integers as int, decimals as float, text as str, or if pandas is unable to use a standard type, it might use the object type as a catch-all type.\nTo extract a single column from this DataFrame, specify its name in square brackets ([]). Note that the name is a string - a piece of text - which needs to be denoted with single (') or double quotes (\"). Without the quotes, Python will think you are referring to a variable, and will then complain that the variable can’t be found!\nA single column is not a DataFrame but a Series, which also means that a DataFrame is a collection of Series!\n\n# this will fetch and return the num_people_women column as a Series\nmadrid_pop[\"num_people_women\"]\n\ndistrict\nCentro           68.917\nArganzuela       81.550\nRetiro           64.460\nSalamanca        81.250\nChamartín        79.551\n                ...    \nBarajas             872\nBarajas           3.786\nBarajas           6.099\nBarajas           3.853\nTodos         1.721.484\nName: num_people_women, Length: 918, dtype: object\n\n\n\n\n\nYou can specifically print the DataFrames top (or bottom) lines by passing a number to the method head or tail. For example, for the top or bottom three lines:\n\nmadrid_pop.head(3)\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\n\n\ndistrict\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n1\nCentro\n139.687\n70.770\n68.917\n\n\nArganzuela\n1 de enero de 2023\n28079\nMadrid\n2\n2\nArganzuela\n153.304\n71.754\n81.550\n\n\nRetiro\n1 de enero de 2023\n28079\nMadrid\n3\n3\nRetiro\n117.918\n53.458\n64.460\n\n\n\n\n\n\n\n\nmadrid_pop.tail(3)\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\n\n\ndistrict\n\n\n\n\n\n\n\n\n\n\n\n\n\nBarajas\n1 de enero de 2018\n28079\nMadrid\n21\n214\nTimón\n11.750\n5.651\n6.099\n\n\nBarajas\n1 de enero de 2018\n28079\nMadrid\n21\n215\nCorralejos\n7.510\n3.657\n3.853\n\n\nTodos\n1 de enero de 2018\n28079\nMadrid\nTodos\nTodos\nTodos\n3.221.824\n1.500.340\n1.721.484\n\n\n\n\n\n\n\nInspecting datasets is vital to find errors that might skew your analysis. The last row of the dataset contains the sums of other column values ( Spanish: Todos -&gt; English: All ).\nBefore continuing, let’s fix this by removing these columns from the dataset. First, let’s check if we have any more occurrences of Todos. We can use Pandas’ loc indexer to fetch rows where the index column contains the text string Todos.\n\nmadrid_pop.loc[\"Todos\"]\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\n\n\ndistrict\n\n\n\n\n\n\n\n\n\n\n\n\n\nTodos\n1 de enero de 2023\n28079\nMadrid\nTodos\nTodos\nTodos\n3.339.931\n1.559.866\n1.780.065\n\n\nTodos\n1 de enero de 2022\n28079\nMadrid\nTodos\nTodos\nTodos\n3.286.662\n1.534.824\n1.751.838\n\n\nTodos\n1 de enero de 2021\n28079\nMadrid\nTodos\nTodos\nTodos\n3.312.310\n1.545.157\n1.767.153\n\n\nTodos\n1 de enero de 2020\n28079\nMadrid\nTodos\nTodos\nTodos\n3.334.730\n1.554.732\n1.779.998\n\n\nTodos\n1 de enero de 2019\n28079\nMadrid\nTodos\nTodos\nTodos\n3.266.126\n1.521.178\n1.744.948\n\n\nTodos\n1 de enero de 2018\n28079\nMadrid\nTodos\nTodos\nTodos\n3.221.824\n1.500.340\n1.721.484\n\n\n\n\n\n\n\nNow that we know that for every year the data has been aggregated and added back as a new row, we know that removing the last row would not be enough to correct our DataFrame.\nWe will make use of the drop function in combination with the native loc function of DataFrames.\n\nprint(\"Rows before droping values: \", len(madrid_pop))\nmadrid_pop.drop(index=\"Todos\", inplace=True)\nprint(\"Rows after droping values: \", len(madrid_pop))\n\nRows before droping values:  918\nRows after droping values:  912\n\n\nNow, let’s get an overview of the table:\n\nmadrid_pop.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 912 entries, Centro to Barajas\nData columns (total 9 columns):\n #   Column              Non-Null Count  Dtype \n---  ------              --------------  ----- \n 0   date                912 non-null    object\n 1   code_municipality   912 non-null    int64 \n 2   municipality        912 non-null    object\n 3   code_district       912 non-null    object\n 4   code_neighbourhood  912 non-null    object\n 5   neighbourhood       912 non-null    object\n 6   num_people          912 non-null    object\n 7   num_people_men      912 non-null    object\n 8   num_people_women    912 non-null    object\ndtypes: int64(1), object(8)\nmemory usage: 71.2+ KB\n\n\nCan you spot something wrong?\nInteger numbers are sometimes stored as text values, especially if the input dataset contained commas, periods, or null data values. The actual values of population counts would be better stored as full integers so that we can more effectively work with this data as a numeric data type. Let’s take the following steps:\n\nCreate a list containg the column names that have people counts\nLoop through the list, removing periods from the numbers, then casting the values to integer types while allowing for null data values\n\n\n# List of columns to process\ncolumns_to_process = [\"num_people\", \"num_people_men\", \"num_people_women\"]\n\n# Loop through columns\nfor column in columns_to_process:\n    # create a copy of the column\n    dirty_numbers = madrid_pop[column]\n    # remove \".\" from the numbers\n    clean_numbers = dirty_numbers.str.replace(\".\", \"\")\n    # convert to numeric - coerce will convert non numeric values to NaN (Not a Number) or \"null\" types\n    numeric_numbers = pd.to_numeric(clean_numbers, errors=\"coerce\")\n    # overwrite the original column\n    madrid_pop[column] = numeric_numbers\n\nmadrid_pop.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 912 entries, Centro to Barajas\nData columns (total 9 columns):\n #   Column              Non-Null Count  Dtype \n---  ------              --------------  ----- \n 0   date                912 non-null    object\n 1   code_municipality   912 non-null    int64 \n 2   municipality        912 non-null    object\n 3   code_district       912 non-null    object\n 4   code_neighbourhood  912 non-null    object\n 5   neighbourhood       912 non-null    object\n 6   num_people          912 non-null    int64 \n 7   num_people_men      912 non-null    int64 \n 8   num_people_women    912 non-null    int64 \ndtypes: int64(4), object(5)\nmemory usage: 71.2+ KB\n\n\n\n\n\n\nmadrid_pop.describe()\n\n\n\n\n\n\n\n\ncode_municipality\nnum_people\nnum_people_men\nnum_people_women\n\n\n\n\ncount\n912.0\n912.000000\n912.000000\n912.000000\n\n\nmean\n28079.0\n43336.804825\n20210.739035\n23126.065789\n\n\nstd\n0.0\n51564.255101\n24072.657139\n27522.444389\n\n\nmin\n28079.0\n945.000000\n490.000000\n455.000000\n\n\n25%\n28079.0\n17330.500000\n8132.000000\n9267.250000\n\n\n50%\n28079.0\n24700.500000\n11580.000000\n13640.000000\n\n\n75%\n28079.0\n42166.500000\n19700.250000\n22349.750000\n\n\nmax\n28079.0\n262339.000000\n122632.000000\n139707.000000\n\n\n\n\n\n\n\nNote how the output is also a DataFrame object, so you can manipulate it the same way that you would with the original table (e.g. writing it to a file).\nIn this case, the summary might be better presented if the table is “transposed”:\n\nmadrid_pop[columns_to_process].describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nnum_people\n912.0\n43336.804825\n51564.255101\n945.0\n17330.50\n24700.5\n42166.50\n262339.0\n\n\nnum_people_men\n912.0\n20210.739035\n24072.657139\n490.0\n8132.00\n11580.0\n19700.25\n122632.0\n\n\nnum_people_women\n912.0\n23126.065789\n27522.444389\n455.0\n9267.25\n13640.0\n22349.75\n139707.0\n\n\n\n\n\n\n\nEqually, common descriptive statistics are also available. To obtain minimum values for each column, you can use .min().\n\nmadrid_pop.min()\n\ndate                  1 de enero de 2018\ncode_municipality                  28079\nmunicipality                      Madrid\ncode_district                          1\ncode_neighbourhood                     1\nneighbourhood                 Arganzuela\nnum_people                           945\nnum_people_men                       490\nnum_people_women                     455\ndtype: object\n\n\nOr to obtain a minimum for a single column only.\n\nmadrid_pop[\"num_people_women\"].min()\n\n455\n\n\nNote here how you have restricted the calculation of the minimum value to one column only by getting the Series and calling .min() on that.\nSimilarly, you can restrict the calculations to a single district using .loc[] indexer:\n\nmadrid_pop.loc[\"Centro\"].min()\n\ndate                  1 de enero de 2018\ncode_municipality                  28079\nmunicipality                      Madrid\ncode_district                          1\ncode_neighbourhood                     1\nneighbourhood                     Centro\nnum_people                          7201\nnum_people_men                      3672\nnum_people_women                    3529\ndtype: object\n\n\nLet’s see when and where the said minimum occurred.\n\n# we can use comparators to index into the DataFrame where a specific column equals a specific value\nmadrid_pop[madrid_pop[\"num_people_women\"] == 3529]\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\n\n\ndistrict\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n16\nSol\n7201\n3672\n3529\n\n\n\n\n\n\n\n\n\n\nYou can generate new variables by applying operations to existing ones. For example, you can calculate the ratio of women.\n\nratio_women = madrid_pop[\"num_people_women\"] / madrid_pop[\"num_people\"]\nratio_women.head()\n\ndistrict\nCentro        0.493367\nArganzuela    0.531950\nRetiro        0.546651\nSalamanca     0.557645\nChamartín     0.549401\ndtype: float64\n\n\nOnce you have created the variable, you can make it part of the table:\n\nmadrid_pop[\"ratio_women\"] = ratio_women\nmadrid_pop.head()\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\n\n\ndistrict\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n1\nCentro\n139687\n70770\n68917\n0.493367\n\n\nArganzuela\n1 de enero de 2023\n28079\nMadrid\n2\n2\nArganzuela\n153304\n71754\n81550\n0.531950\n\n\nRetiro\n1 de enero de 2023\n28079\nMadrid\n3\n3\nRetiro\n117918\n53458\n64460\n0.546651\n\n\nSalamanca\n1 de enero de 2023\n28079\nMadrid\n4\n4\nSalamanca\n145702\n64452\n81250\n0.557645\n\n\nChamartín\n1 de enero de 2023\n28079\nMadrid\n5\n5\nChamartín\n144796\n65245\n79551\n0.549401\n\n\n\n\n\n\n\n\n\n\nHere, you explore how to subset parts of a DataFrame if you know exactly which bits you want.\nFor example, if you want to extract the “date”, “neighbourhood”, and “ratio_women” columns for the “Centro” and “Retiro” districts, you can use the Pandas loc indexer. Indexing in Pandas is very flexible and powerful, but can also be a bit confusing for the same reason. The loc documentation and some back-and-forth with AI can be helpful in clearing up points of confusion!\n\nwomen_ratio_2districts = madrid_pop.loc[\n    [\"Centro\", \"Retiro\"],  # the rows to retrieve\n    [\"date\", \"neighbourhood\", \"ratio_women\"],  # the columns to retrieve\n]\nwomen_ratio_2districts\n\n\n\n\n\n\n\n\ndate\nneighbourhood\nratio_women\n\n\ndistrict\n\n\n\n\n\n\n\nCentro\n1 de enero de 2023\nCentro\n0.493367\n\n\nCentro\n1 de enero de 2023\nPalacio\n0.508234\n\n\nCentro\n1 de enero de 2023\nEmbajadores\n0.478530\n\n\nCentro\n1 de enero de 2023\nCortes\n0.505362\n\n\nCentro\n1 de enero de 2023\nJusticia\n0.491685\n\n\n...\n...\n...\n...\n\n\nRetiro\n1 de enero de 2018\nAdelfas\n0.540638\n\n\nRetiro\n1 de enero de 2018\nEstrella\n0.535416\n\n\nRetiro\n1 de enero de 2018\nIbiza\n0.562966\n\n\nRetiro\n1 de enero de 2018\nLosJerónimos\n0.531449\n\n\nRetiro\n1 de enero de 2018\nNiñoJesús\n0.545746\n\n\n\n\n84 rows × 3 columns\n\n\n\nYou can see how you can create a list with the names (index IDs) along each of the two dimensions of a DataFrame (rows and columns), and loc will return a subset of the original table only with the elements queried for.\nAn alternative to list-based queries is what is called “range-based” queries. These work on the indices of the table, but instead of requiring the ID of each item you want to retrieve, they operate by requiring only two IDs: the first and last element in a range of items. Range queries are expressed with a colon (:). However, to perform this operation Index IDs need to be unique. Since this is not our case we will first create a new index composed the year and the code_neighbourhood as a new index to our DataFrame.\n\n# Reset the index to move 'district' back to a regular column\nmadrid_pop.reset_index(inplace=True)\n# Extract the last 4 digits from the 'date' column and create a new 'year' column\nmadrid_pop[\"year\"] = madrid_pop[\"date\"].str[-4:]\n# Create a new column with the combination of 'year' and 'code_neighbourhood'\nmadrid_pop[\"new_index\"] = madrid_pop[\"year\"] + \"_\" + madrid_pop[\"code_neighbourhood\"]\n# Set this as the new index\nmadrid_pop.set_index(\"new_index\", inplace=True)\nmadrid_pop.head(3)\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023_1\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n1\nCentro\n139687\n70770\n68917\n0.493367\n2023\n\n\n2023_2\nArganzuela\n1 de enero de 2023\n28079\nMadrid\n2\n2\nArganzuela\n153304\n71754\n81550\n0.531950\n2023\n\n\n2023_3\nRetiro\n1 de enero de 2023\n28079\nMadrid\n3\n3\nRetiro\n117918\n53458\n64460\n0.546651\n2023\n\n\n\n\n\n\n\nLook at the order of the row and column indexes. This is important becuase “range-based” queries assume you understand the order and arrangement.\n\n# return all content between rows \"2019_1\":\"2018_1\" and columns \"num_people\":\"num_people_women\"\nrange_query = madrid_pop.loc[\"2019_1\":\"2018_1\", \"num_people\":\"num_people_women\"]\n\nrange_query\n\n\n\n\n\n\n\n\nnum_people\nnum_people_men\nnum_people_women\n\n\nnew_index\n\n\n\n\n\n\n\n2019_1\n134881\n67829\n67052\n\n\n2019_2\n153830\n71631\n82199\n\n\n2019_3\n119379\n54098\n65281\n\n\n2019_4\n146148\n64395\n81753\n\n\n2019_5\n145865\n65565\n80300\n\n\n...\n...\n...\n...\n\n\n2019_212\n1851\n952\n899\n\n\n2019_213\n7565\n3648\n3917\n\n\n2019_214\n12388\n5916\n6472\n\n\n2019_215\n7642\n3746\n3896\n\n\n2018_1\n132352\n66320\n66032\n\n\n\n\n153 rows × 3 columns\n\n\n\nThe range query picks up all the elements between the specified IDs. Note that for this to work, the first ID in the range needs to be placed before the second one in the table’s index.\nOnce you know about list and range-based queries, you can combine them!\n\n\n\nHowever, sometimes, you do not know exactly which observations you want, but you do know what conditions they need to satisfy (e.g. areas with more than 2,000 inhabitants). For these cases, DataFrames support selection based on conditions. Let us see a few examples. Suppose you want to select…\n… neighbourhoods wich over time have had less than 50% of women\n\nfewer_women = madrid_pop[madrid_pop[\"ratio_women\"] &lt; 0.5]\nfewer_women\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023_1\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n1\nCentro\n139687\n70770\n68917\n0.493367\n2023\n\n\n2023_12\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n12\nEmbajadores\n46204\n24094\n22110\n0.478530\n2023\n\n\n2023_14\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n14\nJusticia\n18219\n9261\n8958\n0.491685\n2023\n\n\n2023_16\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n16\nSol\n8164\n4239\n3925\n0.480769\n2023\n\n\n2023_81\nFuencarral-El Pardo\n1 de enero de 2023\n28079\nMadrid\n8\n81\nElPardo\n3421\n1716\n1705\n0.498392\n2023\n\n\n2023_106\nLatina\n1 de enero de 2023\n28079\nMadrid\n10\n106\nCuatroVientos\n6122\n3068\n3054\n0.498857\n2023\n\n\n2023_194\nVicálvaro\n1 de enero de 2023\n28079\nMadrid\n19\n194\nElCañaveral\n13054\n6652\n6402\n0.490424\n2023\n\n\n2023_212\nBarajas\n1 de enero de 2023\n28079\nMadrid\n21\n212\nAeropuerto\n1902\n965\n937\n0.492639\n2023\n\n\n2022_1\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n1\nCentro\n139682\n70986\n68696\n0.491803\n2022\n\n\n2022_12\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n12\nEmbajadores\n46444\n24271\n22173\n0.477414\n2022\n\n\n2022_14\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n14\nJusticia\n18015\n9221\n8794\n0.488149\n2022\n\n\n2022_16\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n16\nSol\n8117\n4232\n3885\n0.478625\n2022\n\n\n2022_106\nLatina\n1 de enero de 2022\n28079\nMadrid\n10\n106\nCuatroVientos\n5966\n2996\n2970\n0.497821\n2022\n\n\n2022_194\nVicálvaro\n1 de enero de 2022\n28079\nMadrid\n19\n194\nElCañaveral\n8944\n4525\n4419\n0.494074\n2022\n\n\n2022_212\nBarajas\n1 de enero de 2022\n28079\nMadrid\n21\n212\nAeropuerto\n1895\n967\n928\n0.489710\n2022\n\n\n2021_1\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n1\nCentro\n141236\n71881\n69355\n0.491058\n2021\n\n\n2021_12\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n12\nEmbajadores\n47238\n24767\n22471\n0.475698\n2021\n\n\n2021_14\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n14\nJusticia\n18208\n9291\n8917\n0.489730\n2021\n\n\n2021_16\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n16\nSol\n7993\n4120\n3873\n0.484549\n2021\n\n\n2021_81\nFuencarral-El Pardo\n1 de enero de 2021\n28079\nMadrid\n8\n81\nElPardo\n3443\n1723\n1720\n0.499564\n2021\n\n\n2021_194\nVicálvaro\n1 de enero de 2021\n28079\nMadrid\n19\n194\nElCañaveral\n4430\n2254\n2176\n0.491196\n2021\n\n\n2021_212\nBarajas\n1 de enero de 2021\n28079\nMadrid\n21\n212\nAeropuerto\n1918\n988\n930\n0.484880\n2021\n\n\n2020_1\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n1\nCentro\n140473\n71127\n69346\n0.493661\n2020\n\n\n2020_12\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n12\nEmbajadores\n47048\n24497\n22551\n0.479319\n2020\n\n\n2020_14\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n14\nJusticia\n18021\n9161\n8860\n0.491649\n2020\n\n\n2020_16\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n16\nSol\n7622\n3895\n3727\n0.488979\n2020\n\n\n2020_106\nLatina\n1 de enero de 2020\n28079\nMadrid\n10\n106\nCuatroVientos\n5881\n2958\n2923\n0.497024\n2020\n\n\n2020_194\nVicálvaro\n1 de enero de 2020\n28079\nMadrid\n19\n194\nElCañaveral\n2398\n1230\n1168\n0.487073\n2020\n\n\n2020_212\nBarajas\n1 de enero de 2020\n28079\nMadrid\n21\n212\nAeropuerto\n1900\n975\n925\n0.486842\n2020\n\n\n2019_1\nCentro\n1 de enero de 2019\n28079\nMadrid\n1\n1\nCentro\n134881\n67829\n67052\n0.497120\n2019\n\n\n2019_12\nCentro\n1 de enero de 2019\n28079\nMadrid\n1\n12\nEmbajadores\n45259\n23390\n21869\n0.483197\n2019\n\n\n2019_14\nCentro\n1 de enero de 2019\n28079\nMadrid\n1\n14\nJusticia\n17153\n8675\n8478\n0.494258\n2019\n\n\n2019_16\nCentro\n1 de enero de 2019\n28079\nMadrid\n1\n16\nSol\n7337\n3749\n3588\n0.489028\n2019\n\n\n2019_106\nLatina\n1 de enero de 2019\n28079\nMadrid\n10\n106\nCuatroVientos\n5748\n2909\n2839\n0.493911\n2019\n\n\n2019_194\nVicálvaro\n1 de enero de 2019\n28079\nMadrid\n19\n194\nElCañaveral\n1530\n785\n745\n0.486928\n2019\n\n\n2019_212\nBarajas\n1 de enero de 2019\n28079\nMadrid\n21\n212\nAeropuerto\n1851\n952\n899\n0.485683\n2019\n\n\n2018_1\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n1\nCentro\n132352\n66320\n66032\n0.498912\n2018\n\n\n2018_12\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n12\nEmbajadores\n44630\n23031\n21599\n0.483957\n2018\n\n\n2018_14\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n14\nJusticia\n16578\n8334\n8244\n0.497286\n2018\n\n\n2018_16\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n16\nSol\n7201\n3672\n3529\n0.490071\n2018\n\n\n2018_106\nLatina\n1 de enero de 2018\n28079\nMadrid\n10\n106\nCuatroVientos\n5662\n2870\n2792\n0.493112\n2018\n\n\n2018_194\nVicálvaro\n1 de enero de 2018\n28079\nMadrid\n19\n194\nElCañaveral\n945\n490\n455\n0.481481\n2018\n\n\n2018_212\nBarajas\n1 de enero de 2018\n28079\nMadrid\n21\n212\nAeropuerto\n1794\n922\n872\n0.486065\n2018\n\n\n\n\n\n\n\n… most populated area across all years:\n\nlargest_hood_num = madrid_pop[\"num_people\"].max()\nlargest_hood = madrid_pop[madrid_pop[\"num_people\"] == largest_hood_num]\nlargest_hood\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023_11\nCarabanchel\n1 de enero de 2023\n28079\nMadrid\n11\n11\nCarabanchel\n262339\n122632\n139707\n0.532544\n2023\n\n\n\n\n\n\n\nIf you are interested, more detail about query is available in the pandas documentation. This is another way of slicing Dataframes, but for now we will stay with the loc function.\n\n\n\nNow, all of these queries can be combined with each other for further flexibility. For example, imagine you want to know the areas that have more than 100K inhabitants and have over 50% of women.\n\nwomen_power = madrid_pop.loc[\n    # the & symbol will combine True conditions for the left result set\n    # with the True conditions from the right result set\n    # there is also an \"or\" operator which uses the \"Pipe\" symbol \"|\" instead\n    (madrid_pop[\"num_people_women\"] &gt; 100000)\n    & (madrid_pop[\"ratio_women\"] &gt; 0.5)\n]\nwomen_power\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023_8\nFuencarral-El Pardo\n1 de enero de 2023\n28079\nMadrid\n8\n8\nFuencarral-El Pardo\n248443\n116944\n131499\n0.529292\n2023\n\n\n2023_10\nLatina\n1 de enero de 2023\n28079\nMadrid\n10\n10\nLatina\n241672\n112093\n129579\n0.536177\n2023\n\n\n2023_11\nCarabanchel\n1 de enero de 2023\n28079\nMadrid\n11\n11\nCarabanchel\n262339\n122632\n139707\n0.532544\n2023\n\n\n2023_13\nPuente de Vallecas\n1 de enero de 2023\n28079\nMadrid\n13\n13\nPuente de Vallecas\n241603\n114542\n127061\n0.525908\n2023\n\n\n2023_15\nCiudad Lineal\n1 de enero de 2023\n28079\nMadrid\n15\n15\nCiudad Lineal\n220345\n100759\n119586\n0.542722\n2023\n\n\n2023_16\nHortaleza\n1 de enero de 2023\n28079\nMadrid\n16\n16\nHortaleza\n198391\n94100\n104291\n0.525684\n2023\n\n\n2022_8\nFuencarral-El Pardo\n1 de enero de 2022\n28079\nMadrid\n8\n8\nFuencarral-El Pardo\n246281\n115955\n130326\n0.529176\n2022\n\n\n2022_10\nLatina\n1 de enero de 2022\n28079\nMadrid\n10\n10\nLatina\n237048\n109928\n127120\n0.536263\n2022\n\n\n2022_11\nCarabanchel\n1 de enero de 2022\n28079\nMadrid\n11\n11\nCarabanchel\n255514\n119381\n136133\n0.532781\n2022\n\n\n2022_13\nPuente de Vallecas\n1 de enero de 2022\n28079\nMadrid\n13\n13\nPuente de Vallecas\n235638\n111748\n123890\n0.525764\n2022\n\n\n2022_15\nCiudad Lineal\n1 de enero de 2022\n28079\nMadrid\n15\n15\nCiudad Lineal\n213905\n97357\n116548\n0.544859\n2022\n\n\n2022_16\nHortaleza\n1 de enero de 2022\n28079\nMadrid\n16\n16\nHortaleza\n195017\n92532\n102485\n0.525518\n2022\n\n\n2021_8\nFuencarral-El Pardo\n1 de enero de 2021\n28079\nMadrid\n8\n8\nFuencarral-ElPardo\n247692\n116520\n131172\n0.529577\n2021\n\n\n2021_10\nLatina\n1 de enero de 2021\n28079\nMadrid\n10\n10\nLatina\n240155\n111209\n128946\n0.536928\n2021\n\n\n2021_11\nCarabanchel\n1 de enero de 2021\n28079\nMadrid\n11\n11\nCarabanchel\n258633\n120600\n138033\n0.533702\n2021\n\n\n2021_13\nPuente de Vallecas\n1 de enero de 2021\n28079\nMadrid\n13\n13\nPuentedeVallecas\n239057\n113355\n125702\n0.525824\n2021\n\n\n2021_15\nCiudad Lineal\n1 de enero de 2021\n28079\nMadrid\n15\n15\nCiudadLineal\n216818\n98514\n118304\n0.545637\n2021\n\n\n2021_16\nHortaleza\n1 de enero de 2021\n28079\nMadrid\n16\n16\nHortaleza\n193228\n91585\n101643\n0.526026\n2021\n\n\n2020_8\nFuencarral-El Pardo\n1 de enero de 2020\n28079\nMadrid\n8\n8\nFuencarral-ElPardo\n249973\n117640\n132333\n0.529389\n2020\n\n\n2020_10\nLatina\n1 de enero de 2020\n28079\nMadrid\n10\n10\nLatina\n242139\n112282\n129857\n0.536291\n2020\n\n\n2020_11\nCarabanchel\n1 de enero de 2020\n28079\nMadrid\n11\n11\nCarabanchel\n260196\n121317\n138879\n0.533748\n2020\n\n\n2020_13\nPuente de Vallecas\n1 de enero de 2020\n28079\nMadrid\n13\n13\nPuentedeVallecas\n240867\n114235\n126632\n0.525734\n2020\n\n\n2020_15\nCiudad Lineal\n1 de enero de 2020\n28079\nMadrid\n15\n15\nCiudadLineal\n219867\n99966\n119901\n0.545334\n2020\n\n\n2020_16\nHortaleza\n1 de enero de 2020\n28079\nMadrid\n16\n16\nHortaleza\n193264\n91659\n101605\n0.525732\n2020\n\n\n2019_8\nFuencarral-El Pardo\n1 de enero de 2019\n28079\nMadrid\n8\n8\nFuencarral-ElPardo\n246021\n115797\n130224\n0.529321\n2019\n\n\n2019_10\nLatina\n1 de enero de 2019\n28079\nMadrid\n10\n10\nLatina\n238154\n110401\n127753\n0.536430\n2019\n\n\n2019_11\nCarabanchel\n1 de enero de 2019\n28079\nMadrid\n11\n11\nCarabanchel\n253040\n117802\n135238\n0.534453\n2019\n\n\n2019_13\nPuente de Vallecas\n1 de enero de 2019\n28079\nMadrid\n13\n13\nPuentedeVallecas\n234770\n111183\n123587\n0.526417\n2019\n\n\n2019_15\nCiudad Lineal\n1 de enero de 2019\n28079\nMadrid\n15\n15\nCiudadLineal\n216270\n98370\n117900\n0.545152\n2019\n\n\n2018_8\nFuencarral-El Pardo\n1 de enero de 2018\n28079\nMadrid\n8\n8\nFuencarral-ElPardo\n242928\n114433\n128495\n0.528943\n2018\n\n\n2018_10\nLatina\n1 de enero de 2018\n28079\nMadrid\n10\n10\nLatina\n235785\n109392\n126393\n0.536052\n2018\n\n\n2018_11\nCarabanchel\n1 de enero de 2018\n28079\nMadrid\n11\n11\nCarabanchel\n248220\n115525\n132695\n0.534586\n2018\n\n\n2018_13\nPuente de Vallecas\n1 de enero de 2018\n28079\nMadrid\n13\n13\nPuentedeVallecas\n230488\n109044\n121444\n0.526899\n2018\n\n\n2018_15\nCiudad Lineal\n1 de enero de 2018\n28079\nMadrid\n15\n15\nCiudadLineal\n214463\n97301\n117162\n0.546304\n2018\n\n\n\n\n\n\n\n\n\n\nAmong the many operations DataFrame objects support, one of the most useful ones is to sort a table based on a given column. For example, imagine you want to sort the table by the ratio of women:\n\nmadrid_sorted = madrid_pop.sort_values(\"ratio_women\", ascending=False)\nmadrid_sorted\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2018_158\nCiudad Lineal\n1 de enero de 2018\n28079\nMadrid\n15\n158\nAtalaya\n1575\n656\n919\n0.583492\n2018\n\n\n2019_158\nCiudad Lineal\n1 de enero de 2019\n28079\nMadrid\n15\n158\nAtalaya\n1568\n654\n914\n0.582908\n2019\n\n\n2023_158\nCiudad Lineal\n1 de enero de 2023\n28079\nMadrid\n15\n158\nAtalaya\n1622\n691\n931\n0.573983\n2023\n\n\n2020_158\nCiudad Lineal\n1 de enero de 2020\n28079\nMadrid\n15\n158\nAtalaya\n1555\n667\n888\n0.571061\n2020\n\n\n2020_45\nSalamanca\n1 de enero de 2020\n28079\nMadrid\n4\n45\nLista\n21211\n9111\n12100\n0.570459\n2020\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2020_12\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n12\nEmbajadores\n47048\n24497\n22551\n0.479319\n2020\n\n\n2022_16\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n16\nSol\n8117\n4232\n3885\n0.478625\n2022\n\n\n2023_12\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n12\nEmbajadores\n46204\n24094\n22110\n0.478530\n2023\n\n\n2022_12\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n12\nEmbajadores\n46444\n24271\n22173\n0.477414\n2022\n\n\n2021_12\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n12\nEmbajadores\n47238\n24767\n22471\n0.475698\n2021\n\n\n\n\n912 rows × 12 columns\n\n\n\nGiven the rates differ, it may be better to sort by neighbourhood and then by year.\n\nsort_ls = [\"code_neighbourhood\", \"year\"]\nmadrid_sorted = madrid_pop.sort_values(sort_ls, ascending=True)\nmadrid_sorted\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2018_1\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n1\nCentro\n132352\n66320\n66032\n0.498912\n2018\n\n\n2019_1\nCentro\n1 de enero de 2019\n28079\nMadrid\n1\n1\nCentro\n134881\n67829\n67052\n0.497120\n2019\n\n\n2020_1\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n1\nCentro\n140473\n71127\n69346\n0.493661\n2020\n\n\n2021_1\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n1\nCentro\n141236\n71881\n69355\n0.491058\n2021\n\n\n2022_1\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n1\nCentro\n139682\n70986\n68696\n0.491803\n2022\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2019_97\nMoncloa-Aravaca\n1 de enero de 2019\n28079\nMadrid\n9\n97\nAravaca\n26823\n12619\n14204\n0.529546\n2019\n\n\n2020_97\nMoncloa-Aravaca\n1 de enero de 2020\n28079\nMadrid\n9\n97\nAravaca\n27503\n12899\n14604\n0.530997\n2020\n\n\n2021_97\nMoncloa-Aravaca\n1 de enero de 2021\n28079\nMadrid\n9\n97\nAravaca\n27568\n12896\n14672\n0.532211\n2021\n\n\n2022_97\nMoncloa-Aravaca\n1 de enero de 2022\n28079\nMadrid\n9\n97\nAravaca\n27323\n12779\n14544\n0.532299\n2022\n\n\n2023_97\nMoncloa-Aravaca\n1 de enero de 2023\n28079\nMadrid\n9\n97\nAravaca\n27445\n12836\n14609\n0.532301\n2023\n\n\n\n\n912 rows × 12 columns\n\n\n\nThis allows you to do so-called hierarchical sorting: sort first based on one column then based on another.\n\n\n\n\nThe next step to continue exploring a dataset is to get a feel for what it looks like, visually. We have already learnt how to uncover and inspect specific parts of the data to check for particular cases we might be interested in. Now, we will see how to plot the data to get a sense of the overall distribution of values. For that, we can use the plotting capabilities of pandas.\n\n\nOne of the most common graphical devices to display the distribution of values in a variable is a histogram. Values are assigned into groups of equal intervals, and the groups are plotted as bars rising as high as the number of values into the group.\nA histogram is easily created with the following command. In this case, let’s have a look at the shape of the overall numbers of people:\n\nmadrid_pop[\"num_people\"].plot.hist(bins=15)\n\n\n\n\n\n\n\n\nHowever, the default pandas plots can be a bit dull. A better option is to use another package, called seaborn.\nseaborn is, by convention, imported as sns. Seaborn is a humorous reference to Samuel Normal Seaborn, a fictional character The West Wing show.\nThe same plot using seaborn has more agreeable default styles and more customisability.\n\nimport seaborn as sns\n\n# Set the style\nsns.set_style(\"darkgrid\")\nsns.histplot(madrid_pop[\"num_people\"], kde=True, bins=15)\n\n\n\n\n\n\n\n\nNote we are using sns instead of pd, as the function belongs to seaborn instead of pandas.\nWe can quickly see most of the areas have seen somewhere between 0 and 50K people; and very few have more than 200K. However, remember that in this case we are visualizing all years together, which could lead to misinterpretations.\n\n\n\nAnother very common way of visually displaying a variable is with a line or a bar chart. For example, if you want to generate a line plot of the (sorted) total population per year:\n\ntotal_people_per_year = madrid_pop.groupby(\"year\")[\"num_people\"].sum()\ntotal_people_per_year.plot()\n\n\n\n\n\n\n\n\nWhat is evident is the impact of COVID on the total population. But understanding that the data is reported on the 1st of January of each year is crucial to understand why you see the offset on the dates.\nFor a bar plot all you need to do is to change from plot to plot.bar:\n\ntotal_people_per_year.plot.bar()\n\n\n\n\n\n\n\n\nLet’s try to plot the ratio_women per neighbourhood, to see if we spot anything in particular.\n\nsns.lineplot(\n    x=\"year\",\n    y=\"ratio_women\",\n    hue=\"neighbourhood\",\n    data=madrid_pop.sort_values(\"year\", ascending=True),\n    legend=False,\n)\n\n\n\n\n\n\n\n\nWe can see some outliers, but the reality is that the data is hard to read so we probably would need some further analysis and visual considerations to efficiently communicate any possible trends.\n\n\n\n\n\nClean vs. Tidy\nThis section is a bit more advanced and hence considered optional. Feel free to skip it and return later when you feel more confident.\n\nOnce you can read your data in, explore specific cases, and have a first visual approach to the entire set, the next step can be preparing it for more sophisticated analysis. Maybe you are thinking of modelling it through regression, or on creating subgroups in the dataset with particular characteristics, or maybe you simply need to present summary measures that relate to a slightly different arrangement of the data than you have been presented with.\nFor all these cases, you first need what statistician, and general R wizard, Hadley Wickham calls “tidy data”. The general idea to “tidy” your data is to convert them from whatever structure they were handed to you into one that allows convenient and standardized manipulation, and that supports directly inputting the data into what he calls “tidy” analysis tools. But, at a more practical level, what is exactly “tidy data”? In Wickham’s own words:\n\nTidy data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types.\n\nHe then goes on to list the three fundamental characteristics of “tidy data”:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\nIf you are further interested in the concept of “tidy data”, we recommend the original paper (open access) and the associated public repository.\n\n\n\nOne of the advantage of tidy datasets is they allow advanced transformations in a more direct way. One of the most common ones is what is called “group-by” operations. These originated in the world of databases, and allow you to group observations from a data table by labels, index, or category, and to then apply operations on the data on a group by group basis.\nFor example, given our DataFrame, we might want to compute the total sum of the population by each district. This task can be split into two different steps:\n\nGroup the table in each of the different districts.\nCompute the sum of num_people for each of them.\n\nTo do this in pandas, meet one of its workhorses and also one of the reasons why the library has become so popular: the groupby operator.\n\nmad_grouped = madrid_pop.groupby(\"year\")\nmad_grouped\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x1442cc850&gt;\n\n\nThe object mad_grouped still hasn’t computed anything. It is only a convenient way of specifying the grouping. But this allows us then to perform a multitude of operations on it. For our example, the sum is calculated as follows:\n\nmad_grouped.sum(numeric_only=True)\n\n\n\n\n\n\n\n\ncode_municipality\nnum_people\nnum_people_men\nnum_people_women\nratio_women\n\n\nyear\n\n\n\n\n\n\n\n\n\n2018\n4268008\n6443648\n3000680\n3442968\n81.100391\n\n\n2019\n4268008\n6532252\n3042356\n3489896\n81.108315\n\n\n2020\n4268008\n6669460\n3109464\n3559996\n81.047788\n\n\n2021\n4268008\n6624620\n3090314\n3534306\n81.013452\n\n\n2022\n4268008\n6573324\n3069648\n3503676\n80.939052\n\n\n2023\n4268008\n6679862\n3119732\n3560130\n80.942860\n\n\n\n\n\n\n\nSimilarly, we can also obtain a summary of each group:\n\nmad_grouped.describe()\n\n\n\n\n\n\n\n\ncode_municipality\nnum_people\n...\nnum_people_women\nratio_women\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ncount\nmean\n...\n75%\nmax\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nyear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2018\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n42392.421053\n...\n21771.75\n132695.0\n152.0\n0.533555\n0.018390\n0.481481\n0.521591\n0.534767\n0.545880\n0.583492\n\n\n2019\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n42975.342105\n...\n21987.00\n135238.0\n152.0\n0.533607\n0.018198\n0.483197\n0.522356\n0.534355\n0.545650\n0.582908\n\n\n2020\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n43878.026316\n...\n22623.75\n138879.0\n152.0\n0.533209\n0.017944\n0.479319\n0.522586\n0.533905\n0.545599\n0.571061\n\n\n2021\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n43583.026316\n...\n22530.75\n138033.0\n152.0\n0.532983\n0.017984\n0.475698\n0.522263\n0.533724\n0.545682\n0.568987\n\n\n2022\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n43245.552632\n...\n22296.50\n136133.0\n152.0\n0.532494\n0.017716\n0.477414\n0.521691\n0.533305\n0.544920\n0.567140\n\n\n2023\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n43946.460526\n...\n22742.75\n139707.0\n152.0\n0.532519\n0.017486\n0.478530\n0.522637\n0.533363\n0.544488\n0.573983\n\n\n\n\n6 rows × 40 columns\n\n\n\nWe will not get into it today as it goes beyond the basics this chapter covers, but keep in mind that groupby allows us to not only call generic functions (like sum or describe), but also custom functions. This opens the door for virtually any kind of transformation and aggregation possible.\nAdditional reading\n\nA good introduction to data manipulation in Python is Wes McKinney’s Python for Data Analysis.\nTo further explore some of the visualization capabilities, the Python library seaborn is an excellent choice. Its online tutorial is a fantastic place to start.\nA good extension is Hadley Wickham’s “Tidy data” paper, which presents a very popular way of organising tabular data for efficient manipulation.",
    "crumbs": [
      "Basic Coding",
      "Pandas & Data"
    ]
  },
  {
    "objectID": "chapter_2/pandas.html#acknowledgements",
    "href": "chapter_2/pandas.html#acknowledgements",
    "title": "Pandas & Data",
    "section": "",
    "text": "This section is adapted from the Spatial Data Science for Social Geography by Martin Fleischman, which in turn is based on A Course on Geographic Data Science by Dani Arribas-Bel, both licensed under CC-BY-SA 4.0. The content and datasets were reworked to accommodate the Madrid neighbourhood population statistics dataset.",
    "crumbs": [
      "Basic Coding",
      "Pandas & Data"
    ]
  },
  {
    "objectID": "chapter_2/pandas.html#munging-and-wrangling",
    "href": "chapter_2/pandas.html#munging-and-wrangling",
    "title": "Pandas & Data",
    "section": "",
    "text": "Real-world datasets are messy. There is no way around it: datasets have “holes” (missing data), the number of formats in which data can be stored can be endless, and the best structure to share data is not always the optimal for analysis. Hence the need to “munge” or “wrangle” data. Much of the time spent in what is called Data Science is related not only to modelling and insight but more often than not has to do with much more basic and less exotic tasks such as obtaining the data, data cleaning, and other preparation which makes analysis possible.\nSurprisingly, very little has been published on patterns, techniques, and best practices for quick and efficient data cleaning, manipulation, and transformation because of how labour-intensive this aspect is. In this session, we will use real-world datasets and learn how to transform and manipulate these, if necessary, prior to analysis. For this, we will introduce some of the bread-and-butter of data analysis and scientific computing in Python. These are fundamental tools that are widely and consistently used on almost any tasks relating to data analysis.\nThis notebook covers the basics and the content that needs to be grasped, and discusses several patterns to clean and structure data properly, including tidying, subsetting, and aggregating. We will conclude with some basic visualisation. An additional extension presents more advanced tricks to manipulate tabular data.",
    "crumbs": [
      "Basic Coding",
      "Pandas & Data"
    ]
  },
  {
    "objectID": "chapter_2/pandas.html#dataset",
    "href": "chapter_2/pandas.html#dataset",
    "title": "Pandas & Data",
    "section": "",
    "text": "We will be exploring demographic characteristics of Madrid. The data has been aggregated to a neighbourhood level by the statistic’s office of Madrid’s City Hall. It contains information per year (from 2018 to 2023) and theme.\nAs with many datasets that will be used during this course, the data was originally found in an online data portal at the following link\nThe main tool we will use for this task is the pandas package. As with any Python module or package, the Pandas package has to be imported into the Notebook prior to usage.\n\nimport pandas as pd\n\nPandas let’s us work with datasets, and stores the information in a DataFrame consisting of rows and columns. Here, we will read an online csv dataset into a Pandas DataFrame. Pandas can open local files directly, but also has the nifty ability to download files directly from the web:\n\n# here we will directly fetch the CSV dataset and save it as a Pandas DataFrame\nmadrid_pop = pd.read_csv(\n    \"https://datos.madrid.es/egob/catalogo/300557-0-poblacion-distrito-barrio.csv\",\n    sep=\";\",\n)\n\n\nDelimiters\nIn this case we are using the Pandas read_csv method because the source file is a csv. By default, csv files are assumed to use commas for data delimitation, but this file uses semi-colons instead. This is why we are passing the optional sep parameter to specify a ; delimeter.\n\n\nFormats\nNote that pandas allows for many more formats to be read and written. A full list of formats supported may be found in the documentation.\n\nIt is also possible to download the file and read it locally. In this case, you would download the file by clicking on this link. Then place it in the same folder as the notebook where you intend to read it from, and run the below cell.\n\n# remember, this cell will only work if you have downloaded the file\n# and, if the filepath is correct!\n# uncomment the below lines to run!\n# madrid_pop = pd.read_csv(\n#     \"poblacion_1_enero.csv\",\n#     sep=\";\",\n# )",
    "crumbs": [
      "Basic Coding",
      "Pandas & Data"
    ]
  },
  {
    "objectID": "chapter_2/pandas.html#pandas-101",
    "href": "chapter_2/pandas.html#pandas-101",
    "title": "Pandas & Data",
    "section": "",
    "text": "Now, we are ready to start playing and interrogating the dataset! What you have at your fingertips is a table summarising, for each of the districts in Madrid, how many people lived there by gender. These tables are called DataFrame objects, and they have a lot of functionality built-in to explore and manipulate the data they contain. Let’s explore a few of those cool tricks!\n\n\nThe first aspect worth spending a bit of time on is the structure of a DataFrame. You can print it by simply typing its name:\n\nmadrid_pop\n\n\n\n\n\n\n\n\nfecha\ncod_municipio\nmunicipio\ncod_distrito\ndistrito\ncod_barrio\nbarrio\nnum_personas\nnum_personas_hombres\nnum_personas_mujeres\n\n\n\n\n0\n1 de enero de 2023\n28079\nMadrid\n1\nCentro\n1\nCentro\n139.687\n70.770\n68.917\n\n\n1\n1 de enero de 2023\n28079\nMadrid\n2\nArganzuela\n2\nArganzuela\n153.304\n71.754\n81.550\n\n\n2\n1 de enero de 2023\n28079\nMadrid\n3\nRetiro\n3\nRetiro\n117.918\n53.458\n64.460\n\n\n3\n1 de enero de 2023\n28079\nMadrid\n4\nSalamanca\n4\nSalamanca\n145.702\n64.452\n81.250\n\n\n4\n1 de enero de 2023\n28079\nMadrid\n5\nChamartín\n5\nChamartín\n144.796\n65.245\n79.551\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n913\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n212\nAeropuerto\n1.794\n922\n872\n\n\n914\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n213\nCascoHistóricodeBarajas\n7.336\n3.550\n3.786\n\n\n915\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n214\nTimón\n11.750\n5.651\n6.099\n\n\n916\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n215\nCorralejos\n7.510\n3.657\n3.853\n\n\n917\n1 de enero de 2018\n28079\nMadrid\nTodos\nTodos\nTodos\nTodos\n3.221.824\n1.500.340\n1.721.484\n\n\n\n\n918 rows × 10 columns\n\n\n\nAs you can expect, the dataset was provided in the original local language, so to make things a little easier, we are going to translate the column names using one list of texts. for this we will reassign the column names corresponding to their English equivalents.\n\nNote that there are multiple ways of arriving at the same output, the below is just one of them\n\n\nen_cols = [\n    \"date\",\n    \"code_municipality\",\n    \"municipality\",\n    \"code_district\",\n    \"district\",\n    \"code_neighbourhood\",\n    \"neighbourhood\",\n    \"num_people\",\n    \"num_people_men\",\n    \"num_people_women\",\n]\nmadrid_pop.columns = en_cols\nmadrid_pop\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ndistrict\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\n\n\n\n\n0\n1 de enero de 2023\n28079\nMadrid\n1\nCentro\n1\nCentro\n139.687\n70.770\n68.917\n\n\n1\n1 de enero de 2023\n28079\nMadrid\n2\nArganzuela\n2\nArganzuela\n153.304\n71.754\n81.550\n\n\n2\n1 de enero de 2023\n28079\nMadrid\n3\nRetiro\n3\nRetiro\n117.918\n53.458\n64.460\n\n\n3\n1 de enero de 2023\n28079\nMadrid\n4\nSalamanca\n4\nSalamanca\n145.702\n64.452\n81.250\n\n\n4\n1 de enero de 2023\n28079\nMadrid\n5\nChamartín\n5\nChamartín\n144.796\n65.245\n79.551\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n913\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n212\nAeropuerto\n1.794\n922\n872\n\n\n914\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n213\nCascoHistóricodeBarajas\n7.336\n3.550\n3.786\n\n\n915\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n214\nTimón\n11.750\n5.651\n6.099\n\n\n916\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n215\nCorralejos\n7.510\n3.657\n3.853\n\n\n917\n1 de enero de 2018\n28079\nMadrid\nTodos\nTodos\nTodos\nTodos\n3.221.824\n1.500.340\n1.721.484\n\n\n\n\n918 rows × 10 columns\n\n\n\nThe default printing is truncated to keep a compact view, but is enough to convey its structure. DataFrame objects have two dimensions: rows and columns. Rows are typically identified with an index column and columns are typically identified with names describing their content. The above example shows how the column names were automatically lifted from the csv file’s column headers.\nWe can set the row index using the set_index() method. Let’s set the index to use the district column.\n\n# Remove leading and trailing spaces from 'distrito'\nmadrid_pop[\"district\"] = madrid_pop[\"district\"].str.strip()\n# set the index based on the district column\nmadrid_pop.set_index(\"district\", inplace=True)\n\nColumns can be assigned as one of various forms of data such as integers as int, decimals as float, text as str, or if pandas is unable to use a standard type, it might use the object type as a catch-all type.\nTo extract a single column from this DataFrame, specify its name in square brackets ([]). Note that the name is a string - a piece of text - which needs to be denoted with single (') or double quotes (\"). Without the quotes, Python will think you are referring to a variable, and will then complain that the variable can’t be found!\nA single column is not a DataFrame but a Series, which also means that a DataFrame is a collection of Series!\n\n# this will fetch and return the num_people_women column as a Series\nmadrid_pop[\"num_people_women\"]\n\ndistrict\nCentro           68.917\nArganzuela       81.550\nRetiro           64.460\nSalamanca        81.250\nChamartín        79.551\n                ...    \nBarajas             872\nBarajas           3.786\nBarajas           6.099\nBarajas           3.853\nTodos         1.721.484\nName: num_people_women, Length: 918, dtype: object\n\n\n\n\n\nYou can specifically print the DataFrames top (or bottom) lines by passing a number to the method head or tail. For example, for the top or bottom three lines:\n\nmadrid_pop.head(3)\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\n\n\ndistrict\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n1\nCentro\n139.687\n70.770\n68.917\n\n\nArganzuela\n1 de enero de 2023\n28079\nMadrid\n2\n2\nArganzuela\n153.304\n71.754\n81.550\n\n\nRetiro\n1 de enero de 2023\n28079\nMadrid\n3\n3\nRetiro\n117.918\n53.458\n64.460\n\n\n\n\n\n\n\n\nmadrid_pop.tail(3)\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\n\n\ndistrict\n\n\n\n\n\n\n\n\n\n\n\n\n\nBarajas\n1 de enero de 2018\n28079\nMadrid\n21\n214\nTimón\n11.750\n5.651\n6.099\n\n\nBarajas\n1 de enero de 2018\n28079\nMadrid\n21\n215\nCorralejos\n7.510\n3.657\n3.853\n\n\nTodos\n1 de enero de 2018\n28079\nMadrid\nTodos\nTodos\nTodos\n3.221.824\n1.500.340\n1.721.484\n\n\n\n\n\n\n\nInspecting datasets is vital to find errors that might skew your analysis. The last row of the dataset contains the sums of other column values ( Spanish: Todos -&gt; English: All ).\nBefore continuing, let’s fix this by removing these columns from the dataset. First, let’s check if we have any more occurrences of Todos. We can use Pandas’ loc indexer to fetch rows where the index column contains the text string Todos.\n\nmadrid_pop.loc[\"Todos\"]\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\n\n\ndistrict\n\n\n\n\n\n\n\n\n\n\n\n\n\nTodos\n1 de enero de 2023\n28079\nMadrid\nTodos\nTodos\nTodos\n3.339.931\n1.559.866\n1.780.065\n\n\nTodos\n1 de enero de 2022\n28079\nMadrid\nTodos\nTodos\nTodos\n3.286.662\n1.534.824\n1.751.838\n\n\nTodos\n1 de enero de 2021\n28079\nMadrid\nTodos\nTodos\nTodos\n3.312.310\n1.545.157\n1.767.153\n\n\nTodos\n1 de enero de 2020\n28079\nMadrid\nTodos\nTodos\nTodos\n3.334.730\n1.554.732\n1.779.998\n\n\nTodos\n1 de enero de 2019\n28079\nMadrid\nTodos\nTodos\nTodos\n3.266.126\n1.521.178\n1.744.948\n\n\nTodos\n1 de enero de 2018\n28079\nMadrid\nTodos\nTodos\nTodos\n3.221.824\n1.500.340\n1.721.484\n\n\n\n\n\n\n\nNow that we know that for every year the data has been aggregated and added back as a new row, we know that removing the last row would not be enough to correct our DataFrame.\nWe will make use of the drop function in combination with the native loc function of DataFrames.\n\nprint(\"Rows before droping values: \", len(madrid_pop))\nmadrid_pop.drop(index=\"Todos\", inplace=True)\nprint(\"Rows after droping values: \", len(madrid_pop))\n\nRows before droping values:  918\nRows after droping values:  912\n\n\nNow, let’s get an overview of the table:\n\nmadrid_pop.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 912 entries, Centro to Barajas\nData columns (total 9 columns):\n #   Column              Non-Null Count  Dtype \n---  ------              --------------  ----- \n 0   date                912 non-null    object\n 1   code_municipality   912 non-null    int64 \n 2   municipality        912 non-null    object\n 3   code_district       912 non-null    object\n 4   code_neighbourhood  912 non-null    object\n 5   neighbourhood       912 non-null    object\n 6   num_people          912 non-null    object\n 7   num_people_men      912 non-null    object\n 8   num_people_women    912 non-null    object\ndtypes: int64(1), object(8)\nmemory usage: 71.2+ KB\n\n\nCan you spot something wrong?\nInteger numbers are sometimes stored as text values, especially if the input dataset contained commas, periods, or null data values. The actual values of population counts would be better stored as full integers so that we can more effectively work with this data as a numeric data type. Let’s take the following steps:\n\nCreate a list containg the column names that have people counts\nLoop through the list, removing periods from the numbers, then casting the values to integer types while allowing for null data values\n\n\n# List of columns to process\ncolumns_to_process = [\"num_people\", \"num_people_men\", \"num_people_women\"]\n\n# Loop through columns\nfor column in columns_to_process:\n    # create a copy of the column\n    dirty_numbers = madrid_pop[column]\n    # remove \".\" from the numbers\n    clean_numbers = dirty_numbers.str.replace(\".\", \"\")\n    # convert to numeric - coerce will convert non numeric values to NaN (Not a Number) or \"null\" types\n    numeric_numbers = pd.to_numeric(clean_numbers, errors=\"coerce\")\n    # overwrite the original column\n    madrid_pop[column] = numeric_numbers\n\nmadrid_pop.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 912 entries, Centro to Barajas\nData columns (total 9 columns):\n #   Column              Non-Null Count  Dtype \n---  ------              --------------  ----- \n 0   date                912 non-null    object\n 1   code_municipality   912 non-null    int64 \n 2   municipality        912 non-null    object\n 3   code_district       912 non-null    object\n 4   code_neighbourhood  912 non-null    object\n 5   neighbourhood       912 non-null    object\n 6   num_people          912 non-null    int64 \n 7   num_people_men      912 non-null    int64 \n 8   num_people_women    912 non-null    int64 \ndtypes: int64(4), object(5)\nmemory usage: 71.2+ KB\n\n\n\n\n\n\nmadrid_pop.describe()\n\n\n\n\n\n\n\n\ncode_municipality\nnum_people\nnum_people_men\nnum_people_women\n\n\n\n\ncount\n912.0\n912.000000\n912.000000\n912.000000\n\n\nmean\n28079.0\n43336.804825\n20210.739035\n23126.065789\n\n\nstd\n0.0\n51564.255101\n24072.657139\n27522.444389\n\n\nmin\n28079.0\n945.000000\n490.000000\n455.000000\n\n\n25%\n28079.0\n17330.500000\n8132.000000\n9267.250000\n\n\n50%\n28079.0\n24700.500000\n11580.000000\n13640.000000\n\n\n75%\n28079.0\n42166.500000\n19700.250000\n22349.750000\n\n\nmax\n28079.0\n262339.000000\n122632.000000\n139707.000000\n\n\n\n\n\n\n\nNote how the output is also a DataFrame object, so you can manipulate it the same way that you would with the original table (e.g. writing it to a file).\nIn this case, the summary might be better presented if the table is “transposed”:\n\nmadrid_pop[columns_to_process].describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nnum_people\n912.0\n43336.804825\n51564.255101\n945.0\n17330.50\n24700.5\n42166.50\n262339.0\n\n\nnum_people_men\n912.0\n20210.739035\n24072.657139\n490.0\n8132.00\n11580.0\n19700.25\n122632.0\n\n\nnum_people_women\n912.0\n23126.065789\n27522.444389\n455.0\n9267.25\n13640.0\n22349.75\n139707.0\n\n\n\n\n\n\n\nEqually, common descriptive statistics are also available. To obtain minimum values for each column, you can use .min().\n\nmadrid_pop.min()\n\ndate                  1 de enero de 2018\ncode_municipality                  28079\nmunicipality                      Madrid\ncode_district                          1\ncode_neighbourhood                     1\nneighbourhood                 Arganzuela\nnum_people                           945\nnum_people_men                       490\nnum_people_women                     455\ndtype: object\n\n\nOr to obtain a minimum for a single column only.\n\nmadrid_pop[\"num_people_women\"].min()\n\n455\n\n\nNote here how you have restricted the calculation of the minimum value to one column only by getting the Series and calling .min() on that.\nSimilarly, you can restrict the calculations to a single district using .loc[] indexer:\n\nmadrid_pop.loc[\"Centro\"].min()\n\ndate                  1 de enero de 2018\ncode_municipality                  28079\nmunicipality                      Madrid\ncode_district                          1\ncode_neighbourhood                     1\nneighbourhood                     Centro\nnum_people                          7201\nnum_people_men                      3672\nnum_people_women                    3529\ndtype: object\n\n\nLet’s see when and where the said minimum occurred.\n\n# we can use comparators to index into the DataFrame where a specific column equals a specific value\nmadrid_pop[madrid_pop[\"num_people_women\"] == 3529]\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\n\n\ndistrict\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n16\nSol\n7201\n3672\n3529\n\n\n\n\n\n\n\n\n\n\nYou can generate new variables by applying operations to existing ones. For example, you can calculate the ratio of women.\n\nratio_women = madrid_pop[\"num_people_women\"] / madrid_pop[\"num_people\"]\nratio_women.head()\n\ndistrict\nCentro        0.493367\nArganzuela    0.531950\nRetiro        0.546651\nSalamanca     0.557645\nChamartín     0.549401\ndtype: float64\n\n\nOnce you have created the variable, you can make it part of the table:\n\nmadrid_pop[\"ratio_women\"] = ratio_women\nmadrid_pop.head()\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\n\n\ndistrict\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n1\nCentro\n139687\n70770\n68917\n0.493367\n\n\nArganzuela\n1 de enero de 2023\n28079\nMadrid\n2\n2\nArganzuela\n153304\n71754\n81550\n0.531950\n\n\nRetiro\n1 de enero de 2023\n28079\nMadrid\n3\n3\nRetiro\n117918\n53458\n64460\n0.546651\n\n\nSalamanca\n1 de enero de 2023\n28079\nMadrid\n4\n4\nSalamanca\n145702\n64452\n81250\n0.557645\n\n\nChamartín\n1 de enero de 2023\n28079\nMadrid\n5\n5\nChamartín\n144796\n65245\n79551\n0.549401\n\n\n\n\n\n\n\n\n\n\nHere, you explore how to subset parts of a DataFrame if you know exactly which bits you want.\nFor example, if you want to extract the “date”, “neighbourhood”, and “ratio_women” columns for the “Centro” and “Retiro” districts, you can use the Pandas loc indexer. Indexing in Pandas is very flexible and powerful, but can also be a bit confusing for the same reason. The loc documentation and some back-and-forth with AI can be helpful in clearing up points of confusion!\n\nwomen_ratio_2districts = madrid_pop.loc[\n    [\"Centro\", \"Retiro\"],  # the rows to retrieve\n    [\"date\", \"neighbourhood\", \"ratio_women\"],  # the columns to retrieve\n]\nwomen_ratio_2districts\n\n\n\n\n\n\n\n\ndate\nneighbourhood\nratio_women\n\n\ndistrict\n\n\n\n\n\n\n\nCentro\n1 de enero de 2023\nCentro\n0.493367\n\n\nCentro\n1 de enero de 2023\nPalacio\n0.508234\n\n\nCentro\n1 de enero de 2023\nEmbajadores\n0.478530\n\n\nCentro\n1 de enero de 2023\nCortes\n0.505362\n\n\nCentro\n1 de enero de 2023\nJusticia\n0.491685\n\n\n...\n...\n...\n...\n\n\nRetiro\n1 de enero de 2018\nAdelfas\n0.540638\n\n\nRetiro\n1 de enero de 2018\nEstrella\n0.535416\n\n\nRetiro\n1 de enero de 2018\nIbiza\n0.562966\n\n\nRetiro\n1 de enero de 2018\nLosJerónimos\n0.531449\n\n\nRetiro\n1 de enero de 2018\nNiñoJesús\n0.545746\n\n\n\n\n84 rows × 3 columns\n\n\n\nYou can see how you can create a list with the names (index IDs) along each of the two dimensions of a DataFrame (rows and columns), and loc will return a subset of the original table only with the elements queried for.\nAn alternative to list-based queries is what is called “range-based” queries. These work on the indices of the table, but instead of requiring the ID of each item you want to retrieve, they operate by requiring only two IDs: the first and last element in a range of items. Range queries are expressed with a colon (:). However, to perform this operation Index IDs need to be unique. Since this is not our case we will first create a new index composed the year and the code_neighbourhood as a new index to our DataFrame.\n\n# Reset the index to move 'district' back to a regular column\nmadrid_pop.reset_index(inplace=True)\n# Extract the last 4 digits from the 'date' column and create a new 'year' column\nmadrid_pop[\"year\"] = madrid_pop[\"date\"].str[-4:]\n# Create a new column with the combination of 'year' and 'code_neighbourhood'\nmadrid_pop[\"new_index\"] = madrid_pop[\"year\"] + \"_\" + madrid_pop[\"code_neighbourhood\"]\n# Set this as the new index\nmadrid_pop.set_index(\"new_index\", inplace=True)\nmadrid_pop.head(3)\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023_1\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n1\nCentro\n139687\n70770\n68917\n0.493367\n2023\n\n\n2023_2\nArganzuela\n1 de enero de 2023\n28079\nMadrid\n2\n2\nArganzuela\n153304\n71754\n81550\n0.531950\n2023\n\n\n2023_3\nRetiro\n1 de enero de 2023\n28079\nMadrid\n3\n3\nRetiro\n117918\n53458\n64460\n0.546651\n2023\n\n\n\n\n\n\n\nLook at the order of the row and column indexes. This is important becuase “range-based” queries assume you understand the order and arrangement.\n\n# return all content between rows \"2019_1\":\"2018_1\" and columns \"num_people\":\"num_people_women\"\nrange_query = madrid_pop.loc[\"2019_1\":\"2018_1\", \"num_people\":\"num_people_women\"]\n\nrange_query\n\n\n\n\n\n\n\n\nnum_people\nnum_people_men\nnum_people_women\n\n\nnew_index\n\n\n\n\n\n\n\n2019_1\n134881\n67829\n67052\n\n\n2019_2\n153830\n71631\n82199\n\n\n2019_3\n119379\n54098\n65281\n\n\n2019_4\n146148\n64395\n81753\n\n\n2019_5\n145865\n65565\n80300\n\n\n...\n...\n...\n...\n\n\n2019_212\n1851\n952\n899\n\n\n2019_213\n7565\n3648\n3917\n\n\n2019_214\n12388\n5916\n6472\n\n\n2019_215\n7642\n3746\n3896\n\n\n2018_1\n132352\n66320\n66032\n\n\n\n\n153 rows × 3 columns\n\n\n\nThe range query picks up all the elements between the specified IDs. Note that for this to work, the first ID in the range needs to be placed before the second one in the table’s index.\nOnce you know about list and range-based queries, you can combine them!\n\n\n\nHowever, sometimes, you do not know exactly which observations you want, but you do know what conditions they need to satisfy (e.g. areas with more than 2,000 inhabitants). For these cases, DataFrames support selection based on conditions. Let us see a few examples. Suppose you want to select…\n… neighbourhoods wich over time have had less than 50% of women\n\nfewer_women = madrid_pop[madrid_pop[\"ratio_women\"] &lt; 0.5]\nfewer_women\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023_1\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n1\nCentro\n139687\n70770\n68917\n0.493367\n2023\n\n\n2023_12\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n12\nEmbajadores\n46204\n24094\n22110\n0.478530\n2023\n\n\n2023_14\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n14\nJusticia\n18219\n9261\n8958\n0.491685\n2023\n\n\n2023_16\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n16\nSol\n8164\n4239\n3925\n0.480769\n2023\n\n\n2023_81\nFuencarral-El Pardo\n1 de enero de 2023\n28079\nMadrid\n8\n81\nElPardo\n3421\n1716\n1705\n0.498392\n2023\n\n\n2023_106\nLatina\n1 de enero de 2023\n28079\nMadrid\n10\n106\nCuatroVientos\n6122\n3068\n3054\n0.498857\n2023\n\n\n2023_194\nVicálvaro\n1 de enero de 2023\n28079\nMadrid\n19\n194\nElCañaveral\n13054\n6652\n6402\n0.490424\n2023\n\n\n2023_212\nBarajas\n1 de enero de 2023\n28079\nMadrid\n21\n212\nAeropuerto\n1902\n965\n937\n0.492639\n2023\n\n\n2022_1\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n1\nCentro\n139682\n70986\n68696\n0.491803\n2022\n\n\n2022_12\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n12\nEmbajadores\n46444\n24271\n22173\n0.477414\n2022\n\n\n2022_14\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n14\nJusticia\n18015\n9221\n8794\n0.488149\n2022\n\n\n2022_16\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n16\nSol\n8117\n4232\n3885\n0.478625\n2022\n\n\n2022_106\nLatina\n1 de enero de 2022\n28079\nMadrid\n10\n106\nCuatroVientos\n5966\n2996\n2970\n0.497821\n2022\n\n\n2022_194\nVicálvaro\n1 de enero de 2022\n28079\nMadrid\n19\n194\nElCañaveral\n8944\n4525\n4419\n0.494074\n2022\n\n\n2022_212\nBarajas\n1 de enero de 2022\n28079\nMadrid\n21\n212\nAeropuerto\n1895\n967\n928\n0.489710\n2022\n\n\n2021_1\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n1\nCentro\n141236\n71881\n69355\n0.491058\n2021\n\n\n2021_12\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n12\nEmbajadores\n47238\n24767\n22471\n0.475698\n2021\n\n\n2021_14\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n14\nJusticia\n18208\n9291\n8917\n0.489730\n2021\n\n\n2021_16\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n16\nSol\n7993\n4120\n3873\n0.484549\n2021\n\n\n2021_81\nFuencarral-El Pardo\n1 de enero de 2021\n28079\nMadrid\n8\n81\nElPardo\n3443\n1723\n1720\n0.499564\n2021\n\n\n2021_194\nVicálvaro\n1 de enero de 2021\n28079\nMadrid\n19\n194\nElCañaveral\n4430\n2254\n2176\n0.491196\n2021\n\n\n2021_212\nBarajas\n1 de enero de 2021\n28079\nMadrid\n21\n212\nAeropuerto\n1918\n988\n930\n0.484880\n2021\n\n\n2020_1\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n1\nCentro\n140473\n71127\n69346\n0.493661\n2020\n\n\n2020_12\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n12\nEmbajadores\n47048\n24497\n22551\n0.479319\n2020\n\n\n2020_14\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n14\nJusticia\n18021\n9161\n8860\n0.491649\n2020\n\n\n2020_16\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n16\nSol\n7622\n3895\n3727\n0.488979\n2020\n\n\n2020_106\nLatina\n1 de enero de 2020\n28079\nMadrid\n10\n106\nCuatroVientos\n5881\n2958\n2923\n0.497024\n2020\n\n\n2020_194\nVicálvaro\n1 de enero de 2020\n28079\nMadrid\n19\n194\nElCañaveral\n2398\n1230\n1168\n0.487073\n2020\n\n\n2020_212\nBarajas\n1 de enero de 2020\n28079\nMadrid\n21\n212\nAeropuerto\n1900\n975\n925\n0.486842\n2020\n\n\n2019_1\nCentro\n1 de enero de 2019\n28079\nMadrid\n1\n1\nCentro\n134881\n67829\n67052\n0.497120\n2019\n\n\n2019_12\nCentro\n1 de enero de 2019\n28079\nMadrid\n1\n12\nEmbajadores\n45259\n23390\n21869\n0.483197\n2019\n\n\n2019_14\nCentro\n1 de enero de 2019\n28079\nMadrid\n1\n14\nJusticia\n17153\n8675\n8478\n0.494258\n2019\n\n\n2019_16\nCentro\n1 de enero de 2019\n28079\nMadrid\n1\n16\nSol\n7337\n3749\n3588\n0.489028\n2019\n\n\n2019_106\nLatina\n1 de enero de 2019\n28079\nMadrid\n10\n106\nCuatroVientos\n5748\n2909\n2839\n0.493911\n2019\n\n\n2019_194\nVicálvaro\n1 de enero de 2019\n28079\nMadrid\n19\n194\nElCañaveral\n1530\n785\n745\n0.486928\n2019\n\n\n2019_212\nBarajas\n1 de enero de 2019\n28079\nMadrid\n21\n212\nAeropuerto\n1851\n952\n899\n0.485683\n2019\n\n\n2018_1\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n1\nCentro\n132352\n66320\n66032\n0.498912\n2018\n\n\n2018_12\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n12\nEmbajadores\n44630\n23031\n21599\n0.483957\n2018\n\n\n2018_14\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n14\nJusticia\n16578\n8334\n8244\n0.497286\n2018\n\n\n2018_16\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n16\nSol\n7201\n3672\n3529\n0.490071\n2018\n\n\n2018_106\nLatina\n1 de enero de 2018\n28079\nMadrid\n10\n106\nCuatroVientos\n5662\n2870\n2792\n0.493112\n2018\n\n\n2018_194\nVicálvaro\n1 de enero de 2018\n28079\nMadrid\n19\n194\nElCañaveral\n945\n490\n455\n0.481481\n2018\n\n\n2018_212\nBarajas\n1 de enero de 2018\n28079\nMadrid\n21\n212\nAeropuerto\n1794\n922\n872\n0.486065\n2018\n\n\n\n\n\n\n\n… most populated area across all years:\n\nlargest_hood_num = madrid_pop[\"num_people\"].max()\nlargest_hood = madrid_pop[madrid_pop[\"num_people\"] == largest_hood_num]\nlargest_hood\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023_11\nCarabanchel\n1 de enero de 2023\n28079\nMadrid\n11\n11\nCarabanchel\n262339\n122632\n139707\n0.532544\n2023\n\n\n\n\n\n\n\nIf you are interested, more detail about query is available in the pandas documentation. This is another way of slicing Dataframes, but for now we will stay with the loc function.\n\n\n\nNow, all of these queries can be combined with each other for further flexibility. For example, imagine you want to know the areas that have more than 100K inhabitants and have over 50% of women.\n\nwomen_power = madrid_pop.loc[\n    # the & symbol will combine True conditions for the left result set\n    # with the True conditions from the right result set\n    # there is also an \"or\" operator which uses the \"Pipe\" symbol \"|\" instead\n    (madrid_pop[\"num_people_women\"] &gt; 100000)\n    & (madrid_pop[\"ratio_women\"] &gt; 0.5)\n]\nwomen_power\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023_8\nFuencarral-El Pardo\n1 de enero de 2023\n28079\nMadrid\n8\n8\nFuencarral-El Pardo\n248443\n116944\n131499\n0.529292\n2023\n\n\n2023_10\nLatina\n1 de enero de 2023\n28079\nMadrid\n10\n10\nLatina\n241672\n112093\n129579\n0.536177\n2023\n\n\n2023_11\nCarabanchel\n1 de enero de 2023\n28079\nMadrid\n11\n11\nCarabanchel\n262339\n122632\n139707\n0.532544\n2023\n\n\n2023_13\nPuente de Vallecas\n1 de enero de 2023\n28079\nMadrid\n13\n13\nPuente de Vallecas\n241603\n114542\n127061\n0.525908\n2023\n\n\n2023_15\nCiudad Lineal\n1 de enero de 2023\n28079\nMadrid\n15\n15\nCiudad Lineal\n220345\n100759\n119586\n0.542722\n2023\n\n\n2023_16\nHortaleza\n1 de enero de 2023\n28079\nMadrid\n16\n16\nHortaleza\n198391\n94100\n104291\n0.525684\n2023\n\n\n2022_8\nFuencarral-El Pardo\n1 de enero de 2022\n28079\nMadrid\n8\n8\nFuencarral-El Pardo\n246281\n115955\n130326\n0.529176\n2022\n\n\n2022_10\nLatina\n1 de enero de 2022\n28079\nMadrid\n10\n10\nLatina\n237048\n109928\n127120\n0.536263\n2022\n\n\n2022_11\nCarabanchel\n1 de enero de 2022\n28079\nMadrid\n11\n11\nCarabanchel\n255514\n119381\n136133\n0.532781\n2022\n\n\n2022_13\nPuente de Vallecas\n1 de enero de 2022\n28079\nMadrid\n13\n13\nPuente de Vallecas\n235638\n111748\n123890\n0.525764\n2022\n\n\n2022_15\nCiudad Lineal\n1 de enero de 2022\n28079\nMadrid\n15\n15\nCiudad Lineal\n213905\n97357\n116548\n0.544859\n2022\n\n\n2022_16\nHortaleza\n1 de enero de 2022\n28079\nMadrid\n16\n16\nHortaleza\n195017\n92532\n102485\n0.525518\n2022\n\n\n2021_8\nFuencarral-El Pardo\n1 de enero de 2021\n28079\nMadrid\n8\n8\nFuencarral-ElPardo\n247692\n116520\n131172\n0.529577\n2021\n\n\n2021_10\nLatina\n1 de enero de 2021\n28079\nMadrid\n10\n10\nLatina\n240155\n111209\n128946\n0.536928\n2021\n\n\n2021_11\nCarabanchel\n1 de enero de 2021\n28079\nMadrid\n11\n11\nCarabanchel\n258633\n120600\n138033\n0.533702\n2021\n\n\n2021_13\nPuente de Vallecas\n1 de enero de 2021\n28079\nMadrid\n13\n13\nPuentedeVallecas\n239057\n113355\n125702\n0.525824\n2021\n\n\n2021_15\nCiudad Lineal\n1 de enero de 2021\n28079\nMadrid\n15\n15\nCiudadLineal\n216818\n98514\n118304\n0.545637\n2021\n\n\n2021_16\nHortaleza\n1 de enero de 2021\n28079\nMadrid\n16\n16\nHortaleza\n193228\n91585\n101643\n0.526026\n2021\n\n\n2020_8\nFuencarral-El Pardo\n1 de enero de 2020\n28079\nMadrid\n8\n8\nFuencarral-ElPardo\n249973\n117640\n132333\n0.529389\n2020\n\n\n2020_10\nLatina\n1 de enero de 2020\n28079\nMadrid\n10\n10\nLatina\n242139\n112282\n129857\n0.536291\n2020\n\n\n2020_11\nCarabanchel\n1 de enero de 2020\n28079\nMadrid\n11\n11\nCarabanchel\n260196\n121317\n138879\n0.533748\n2020\n\n\n2020_13\nPuente de Vallecas\n1 de enero de 2020\n28079\nMadrid\n13\n13\nPuentedeVallecas\n240867\n114235\n126632\n0.525734\n2020\n\n\n2020_15\nCiudad Lineal\n1 de enero de 2020\n28079\nMadrid\n15\n15\nCiudadLineal\n219867\n99966\n119901\n0.545334\n2020\n\n\n2020_16\nHortaleza\n1 de enero de 2020\n28079\nMadrid\n16\n16\nHortaleza\n193264\n91659\n101605\n0.525732\n2020\n\n\n2019_8\nFuencarral-El Pardo\n1 de enero de 2019\n28079\nMadrid\n8\n8\nFuencarral-ElPardo\n246021\n115797\n130224\n0.529321\n2019\n\n\n2019_10\nLatina\n1 de enero de 2019\n28079\nMadrid\n10\n10\nLatina\n238154\n110401\n127753\n0.536430\n2019\n\n\n2019_11\nCarabanchel\n1 de enero de 2019\n28079\nMadrid\n11\n11\nCarabanchel\n253040\n117802\n135238\n0.534453\n2019\n\n\n2019_13\nPuente de Vallecas\n1 de enero de 2019\n28079\nMadrid\n13\n13\nPuentedeVallecas\n234770\n111183\n123587\n0.526417\n2019\n\n\n2019_15\nCiudad Lineal\n1 de enero de 2019\n28079\nMadrid\n15\n15\nCiudadLineal\n216270\n98370\n117900\n0.545152\n2019\n\n\n2018_8\nFuencarral-El Pardo\n1 de enero de 2018\n28079\nMadrid\n8\n8\nFuencarral-ElPardo\n242928\n114433\n128495\n0.528943\n2018\n\n\n2018_10\nLatina\n1 de enero de 2018\n28079\nMadrid\n10\n10\nLatina\n235785\n109392\n126393\n0.536052\n2018\n\n\n2018_11\nCarabanchel\n1 de enero de 2018\n28079\nMadrid\n11\n11\nCarabanchel\n248220\n115525\n132695\n0.534586\n2018\n\n\n2018_13\nPuente de Vallecas\n1 de enero de 2018\n28079\nMadrid\n13\n13\nPuentedeVallecas\n230488\n109044\n121444\n0.526899\n2018\n\n\n2018_15\nCiudad Lineal\n1 de enero de 2018\n28079\nMadrid\n15\n15\nCiudadLineal\n214463\n97301\n117162\n0.546304\n2018\n\n\n\n\n\n\n\n\n\n\nAmong the many operations DataFrame objects support, one of the most useful ones is to sort a table based on a given column. For example, imagine you want to sort the table by the ratio of women:\n\nmadrid_sorted = madrid_pop.sort_values(\"ratio_women\", ascending=False)\nmadrid_sorted\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2018_158\nCiudad Lineal\n1 de enero de 2018\n28079\nMadrid\n15\n158\nAtalaya\n1575\n656\n919\n0.583492\n2018\n\n\n2019_158\nCiudad Lineal\n1 de enero de 2019\n28079\nMadrid\n15\n158\nAtalaya\n1568\n654\n914\n0.582908\n2019\n\n\n2023_158\nCiudad Lineal\n1 de enero de 2023\n28079\nMadrid\n15\n158\nAtalaya\n1622\n691\n931\n0.573983\n2023\n\n\n2020_158\nCiudad Lineal\n1 de enero de 2020\n28079\nMadrid\n15\n158\nAtalaya\n1555\n667\n888\n0.571061\n2020\n\n\n2020_45\nSalamanca\n1 de enero de 2020\n28079\nMadrid\n4\n45\nLista\n21211\n9111\n12100\n0.570459\n2020\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2020_12\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n12\nEmbajadores\n47048\n24497\n22551\n0.479319\n2020\n\n\n2022_16\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n16\nSol\n8117\n4232\n3885\n0.478625\n2022\n\n\n2023_12\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n12\nEmbajadores\n46204\n24094\n22110\n0.478530\n2023\n\n\n2022_12\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n12\nEmbajadores\n46444\n24271\n22173\n0.477414\n2022\n\n\n2021_12\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n12\nEmbajadores\n47238\n24767\n22471\n0.475698\n2021\n\n\n\n\n912 rows × 12 columns\n\n\n\nGiven the rates differ, it may be better to sort by neighbourhood and then by year.\n\nsort_ls = [\"code_neighbourhood\", \"year\"]\nmadrid_sorted = madrid_pop.sort_values(sort_ls, ascending=True)\nmadrid_sorted\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2018_1\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n1\nCentro\n132352\n66320\n66032\n0.498912\n2018\n\n\n2019_1\nCentro\n1 de enero de 2019\n28079\nMadrid\n1\n1\nCentro\n134881\n67829\n67052\n0.497120\n2019\n\n\n2020_1\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n1\nCentro\n140473\n71127\n69346\n0.493661\n2020\n\n\n2021_1\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n1\nCentro\n141236\n71881\n69355\n0.491058\n2021\n\n\n2022_1\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n1\nCentro\n139682\n70986\n68696\n0.491803\n2022\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2019_97\nMoncloa-Aravaca\n1 de enero de 2019\n28079\nMadrid\n9\n97\nAravaca\n26823\n12619\n14204\n0.529546\n2019\n\n\n2020_97\nMoncloa-Aravaca\n1 de enero de 2020\n28079\nMadrid\n9\n97\nAravaca\n27503\n12899\n14604\n0.530997\n2020\n\n\n2021_97\nMoncloa-Aravaca\n1 de enero de 2021\n28079\nMadrid\n9\n97\nAravaca\n27568\n12896\n14672\n0.532211\n2021\n\n\n2022_97\nMoncloa-Aravaca\n1 de enero de 2022\n28079\nMadrid\n9\n97\nAravaca\n27323\n12779\n14544\n0.532299\n2022\n\n\n2023_97\nMoncloa-Aravaca\n1 de enero de 2023\n28079\nMadrid\n9\n97\nAravaca\n27445\n12836\n14609\n0.532301\n2023\n\n\n\n\n912 rows × 12 columns\n\n\n\nThis allows you to do so-called hierarchical sorting: sort first based on one column then based on another.",
    "crumbs": [
      "Basic Coding",
      "Pandas & Data"
    ]
  },
  {
    "objectID": "chapter_2/pandas.html#visual-exploration",
    "href": "chapter_2/pandas.html#visual-exploration",
    "title": "Pandas & Data",
    "section": "",
    "text": "The next step to continue exploring a dataset is to get a feel for what it looks like, visually. We have already learnt how to uncover and inspect specific parts of the data to check for particular cases we might be interested in. Now, we will see how to plot the data to get a sense of the overall distribution of values. For that, we can use the plotting capabilities of pandas.\n\n\nOne of the most common graphical devices to display the distribution of values in a variable is a histogram. Values are assigned into groups of equal intervals, and the groups are plotted as bars rising as high as the number of values into the group.\nA histogram is easily created with the following command. In this case, let’s have a look at the shape of the overall numbers of people:\n\nmadrid_pop[\"num_people\"].plot.hist(bins=15)\n\n\n\n\n\n\n\n\nHowever, the default pandas plots can be a bit dull. A better option is to use another package, called seaborn.\nseaborn is, by convention, imported as sns. Seaborn is a humorous reference to Samuel Normal Seaborn, a fictional character The West Wing show.\nThe same plot using seaborn has more agreeable default styles and more customisability.\n\nimport seaborn as sns\n\n# Set the style\nsns.set_style(\"darkgrid\")\nsns.histplot(madrid_pop[\"num_people\"], kde=True, bins=15)\n\n\n\n\n\n\n\n\nNote we are using sns instead of pd, as the function belongs to seaborn instead of pandas.\nWe can quickly see most of the areas have seen somewhere between 0 and 50K people; and very few have more than 200K. However, remember that in this case we are visualizing all years together, which could lead to misinterpretations.\n\n\n\nAnother very common way of visually displaying a variable is with a line or a bar chart. For example, if you want to generate a line plot of the (sorted) total population per year:\n\ntotal_people_per_year = madrid_pop.groupby(\"year\")[\"num_people\"].sum()\ntotal_people_per_year.plot()\n\n\n\n\n\n\n\n\nWhat is evident is the impact of COVID on the total population. But understanding that the data is reported on the 1st of January of each year is crucial to understand why you see the offset on the dates.\nFor a bar plot all you need to do is to change from plot to plot.bar:\n\ntotal_people_per_year.plot.bar()\n\n\n\n\n\n\n\n\nLet’s try to plot the ratio_women per neighbourhood, to see if we spot anything in particular.\n\nsns.lineplot(\n    x=\"year\",\n    y=\"ratio_women\",\n    hue=\"neighbourhood\",\n    data=madrid_pop.sort_values(\"year\", ascending=True),\n    legend=False,\n)\n\n\n\n\n\n\n\n\nWe can see some outliers, but the reality is that the data is hard to read so we probably would need some further analysis and visual considerations to efficiently communicate any possible trends.",
    "crumbs": [
      "Basic Coding",
      "Pandas & Data"
    ]
  },
  {
    "objectID": "chapter_2/pandas.html#tidy-data",
    "href": "chapter_2/pandas.html#tidy-data",
    "title": "Pandas & Data",
    "section": "",
    "text": "Clean vs. Tidy\nThis section is a bit more advanced and hence considered optional. Feel free to skip it and return later when you feel more confident.\n\nOnce you can read your data in, explore specific cases, and have a first visual approach to the entire set, the next step can be preparing it for more sophisticated analysis. Maybe you are thinking of modelling it through regression, or on creating subgroups in the dataset with particular characteristics, or maybe you simply need to present summary measures that relate to a slightly different arrangement of the data than you have been presented with.\nFor all these cases, you first need what statistician, and general R wizard, Hadley Wickham calls “tidy data”. The general idea to “tidy” your data is to convert them from whatever structure they were handed to you into one that allows convenient and standardized manipulation, and that supports directly inputting the data into what he calls “tidy” analysis tools. But, at a more practical level, what is exactly “tidy data”? In Wickham’s own words:\n\nTidy data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types.\n\nHe then goes on to list the three fundamental characteristics of “tidy data”:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\nIf you are further interested in the concept of “tidy data”, we recommend the original paper (open access) and the associated public repository.",
    "crumbs": [
      "Basic Coding",
      "Pandas & Data"
    ]
  },
  {
    "objectID": "chapter_2/pandas.html#grouping-transforming-aggregating",
    "href": "chapter_2/pandas.html#grouping-transforming-aggregating",
    "title": "Pandas & Data",
    "section": "",
    "text": "One of the advantage of tidy datasets is they allow advanced transformations in a more direct way. One of the most common ones is what is called “group-by” operations. These originated in the world of databases, and allow you to group observations from a data table by labels, index, or category, and to then apply operations on the data on a group by group basis.\nFor example, given our DataFrame, we might want to compute the total sum of the population by each district. This task can be split into two different steps:\n\nGroup the table in each of the different districts.\nCompute the sum of num_people for each of them.\n\nTo do this in pandas, meet one of its workhorses and also one of the reasons why the library has become so popular: the groupby operator.\n\nmad_grouped = madrid_pop.groupby(\"year\")\nmad_grouped\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x1442cc850&gt;\n\n\nThe object mad_grouped still hasn’t computed anything. It is only a convenient way of specifying the grouping. But this allows us then to perform a multitude of operations on it. For our example, the sum is calculated as follows:\n\nmad_grouped.sum(numeric_only=True)\n\n\n\n\n\n\n\n\ncode_municipality\nnum_people\nnum_people_men\nnum_people_women\nratio_women\n\n\nyear\n\n\n\n\n\n\n\n\n\n2018\n4268008\n6443648\n3000680\n3442968\n81.100391\n\n\n2019\n4268008\n6532252\n3042356\n3489896\n81.108315\n\n\n2020\n4268008\n6669460\n3109464\n3559996\n81.047788\n\n\n2021\n4268008\n6624620\n3090314\n3534306\n81.013452\n\n\n2022\n4268008\n6573324\n3069648\n3503676\n80.939052\n\n\n2023\n4268008\n6679862\n3119732\n3560130\n80.942860\n\n\n\n\n\n\n\nSimilarly, we can also obtain a summary of each group:\n\nmad_grouped.describe()\n\n\n\n\n\n\n\n\ncode_municipality\nnum_people\n...\nnum_people_women\nratio_women\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ncount\nmean\n...\n75%\nmax\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nyear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2018\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n42392.421053\n...\n21771.75\n132695.0\n152.0\n0.533555\n0.018390\n0.481481\n0.521591\n0.534767\n0.545880\n0.583492\n\n\n2019\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n42975.342105\n...\n21987.00\n135238.0\n152.0\n0.533607\n0.018198\n0.483197\n0.522356\n0.534355\n0.545650\n0.582908\n\n\n2020\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n43878.026316\n...\n22623.75\n138879.0\n152.0\n0.533209\n0.017944\n0.479319\n0.522586\n0.533905\n0.545599\n0.571061\n\n\n2021\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n43583.026316\n...\n22530.75\n138033.0\n152.0\n0.532983\n0.017984\n0.475698\n0.522263\n0.533724\n0.545682\n0.568987\n\n\n2022\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n43245.552632\n...\n22296.50\n136133.0\n152.0\n0.532494\n0.017716\n0.477414\n0.521691\n0.533305\n0.544920\n0.567140\n\n\n2023\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n43946.460526\n...\n22742.75\n139707.0\n152.0\n0.532519\n0.017486\n0.478530\n0.522637\n0.533363\n0.544488\n0.573983\n\n\n\n\n6 rows × 40 columns\n\n\n\nWe will not get into it today as it goes beyond the basics this chapter covers, but keep in mind that groupby allows us to not only call generic functions (like sum or describe), but also custom functions. This opens the door for virtually any kind of transformation and aggregation possible.\nAdditional reading\n\nA good introduction to data manipulation in Python is Wes McKinney’s Python for Data Analysis.\nTo further explore some of the visualization capabilities, the Python library seaborn is an excellent choice. Its online tutorial is a fantastic place to start.\nA good extension is Hadley Wickham’s “Tidy data” paper, which presents a very popular way of organising tabular data for efficient manipulation.",
    "crumbs": [
      "Basic Coding",
      "Pandas & Data"
    ]
  },
  {
    "objectID": "chapter_0/anaconda.html",
    "href": "chapter_0/anaconda.html",
    "title": "Python Notebooks",
    "section": "",
    "text": "Welcome to the first hands-on section of the course where you will familiarise yourself with Jupyter Lab for running Python ‘Notebooks’.\nThis guide assumes the prior installation of Anaconda as detailed previously.\n\n\nCreate a new “environment” the first time you use Anaconda. Environments ensure that packages remain self-contained and make it easier to synchronise packages without unnecessary contamination from unrelated projects.\n\nOpen Anaconda Navigator.\nClick on Environments\nClick to create a new environment called nfi.\nSelect the newly created nfi environment from the list of environments.\nIn the right pane, change the dropdown view mode to “Not Installed”.\nSearch for jupyterlab, then select it and click to install.\nSelect the Home button to return to the main view.\n\n\n\n\nTo launch Juptyer Lab:\n\nOpen Anaconda Navigator.\nUsing the top dropdown menus, select “All applications” and select the newly created “nfi” environment.\nClick to launch “Jupyter Lab” from the list of applications. This will launch a new Jupyter Lab instance in a web browser.\nUse the “Python 3” kernel if prompted.\n\n\n\n\n\n\n\nTip\n\n\n\nDon’t confuse the “Jupyer Lab” with the “Jupyter Notebook” apps. The former is a newer replacement for the latter.\n\n\n\n\n\nYou can now create a new Notebook or, otherwise, open an existing Python Notebook. Notebooks are composed of cells.\n\n\n\nJupyter Notebook cell\n\n\nCells can either consist of code or Markdown formatted text. A typical notebook consists of a series of cells; some would include text with explanations or background information, while others would contain the executable code. The code in the cells has to be “executed” by ppressing the play button. Once executed, the code will generate outputs showing the results of the computation.\nYou can try creating your own cell content and experimenting with running the cells. Remember that the cells are only aware of previously executed cells. We can start with simple math that Python can do natively. Run the following code cell in your notebook. After inserting the text, you can either click the “play” button or press the Shift + Enter keys.\n\n# this will display the number \"2\" once the cell is executed\n1 + 1\n\n2\n\n\nYou can also try using variables and printing:\n\n# this stores the calculation in \"my_variable\"\nmy_variable = 2 * 10 / 3\n# this prints the variable using \"f-string\" interpolation\nprint(f'The calculation result is: {my_variable}')\n# let's round the number to two decimal places\nprint(f'The rounded result is: {round(my_variable, 2)}')\n\nThe calculation result is: 6.666666666666667\nThe rounded result is: 6.67\n\n\n\n\n\nShut down Jupyter Lab from the menubar in the web interface using File &gt; Shut Down.\n\n\n\nShut down Jupyter Lab\n\n\n\n\n\nIf you are unable to install an environment using the instructions above, you can follow the course using Google Colab. You will need to install any required packages to your Colab environment. Reach out in class if you need help getting set up.\n\n\n\nIf you already have experience with Python, IDEs, and the command line, feel free to setup your development environment accordingly. The method for doing so will depend on your operating system and your preference for IDEs. We recommend VSCode as an IDE.",
    "crumbs": [
      "Setup",
      "Python Notebooks"
    ]
  },
  {
    "objectID": "chapter_0/anaconda.html#overview",
    "href": "chapter_0/anaconda.html#overview",
    "title": "Python Notebooks",
    "section": "",
    "text": "Welcome to the first hands-on section of the course where you will familiarise yourself with Jupyter Lab for running Python ‘Notebooks’.\nThis guide assumes the prior installation of Anaconda as detailed previously.\n\n\nCreate a new “environment” the first time you use Anaconda. Environments ensure that packages remain self-contained and make it easier to synchronise packages without unnecessary contamination from unrelated projects.\n\nOpen Anaconda Navigator.\nClick on Environments\nClick to create a new environment called nfi.\nSelect the newly created nfi environment from the list of environments.\nIn the right pane, change the dropdown view mode to “Not Installed”.\nSearch for jupyterlab, then select it and click to install.\nSelect the Home button to return to the main view.\n\n\n\n\nTo launch Juptyer Lab:\n\nOpen Anaconda Navigator.\nUsing the top dropdown menus, select “All applications” and select the newly created “nfi” environment.\nClick to launch “Jupyter Lab” from the list of applications. This will launch a new Jupyter Lab instance in a web browser.\nUse the “Python 3” kernel if prompted.\n\n\n\n\n\n\n\nTip\n\n\n\nDon’t confuse the “Jupyer Lab” with the “Jupyter Notebook” apps. The former is a newer replacement for the latter.\n\n\n\n\n\nYou can now create a new Notebook or, otherwise, open an existing Python Notebook. Notebooks are composed of cells.\n\n\n\nJupyter Notebook cell\n\n\nCells can either consist of code or Markdown formatted text. A typical notebook consists of a series of cells; some would include text with explanations or background information, while others would contain the executable code. The code in the cells has to be “executed” by ppressing the play button. Once executed, the code will generate outputs showing the results of the computation.\nYou can try creating your own cell content and experimenting with running the cells. Remember that the cells are only aware of previously executed cells. We can start with simple math that Python can do natively. Run the following code cell in your notebook. After inserting the text, you can either click the “play” button or press the Shift + Enter keys.\n\n# this will display the number \"2\" once the cell is executed\n1 + 1\n\n2\n\n\nYou can also try using variables and printing:\n\n# this stores the calculation in \"my_variable\"\nmy_variable = 2 * 10 / 3\n# this prints the variable using \"f-string\" interpolation\nprint(f'The calculation result is: {my_variable}')\n# let's round the number to two decimal places\nprint(f'The rounded result is: {round(my_variable, 2)}')\n\nThe calculation result is: 6.666666666666667\nThe rounded result is: 6.67\n\n\n\n\n\nShut down Jupyter Lab from the menubar in the web interface using File &gt; Shut Down.\n\n\n\nShut down Jupyter Lab\n\n\n\n\n\nIf you are unable to install an environment using the instructions above, you can follow the course using Google Colab. You will need to install any required packages to your Colab environment. Reach out in class if you need help getting set up.\n\n\n\nIf you already have experience with Python, IDEs, and the command line, feel free to setup your development environment accordingly. The method for doing so will depend on your operating system and your preference for IDEs. We recommend VSCode as an IDE.",
    "crumbs": [
      "Setup",
      "Python Notebooks"
    ]
  }
]