[
  {
    "objectID": "chapter_1/cloud.html",
    "href": "chapter_1/cloud.html",
    "title": "Cloud GIS",
    "section": "",
    "text": "Cloud GIS\nThis will be an in-class hands-on session providing an introduction to the use of CARTO for publishing and styling interactive maps.",
    "crumbs": [
      "GIS Tools",
      "Cloud GIS"
    ]
  },
  {
    "objectID": "chapter_1/qgis.html",
    "href": "chapter_1/qgis.html",
    "title": "Intro to QGIS",
    "section": "",
    "text": "We’ll first explore the QGIS user interface, taking some time to explore:\n\nMain Interface: The map area, toolbars, layers panel, status bar, dock panels.\nToolbars: These are collections of icons and tools that provide quick access to commonly used features and functions in QGIS. These can be customised based on your preferences. Some of the common toolbars provide functionality such as zooming, panning, selecting features, and editing features.\nMap View: This is the central area where your maps and layers are displayed. We’ll explore how to navigate the map view, including zooming, panning, and identifying features.\nBrowser Panel: For browsing and loading data from your file system, databases, and web services.\nLayers Panel: Here, you can see a list of all the layers loaded in your project. Layers can be organised, styled, and otherwise configured.\nAttribute Table: This panel lets you view and edit the attribute data associated with your vector layers.\nStatus Bar: Located at the bottom, the status bar shows information about the map and your project, such as the current map scale, the coordinate reference system, and messaging about ongoing processes.\nPlugins: QGIS can be extended with plugins. We’ll briefly touch on how to find, install, and use plugins to enhance the functionality of QGIS.\n\n\n\n\nInstall the Quick Map Services (QMS) Plugin:\n\nFrom the Plugins menu, select Manage and Install Plugins\nSearch for QMS\nInstall QuickMapServices\nInstall and Close the window.\n\nThis plugin will add a new menu entry under the Web menu.\n\nFrom the Web menu, select QuickMapServices, then OSM and OSM Standard.\nThis will add a map to your map view.\n\n\n\n\n\n\n\n\nDownload the municipal neighbourhoods as a SHP file: Barrios municipales de Madrid\nDownload the population by district and neighbourhood dataset as a CSV file: Población por distrito y barrio a 1 de enero\n\n\n\n\nGeospatial data is represented using a coordinate reference system (CRS). QGIS will ordinarily detect this information automatically. However, it can display information in any selected CRS, so you can open multiple files in different CRSs and these will display correctly. This is because QGIS will reproject each respective file into the map’s currently selected CRS system.\nIn the lower right corner of the status bar at the bottom of the screen:\n\nSelect the CRS button to open the project’s CRS configuration.\nSearch for CRS code 3857, then select it, Apply, and OK.\n\n\nHint: When using OSM base maps, the base maps will render more performantly in the Web Mercator projection (3857). This is because QGIS then doesn’t need to dynamically reproject the map tiles out of their native 3857 projection.\n\n\n\n\nLoad your first dataset by dragging and dropping the shapefile (the downloaded file ending with .shp) into your map view.\n\nYou can toggle layer visibility and order using the Layers Panel.\nUse the Select Features button to select a feature and see it highlighted.\nClick the Pan Map to Selection button to see the map zoom to the currently selected feature\nClick the Zoom Full button to return to a view of the full dataset.\nUse the Identify Features button to see attribute information for a particular feature.\nClick the Toggle Editing button -&gt; this will let you delete existing features or add new features - use with caution! Don’t make any changes for now, just click the Toggle Editing button again to exit edit mode.\n\n\n\n\nWe don’t want to edit the original dataset, so let’s make a copy and work with that instead:\n\nRight click the layer in the Layer Panel and select export -&gt; Save Features as\nSelect the GeoPackage format.\nSelect a file location on disk and give it a distinctive name such as my_gpkg.\nSelect 3857 for the CRS projection\nCheck that Add saved file to map at the bottom of the window is selected\nSelect OK\nThe file will be saved and will automatically load into your map.\nRemove the original SHP file from your Layers Panel since it is no longer needed. This can be done with the Remove Layer button.\n\n\n\n\nAttributes in GIS layers provide additional information about the spatial features in the data. They include text, numbers, dates, or images. Attributes help add context to the geographic elements, enabling better analysis and visualization. They can be used to filter, classify, and symbolize features based on their characteristics. Attributes play a crucial role in data analysis, allowing us to gain insights, make comparisons, and create meaningful visual representations.\nUsing the Municipal Boundaries my_gpkg dataset:\n\nOpen the attribute table for the neighbourhood boundaries by right clicking on the layer in the Layers Panel and selecting Attribute Table. From the Attribute table you can try the same steps you tried for the map view (selecting, zooming to, editing).\nToggle Editing Mode by pressing the Toggle Editing Mode button.\nClick on the New Field button in the toolbar\nSet the Name as area and the type as Decimal number, then OK.\nCheck that no rows are selected (otherwise the calculation only applies to the selected rows).\nOpen the Field Calculator by clicking its icon (abacus symbol) in the toolbar.\nSelect Update existing field and select the area column.\nIn the Expression pane, enter $area, then press the blue forward button below to preview the results of the calculation.\nPress OK and check that your area column has been updated.\nPress the Save edits button to save your calculation.\nTurn off editing mode.\n\nWe’ll now add another column, this time we’ll use another column as input and we will transform the data from a text to an integer.\n\nTurn editing mode on\nOpen the Field Calculator\nThis time select Create a new field and call it cod_barrio\nCheck that it is an Integer type\nIn the expression, enter to_int(COD_BAR) -&gt; this will take the text from the COD_BAR column and will cast this to an integer. In the process, the data will drop the leading zero.\nApply the change, save to disk, and toggle editing to off.\n\n\n\n\n\nTo load the CSV file we’ll go to the Layer menu bar, Add Layer, then Add Delimited Text Layer.\nFor File name, navigate to your CSV file\nThis file uses semi-colon delimeters, so select Custom delimiters and check that Semicolon is selected\nCheck that the option for First record has field names is selected. This will automatically use the first line of the CSV to set the attribute names.\nSelect No geometry. This will load the dataset without visual geometric information. Note that if you have longitude and latitude information in CSV columns (or Eastings and Northings for projected CRS), then you can specify these columns and QGIS will generate the point geometries accordingly.\nSelect Add then Close.\nNote that this layer looks different in the Layers Panel because it doesn’t have associated geometry. However, you can still view the Attribute Table.\nRight click on the layer and select export -&gt; Save Features as. Then save the layer as a new CSV file called my_csv. As before, select the option to add the layer to the map, save, then remove the original input CSV from the Layers Panel.\n\n\n\n\n\nOpen the attribute table for the my_csv layer by right clicking on the data table in the Layers Panel and selecting Attribute Table.\nOrder the data by clicking the fecha column header, order in descending order by clicking again.\nClick on the first entry then hold down shift and click on the last entry with a fecha value of 1 de enero de 2023\nClick the Invert selection button.\nToggle editing.\nClick the Delete selected features button.\nSave and turn off editing.\n\n\n\n\n\nOpen the Attribute Panels for both the my_gpkg and the my_csv layers.\nWe want to “Join” information from the CSV dataset to the neighbourhood boundaries using the matching identifiers in the respective cod_barrio / cod_barrio columns.\nClose the Attribute Panels.\nDouble click on the my_gpkg layer to open the properties panel.\nClick Joins in the left sidebar.\nClick the plus button to create a new join.\nSelect the my_csv table for the Join layer -&gt; the layer from which attributes will be joined to my_gpkg.\nSelect the cod_barrio column for the Join field\nSelect the cod_barrio column for the Target field\nClick OK and close the layer properties.\nReopen the Attribute Panel for the my_gpkg dataset, you’ll see extra columns which have now joined the data from the my_csv table into their corresponding boundaries based on the matching identifiers.\nLet’s export this to a new file to my_joined_gpkg, which will now include the joined data.\nRemove my_csv and my_gpkg from the Layers Panel.\n\n\n\n\n\nOpen the my_joined_gpkg layer properties by double clicking on the layer name in the Layers Panel.\nSelect Labels in the left pane, select Single Labels, then select the BARRIO_MT column, then apply. This will add the Barrio names to the map. You can format labels to your heart’s content.\nSelect Symbology in the left pane. In class, we will walk through a variety of options for map styling. There are almost limitless options so feel free to be creative.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nConnection details will be provided during class. Note that all connections to the database are IP protected.\n\n\n\n\n\n\n\n\nThere is a lot that can be done with QGIS and we have just skimmed the surface!\nThe QGIS Tutorials and Tips is a great resource for QGIS.\nFor lab-work, please do the following tutorials from QGIS Tutorials and Tips in your own time to build some general familiarity with QGIS:\n\nMaking a Map\nBasic Vector Styling\nDigitizing Map Data\nWorking with Projections\nCreating Heatmaps",
    "crumbs": [
      "GIS Tools",
      "Intro to QGIS"
    ]
  },
  {
    "objectID": "chapter_1/qgis.html#qgis-ui",
    "href": "chapter_1/qgis.html#qgis-ui",
    "title": "Intro to QGIS",
    "section": "",
    "text": "We’ll first explore the QGIS user interface, taking some time to explore:\n\nMain Interface: The map area, toolbars, layers panel, status bar, dock panels.\nToolbars: These are collections of icons and tools that provide quick access to commonly used features and functions in QGIS. These can be customised based on your preferences. Some of the common toolbars provide functionality such as zooming, panning, selecting features, and editing features.\nMap View: This is the central area where your maps and layers are displayed. We’ll explore how to navigate the map view, including zooming, panning, and identifying features.\nBrowser Panel: For browsing and loading data from your file system, databases, and web services.\nLayers Panel: Here, you can see a list of all the layers loaded in your project. Layers can be organised, styled, and otherwise configured.\nAttribute Table: This panel lets you view and edit the attribute data associated with your vector layers.\nStatus Bar: Located at the bottom, the status bar shows information about the map and your project, such as the current map scale, the coordinate reference system, and messaging about ongoing processes.\nPlugins: QGIS can be extended with plugins. We’ll briefly touch on how to find, install, and use plugins to enhance the functionality of QGIS.",
    "crumbs": [
      "GIS Tools",
      "Intro to QGIS"
    ]
  },
  {
    "objectID": "chapter_1/qgis.html#plugins",
    "href": "chapter_1/qgis.html#plugins",
    "title": "Intro to QGIS",
    "section": "",
    "text": "Install the Quick Map Services (QMS) Plugin:\n\nFrom the Plugins menu, select Manage and Install Plugins\nSearch for QMS\nInstall QuickMapServices\nInstall and Close the window.\n\nThis plugin will add a new menu entry under the Web menu.\n\nFrom the Web menu, select QuickMapServices, then OSM and OSM Standard.\nThis will add a map to your map view.",
    "crumbs": [
      "GIS Tools",
      "Intro to QGIS"
    ]
  },
  {
    "objectID": "chapter_1/qgis.html#datasets",
    "href": "chapter_1/qgis.html#datasets",
    "title": "Intro to QGIS",
    "section": "",
    "text": "Download the municipal neighbourhoods as a SHP file: Barrios municipales de Madrid\nDownload the population by district and neighbourhood dataset as a CSV file: Población por distrito y barrio a 1 de enero",
    "crumbs": [
      "GIS Tools",
      "Intro to QGIS"
    ]
  },
  {
    "objectID": "chapter_1/qgis.html#coordinate-reference-systems",
    "href": "chapter_1/qgis.html#coordinate-reference-systems",
    "title": "Intro to QGIS",
    "section": "",
    "text": "Geospatial data is represented using a coordinate reference system (CRS). QGIS will ordinarily detect this information automatically. However, it can display information in any selected CRS, so you can open multiple files in different CRSs and these will display correctly. This is because QGIS will reproject each respective file into the map’s currently selected CRS system.\nIn the lower right corner of the status bar at the bottom of the screen:\n\nSelect the CRS button to open the project’s CRS configuration.\nSearch for CRS code 3857, then select it, Apply, and OK.\n\n\nHint: When using OSM base maps, the base maps will render more performantly in the Web Mercator projection (3857). This is because QGIS then doesn’t need to dynamically reproject the map tiles out of their native 3857 projection.",
    "crumbs": [
      "GIS Tools",
      "Intro to QGIS"
    ]
  },
  {
    "objectID": "chapter_1/qgis.html#loading-data",
    "href": "chapter_1/qgis.html#loading-data",
    "title": "Intro to QGIS",
    "section": "",
    "text": "Load your first dataset by dragging and dropping the shapefile (the downloaded file ending with .shp) into your map view.\n\nYou can toggle layer visibility and order using the Layers Panel.\nUse the Select Features button to select a feature and see it highlighted.\nClick the Pan Map to Selection button to see the map zoom to the currently selected feature\nClick the Zoom Full button to return to a view of the full dataset.\nUse the Identify Features button to see attribute information for a particular feature.\nClick the Toggle Editing button -&gt; this will let you delete existing features or add new features - use with caution! Don’t make any changes for now, just click the Toggle Editing button again to exit edit mode.",
    "crumbs": [
      "GIS Tools",
      "Intro to QGIS"
    ]
  },
  {
    "objectID": "chapter_1/qgis.html#saving-and-exporting-data",
    "href": "chapter_1/qgis.html#saving-and-exporting-data",
    "title": "Intro to QGIS",
    "section": "",
    "text": "We don’t want to edit the original dataset, so let’s make a copy and work with that instead:\n\nRight click the layer in the Layer Panel and select export -&gt; Save Features as\nSelect the GeoPackage format.\nSelect a file location on disk and give it a distinctive name such as my_gpkg.\nSelect 3857 for the CRS projection\nCheck that Add saved file to map at the bottom of the window is selected\nSelect OK\nThe file will be saved and will automatically load into your map.\nRemove the original SHP file from your Layers Panel since it is no longer needed. This can be done with the Remove Layer button.",
    "crumbs": [
      "GIS Tools",
      "Intro to QGIS"
    ]
  },
  {
    "objectID": "chapter_1/qgis.html#attributes",
    "href": "chapter_1/qgis.html#attributes",
    "title": "Intro to QGIS",
    "section": "",
    "text": "Attributes in GIS layers provide additional information about the spatial features in the data. They include text, numbers, dates, or images. Attributes help add context to the geographic elements, enabling better analysis and visualization. They can be used to filter, classify, and symbolize features based on their characteristics. Attributes play a crucial role in data analysis, allowing us to gain insights, make comparisons, and create meaningful visual representations.\nUsing the Municipal Boundaries my_gpkg dataset:\n\nOpen the attribute table for the neighbourhood boundaries by right clicking on the layer in the Layers Panel and selecting Attribute Table. From the Attribute table you can try the same steps you tried for the map view (selecting, zooming to, editing).\nToggle Editing Mode by pressing the Toggle Editing Mode button.\nClick on the New Field button in the toolbar\nSet the Name as area and the type as Decimal number, then OK.\nCheck that no rows are selected (otherwise the calculation only applies to the selected rows).\nOpen the Field Calculator by clicking its icon (abacus symbol) in the toolbar.\nSelect Update existing field and select the area column.\nIn the Expression pane, enter $area, then press the blue forward button below to preview the results of the calculation.\nPress OK and check that your area column has been updated.\nPress the Save edits button to save your calculation.\nTurn off editing mode.\n\nWe’ll now add another column, this time we’ll use another column as input and we will transform the data from a text to an integer.\n\nTurn editing mode on\nOpen the Field Calculator\nThis time select Create a new field and call it cod_barrio\nCheck that it is an Integer type\nIn the expression, enter to_int(COD_BAR) -&gt; this will take the text from the COD_BAR column and will cast this to an integer. In the process, the data will drop the leading zero.\nApply the change, save to disk, and toggle editing to off.",
    "crumbs": [
      "GIS Tools",
      "Intro to QGIS"
    ]
  },
  {
    "objectID": "chapter_1/qgis.html#loading-csv-files",
    "href": "chapter_1/qgis.html#loading-csv-files",
    "title": "Intro to QGIS",
    "section": "",
    "text": "To load the CSV file we’ll go to the Layer menu bar, Add Layer, then Add Delimited Text Layer.\nFor File name, navigate to your CSV file\nThis file uses semi-colon delimeters, so select Custom delimiters and check that Semicolon is selected\nCheck that the option for First record has field names is selected. This will automatically use the first line of the CSV to set the attribute names.\nSelect No geometry. This will load the dataset without visual geometric information. Note that if you have longitude and latitude information in CSV columns (or Eastings and Northings for projected CRS), then you can specify these columns and QGIS will generate the point geometries accordingly.\nSelect Add then Close.\nNote that this layer looks different in the Layers Panel because it doesn’t have associated geometry. However, you can still view the Attribute Table.\nRight click on the layer and select export -&gt; Save Features as. Then save the layer as a new CSV file called my_csv. As before, select the option to add the layer to the map, save, then remove the original input CSV from the Layers Panel.",
    "crumbs": [
      "GIS Tools",
      "Intro to QGIS"
    ]
  },
  {
    "objectID": "chapter_1/qgis.html#selecting-and-deleting-features",
    "href": "chapter_1/qgis.html#selecting-and-deleting-features",
    "title": "Intro to QGIS",
    "section": "",
    "text": "Open the attribute table for the my_csv layer by right clicking on the data table in the Layers Panel and selecting Attribute Table.\nOrder the data by clicking the fecha column header, order in descending order by clicking again.\nClick on the first entry then hold down shift and click on the last entry with a fecha value of 1 de enero de 2023\nClick the Invert selection button.\nToggle editing.\nClick the Delete selected features button.\nSave and turn off editing.",
    "crumbs": [
      "GIS Tools",
      "Intro to QGIS"
    ]
  },
  {
    "objectID": "chapter_1/qgis.html#joins",
    "href": "chapter_1/qgis.html#joins",
    "title": "Intro to QGIS",
    "section": "",
    "text": "Open the Attribute Panels for both the my_gpkg and the my_csv layers.\nWe want to “Join” information from the CSV dataset to the neighbourhood boundaries using the matching identifiers in the respective cod_barrio / cod_barrio columns.\nClose the Attribute Panels.\nDouble click on the my_gpkg layer to open the properties panel.\nClick Joins in the left sidebar.\nClick the plus button to create a new join.\nSelect the my_csv table for the Join layer -&gt; the layer from which attributes will be joined to my_gpkg.\nSelect the cod_barrio column for the Join field\nSelect the cod_barrio column for the Target field\nClick OK and close the layer properties.\nReopen the Attribute Panel for the my_gpkg dataset, you’ll see extra columns which have now joined the data from the my_csv table into their corresponding boundaries based on the matching identifiers.\nLet’s export this to a new file to my_joined_gpkg, which will now include the joined data.\nRemove my_csv and my_gpkg from the Layers Panel.",
    "crumbs": [
      "GIS Tools",
      "Intro to QGIS"
    ]
  },
  {
    "objectID": "chapter_1/qgis.html#visualisation",
    "href": "chapter_1/qgis.html#visualisation",
    "title": "Intro to QGIS",
    "section": "",
    "text": "Open the my_joined_gpkg layer properties by double clicking on the layer name in the Layers Panel.\nSelect Labels in the left pane, select Single Labels, then select the BARRIO_MT column, then apply. This will add the Barrio names to the map. You can format labels to your heart’s content.\nSelect Symbology in the left pane. In class, we will walk through a variety of options for map styling. There are almost limitless options so feel free to be creative.",
    "crumbs": [
      "GIS Tools",
      "Intro to QGIS"
    ]
  },
  {
    "objectID": "chapter_1/qgis.html#connecting-to-db",
    "href": "chapter_1/qgis.html#connecting-to-db",
    "title": "Intro to QGIS",
    "section": "",
    "text": "Tip\n\n\n\nConnection details will be provided during class. Note that all connections to the database are IP protected.",
    "crumbs": [
      "GIS Tools",
      "Intro to QGIS"
    ]
  },
  {
    "objectID": "chapter_1/qgis.html#resources",
    "href": "chapter_1/qgis.html#resources",
    "title": "Intro to QGIS",
    "section": "",
    "text": "There is a lot that can be done with QGIS and we have just skimmed the surface!\nThe QGIS Tutorials and Tips is a great resource for QGIS.\nFor lab-work, please do the following tutorials from QGIS Tutorials and Tips in your own time to build some general familiarity with QGIS:\n\nMaking a Map\nBasic Vector Styling\nDigitizing Map Data\nWorking with Projections\nCreating Heatmaps",
    "crumbs": [
      "GIS Tools",
      "Intro to QGIS"
    ]
  },
  {
    "objectID": "chapter_0/installation.html",
    "href": "chapter_0/installation.html",
    "title": "Software & Accounts",
    "section": "",
    "text": "Instructions for how to setup your Github account and how to sign-up for the Github Student Developer Pack will be given in class. Please don’t try to activate the Student Developer Pack ahead of class. This is so that it can be configured with the correct accounts and procedure.\n\n\n\nSetup for the CARTO account will be detailed in class after activating the Github Student Developer Pack.\n\n\n\nDownload and install QGIS form the official download links.\n\n\n\nDownload and install Anaconda from the official download links.\n\n\n\nDownload and install TablePlus.. This is a tool for interacting with databases.",
    "crumbs": [
      "Setup",
      "Software & Accounts"
    ]
  },
  {
    "objectID": "chapter_0/installation.html#github-student-developer-pack",
    "href": "chapter_0/installation.html#github-student-developer-pack",
    "title": "Software & Accounts",
    "section": "",
    "text": "Instructions for how to setup your Github account and how to sign-up for the Github Student Developer Pack will be given in class. Please don’t try to activate the Student Developer Pack ahead of class. This is so that it can be configured with the correct accounts and procedure.",
    "crumbs": [
      "Setup",
      "Software & Accounts"
    ]
  },
  {
    "objectID": "chapter_0/installation.html#carto-account",
    "href": "chapter_0/installation.html#carto-account",
    "title": "Software & Accounts",
    "section": "",
    "text": "Setup for the CARTO account will be detailed in class after activating the Github Student Developer Pack.",
    "crumbs": [
      "Setup",
      "Software & Accounts"
    ]
  },
  {
    "objectID": "chapter_0/installation.html#qgis",
    "href": "chapter_0/installation.html#qgis",
    "title": "Software & Accounts",
    "section": "",
    "text": "Download and install QGIS form the official download links.",
    "crumbs": [
      "Setup",
      "Software & Accounts"
    ]
  },
  {
    "objectID": "chapter_0/installation.html#anaconda",
    "href": "chapter_0/installation.html#anaconda",
    "title": "Software & Accounts",
    "section": "",
    "text": "Download and install Anaconda from the official download links.",
    "crumbs": [
      "Setup",
      "Software & Accounts"
    ]
  },
  {
    "objectID": "chapter_0/installation.html#table-plus",
    "href": "chapter_0/installation.html#table-plus",
    "title": "Software & Accounts",
    "section": "",
    "text": "Download and install TablePlus.. This is a tool for interacting with databases.",
    "crumbs": [
      "Setup",
      "Software & Accounts"
    ]
  },
  {
    "objectID": "chapter_0/index.html",
    "href": "chapter_0/index.html",
    "title": "Overview",
    "section": "",
    "text": "Overview\nThis section details installation and steps for setting up your Github account, signing up for the Github Student Developer Pack, software downloads, and getting Python notebooks running on your system.",
    "crumbs": [
      "Setup",
      "Overview"
    ]
  },
  {
    "objectID": "chapter_2/python.html",
    "href": "chapter_2/python.html",
    "title": "Python Notebooks",
    "section": "",
    "text": "Welcome to Python and Notebooks, a potent combination for learning Python and structuring code workflows.\n\nThis notebook can be downloaded from here.\n\n\n\nThis Notebook is adapted from ‘Let the snake in’ from the A taste of Python section of the Geo-Python course 2022 by D. Whipp, H. Tenkanen, V. Heikinheimo, H. Aagesen, and C. Fink from the Department of Geosciences and Geography, University of Helsinki, licensed under CC-BY-SA 4.0.\n\n\n\nPython can be used as a simple calculator. Remember, you can press Shift + Enter to execute the code in the cells below. Try it out by typing some simple math into new cells and see what you get.\n\n42 * 12\n\n504\n\n\n\n12 / 3\n\n4.0\n\n\nIf you want to edit and re-run some code, change the cell re-execute.\n\n\n\nYou can use Python for more advanced math by using a function. Functions are pieces of code that perform a single action, such as printing information to the screen (e.g., the print() function). Functions exist for a huge number of operations in Python.\nLet’s try out a few simple examples using functions to find the sin or square root of a value. You can type sin(3) or sqrt(4) into the cells below to test this out.\n\nsin(3)\n\nNameError: name 'sin' is not defined\n\n\n\nsqrt(4)\n\nNameError: name 'sqrt' is not defined\n\n\nWell, that didn’t work. Python can calculate square roots or do basic trigonometry, but we need one more step.\n\n\nThe table below shows the list of basic arithmetic operations that can be done by default in Python.\n\n\n\nOperation\nSymbol\nExample syntax\nReturned value\n\n\n\n\nAddition\n+\n2 + 2\n4\n\n\nSubtraction\n-\n4 - 2\n2\n\n\nMultiplication\n*\n2 * 3\n6\n\n\nDivision\n/\n4 / 2\n2\n\n\nExponentiation\n**\n2 ** 3\n8\n\n\n\nFor anything more advanced, we need to load a module or a package. For math operations, this module is called math and can be loaded by typing import math.\n\nimport math\n\nNow that we have access to functions in the math module, we can use it by typing the module name, a period (dot), and the name of the function we want to use. For example, math.sin(3). Try this with the sine and square root examples from above.\n\nmath.sin(3)\n\n0.1411200080598672\n\n\n\nmath.sqrt(4)\n\n2.0\n\n\nLet’s summarise what you’ve just done with modules:\n\nA module is a group of code items, such as functions, related to one another. Individual modules are often in a group called a package.\nModules can be loaded using import. Functions that are part of the module modulename can then be used by typing modulename.functionname(). For example, sin() is a function that is part of the math module and is used by typing math.sin() with some number between the parentheses.\nWithin a Jupyter Notebook, the variables you defined earlier in the notebook will be available for use in the following cells as long as you have executed the cells.\nModules may also contain constants such as math.pi (notice no parentheses at the end). Type this in the cell below to see the constant’s math.pi value.\n\n\nmath.pi\n\n3.141592653589793\n\n\n\n\n\nFunctions can also be combined. The print() function returns values within the parentheses as text on the screen. Below, try printing the value of the square root of four.\n\nprint(math.sqrt(4))\n\n2.0\n\n\nYou can also combine text with other calculated values using the print() function. For example, print('Two plus two is', 2+2) would generate the text reading 'Two plus two is 4'. Combine the print() function with the math.sqrt() function in the cell below to produce text that reads 'The square root of 4 is 2.0'.\n\nprint(\"The square root of 4 is\", math.sqrt(4))\n\nThe square root of 4 is 2.0\n\n\n\n\n\n\nA variable can store values calculated in expressions and used for other calculations. Assigning value to variables is straightforward. To assign a value, you type variable_name = value, where variable_name is the name of the variable you wish to define. In the cell below, define a variable called temp_celsius, assign it a value of 10.0, and then print that variable value using the print() function. Note that you should do this on two separate lines.\n\ntemp_celsius = 10.0\nprint(temp_celsius)\n\n10.0\n\n\nAs we did above, you can combine text and even use some math when printing out variable values. The idea is similar to adding 2+2 or calculating the square root of four from the previous section. In the cell below, print out the value of temp_celsius in degrees Fahrenheit by multiplying temp_celsius by 9/5 and adding 32. This should be done within the print() function to produce output that reads 'Temperature in Fahrenheit: 50.0'.\n\nprint(\"Temperature in Fahrenheit:\", 9 / 5 * temp_celsius + 32)\n\nTemperature in Fahrenheit: 50.0\n\n\n\n\nValues stored in variables can also be updated. Let’s redefine the value of temp_celsius to be equal to 15.0 and print its value in the cells below.\n\ntemp_celsius = 15.0\n\n\nprint(\"temperature in Celsius is now:\", temp_celsius)\n\ntemperature in Celsius is now: 15.0\n\n\nWarning\nIf you try to run some code that accesses a variable that has not yet been defined, you will get a NameError message. Try printing out the value of the variable temp_fahrenheit using the print() function in the cell below.\n\nprint(\"Temperature in Celsius:\", 5 / 9 * (temp_fahrenheit - 32))\n\nTemperature in Celsius: 15.0\n\n\nNote\nOne of the interesting things here is that if we define the undefined variable in a cell lower down in the notebook and execute that cell, we can return to the earlier cell, and the code should now work. That was a bit of a complicated sentence, so let’s test this all out. First, let’s define a variable called temp_fahrenheit in the cell below and assign it to be equal to 9/5 * temp_celsius + 32, the conversion factor from temperatures in Celsius to Fahrenheit. Then, return to the cell above this text and run that cell again. See how the error message has gone away? temp_fahrenheit has now been defined, and thus, the cell above no longer generates a NameError when the code is executed.\nAlso, the number beside the cell, for example, In [2], tells you the order in which the Python cells have been executed. This way, you can see a history of the order in which you have run the cells.\n\ntemp_fahrenheit = 9 / 5 * temp_celsius + 32\n\nTo check their current values, print out the values of temp_celsius and temp_fahrenheit in the cell below.\n\nprint(\"temperature in Celsius:\", temp_celsius, \"and in Fahrenheit:\", temp_fahrenheit)\n\ntemperature in Celsius: 15.0 and in Fahrenheit: 59.0\n\n\n\n\n\n\nA data type determines the characteristics of data in a program. There are four basic data types in Python, as shown in the table below.\n\n\n\nData type name\nData type\nExample\n\n\n\n\nint\nWhole integer values\n4\n\n\nfloat\nDecimal values\n3.1415\n\n\nstr\nCharacter strings\n'Hot'\n\n\nbool\nTrue/false values\nTrue\n\n\n\nThe data type can be found using the type() function. As you will see, the data types are essential because some are incompatible.\nLet’s define a variable weather_forecast and assign it the value 'Hot'. After this, we can check its data type using the type() function.\n\nweather_forecast = \"Hot\"\ntype(weather_forecast)\n\nstr\n\n\nLet’s also check the type of temp_fahrenheit.\n\ntype(temp_fahrenheit)\n\nfloat\n\n\nWhat happens if you try to combine temp_fahrenheit and weather_forecast in a single math equation such as temp_fahrenheit = temp_fahrenheit + 5.0 * weather_forecast?\n\ntemp_fahrenheit = temp_fahrenheit + 5.0 * weather_forecast\n\nTypeError: can't multiply sequence by non-int of type 'float'\n\n\nIn this case, we get at TypeError because we are trying to execute a math operation with data types that are not compatible. There is no way in Python to multiply numbers with a character string.",
    "crumbs": [
      "Basic Coding",
      "Python Notebooks"
    ]
  },
  {
    "objectID": "chapter_2/python.html#acknowledgements",
    "href": "chapter_2/python.html#acknowledgements",
    "title": "Python Notebooks",
    "section": "",
    "text": "This Notebook is adapted from ‘Let the snake in’ from the A taste of Python section of the Geo-Python course 2022 by D. Whipp, H. Tenkanen, V. Heikinheimo, H. Aagesen, and C. Fink from the Department of Geosciences and Geography, University of Helsinki, licensed under CC-BY-SA 4.0.",
    "crumbs": [
      "Basic Coding",
      "Python Notebooks"
    ]
  },
  {
    "objectID": "chapter_2/python.html#simple-python-math",
    "href": "chapter_2/python.html#simple-python-math",
    "title": "Python Notebooks",
    "section": "",
    "text": "Python can be used as a simple calculator. Remember, you can press Shift + Enter to execute the code in the cells below. Try it out by typing some simple math into new cells and see what you get.\n\n42 * 12\n\n504\n\n\n\n12 / 3\n\n4.0\n\n\nIf you want to edit and re-run some code, change the cell re-execute.",
    "crumbs": [
      "Basic Coding",
      "Python Notebooks"
    ]
  },
  {
    "objectID": "chapter_2/python.html#functions",
    "href": "chapter_2/python.html#functions",
    "title": "Python Notebooks",
    "section": "",
    "text": "You can use Python for more advanced math by using a function. Functions are pieces of code that perform a single action, such as printing information to the screen (e.g., the print() function). Functions exist for a huge number of operations in Python.\nLet’s try out a few simple examples using functions to find the sin or square root of a value. You can type sin(3) or sqrt(4) into the cells below to test this out.\n\nsin(3)\n\nNameError: name 'sin' is not defined\n\n\n\nsqrt(4)\n\nNameError: name 'sqrt' is not defined\n\n\nWell, that didn’t work. Python can calculate square roots or do basic trigonometry, but we need one more step.\n\n\nThe table below shows the list of basic arithmetic operations that can be done by default in Python.\n\n\n\nOperation\nSymbol\nExample syntax\nReturned value\n\n\n\n\nAddition\n+\n2 + 2\n4\n\n\nSubtraction\n-\n4 - 2\n2\n\n\nMultiplication\n*\n2 * 3\n6\n\n\nDivision\n/\n4 / 2\n2\n\n\nExponentiation\n**\n2 ** 3\n8\n\n\n\nFor anything more advanced, we need to load a module or a package. For math operations, this module is called math and can be loaded by typing import math.\n\nimport math\n\nNow that we have access to functions in the math module, we can use it by typing the module name, a period (dot), and the name of the function we want to use. For example, math.sin(3). Try this with the sine and square root examples from above.\n\nmath.sin(3)\n\n0.1411200080598672\n\n\n\nmath.sqrt(4)\n\n2.0\n\n\nLet’s summarise what you’ve just done with modules:\n\nA module is a group of code items, such as functions, related to one another. Individual modules are often in a group called a package.\nModules can be loaded using import. Functions that are part of the module modulename can then be used by typing modulename.functionname(). For example, sin() is a function that is part of the math module and is used by typing math.sin() with some number between the parentheses.\nWithin a Jupyter Notebook, the variables you defined earlier in the notebook will be available for use in the following cells as long as you have executed the cells.\nModules may also contain constants such as math.pi (notice no parentheses at the end). Type this in the cell below to see the constant’s math.pi value.\n\n\nmath.pi\n\n3.141592653589793\n\n\n\n\n\nFunctions can also be combined. The print() function returns values within the parentheses as text on the screen. Below, try printing the value of the square root of four.\n\nprint(math.sqrt(4))\n\n2.0\n\n\nYou can also combine text with other calculated values using the print() function. For example, print('Two plus two is', 2+2) would generate the text reading 'Two plus two is 4'. Combine the print() function with the math.sqrt() function in the cell below to produce text that reads 'The square root of 4 is 2.0'.\n\nprint(\"The square root of 4 is\", math.sqrt(4))\n\nThe square root of 4 is 2.0",
    "crumbs": [
      "Basic Coding",
      "Python Notebooks"
    ]
  },
  {
    "objectID": "chapter_2/python.html#variables",
    "href": "chapter_2/python.html#variables",
    "title": "Python Notebooks",
    "section": "",
    "text": "A variable can store values calculated in expressions and used for other calculations. Assigning value to variables is straightforward. To assign a value, you type variable_name = value, where variable_name is the name of the variable you wish to define. In the cell below, define a variable called temp_celsius, assign it a value of 10.0, and then print that variable value using the print() function. Note that you should do this on two separate lines.\n\ntemp_celsius = 10.0\nprint(temp_celsius)\n\n10.0\n\n\nAs we did above, you can combine text and even use some math when printing out variable values. The idea is similar to adding 2+2 or calculating the square root of four from the previous section. In the cell below, print out the value of temp_celsius in degrees Fahrenheit by multiplying temp_celsius by 9/5 and adding 32. This should be done within the print() function to produce output that reads 'Temperature in Fahrenheit: 50.0'.\n\nprint(\"Temperature in Fahrenheit:\", 9 / 5 * temp_celsius + 32)\n\nTemperature in Fahrenheit: 50.0\n\n\n\n\nValues stored in variables can also be updated. Let’s redefine the value of temp_celsius to be equal to 15.0 and print its value in the cells below.\n\ntemp_celsius = 15.0\n\n\nprint(\"temperature in Celsius is now:\", temp_celsius)\n\ntemperature in Celsius is now: 15.0\n\n\nWarning\nIf you try to run some code that accesses a variable that has not yet been defined, you will get a NameError message. Try printing out the value of the variable temp_fahrenheit using the print() function in the cell below.\n\nprint(\"Temperature in Celsius:\", 5 / 9 * (temp_fahrenheit - 32))\n\nTemperature in Celsius: 15.0\n\n\nNote\nOne of the interesting things here is that if we define the undefined variable in a cell lower down in the notebook and execute that cell, we can return to the earlier cell, and the code should now work. That was a bit of a complicated sentence, so let’s test this all out. First, let’s define a variable called temp_fahrenheit in the cell below and assign it to be equal to 9/5 * temp_celsius + 32, the conversion factor from temperatures in Celsius to Fahrenheit. Then, return to the cell above this text and run that cell again. See how the error message has gone away? temp_fahrenheit has now been defined, and thus, the cell above no longer generates a NameError when the code is executed.\nAlso, the number beside the cell, for example, In [2], tells you the order in which the Python cells have been executed. This way, you can see a history of the order in which you have run the cells.\n\ntemp_fahrenheit = 9 / 5 * temp_celsius + 32\n\nTo check their current values, print out the values of temp_celsius and temp_fahrenheit in the cell below.\n\nprint(\"temperature in Celsius:\", temp_celsius, \"and in Fahrenheit:\", temp_fahrenheit)\n\ntemperature in Celsius: 15.0 and in Fahrenheit: 59.0",
    "crumbs": [
      "Basic Coding",
      "Python Notebooks"
    ]
  },
  {
    "objectID": "chapter_2/python.html#data-types",
    "href": "chapter_2/python.html#data-types",
    "title": "Python Notebooks",
    "section": "",
    "text": "A data type determines the characteristics of data in a program. There are four basic data types in Python, as shown in the table below.\n\n\n\nData type name\nData type\nExample\n\n\n\n\nint\nWhole integer values\n4\n\n\nfloat\nDecimal values\n3.1415\n\n\nstr\nCharacter strings\n'Hot'\n\n\nbool\nTrue/false values\nTrue\n\n\n\nThe data type can be found using the type() function. As you will see, the data types are essential because some are incompatible.\nLet’s define a variable weather_forecast and assign it the value 'Hot'. After this, we can check its data type using the type() function.\n\nweather_forecast = \"Hot\"\ntype(weather_forecast)\n\nstr\n\n\nLet’s also check the type of temp_fahrenheit.\n\ntype(temp_fahrenheit)\n\nfloat\n\n\nWhat happens if you try to combine temp_fahrenheit and weather_forecast in a single math equation such as temp_fahrenheit = temp_fahrenheit + 5.0 * weather_forecast?\n\ntemp_fahrenheit = temp_fahrenheit + 5.0 * weather_forecast\n\nTypeError: can't multiply sequence by non-int of type 'float'\n\n\nIn this case, we get at TypeError because we are trying to execute a math operation with data types that are not compatible. There is no way in Python to multiply numbers with a character string.",
    "crumbs": [
      "Basic Coding",
      "Python Notebooks"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NFI Urban Analytics 2024",
    "section": "",
    "text": "Welcome to Part 1 of the Urban Analytics course at NFI. This course is designed to empower scholars with the skills and knowledge to harness the power of urban analytics for analyzing and interpreting urban data.\nThis is a practical course and is best approached with a hands-on and explorative mindset. It consists of three parts:\n\nPart 1 includes information on general setup and provides an introduction to basic concepts. See the sidebar for the Sections.\nPart 2 will be released separately and delves into targeted themes and skills development.\nPart 3 will be framed around pilot cities specific workflows.\n\nWe hope you enjoy the course!\n\n\n\nThis course is will touch on a number of broad topics. High-level guidance will be provided and scholars will then be encouraged to further explore these approaches and develop their skillsets based on methods of interest or relevance to their work:\n\nSpatial Analysis: Exploring Geographic Information Systems, spatial data handling, and mapping urban data.\nPython: Introduction to Python and its usage for geospatial analysis.\nData Handling: Techniques for handling datasets and an introduction to data science methods.\nProject Work: Lab work tasks and workflows.\n\n\n\n\nYou have been added to a Google Spaces for online discussions. This platform is your primary hub for:\n\nSeeking peer assistance.\nDiscussing course materials and urban analytics topics.\nCollaborating on projects and assignments.\nAnnouncements and updates from the course team.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#ua-part-1",
    "href": "index.html#ua-part-1",
    "title": "NFI Urban Analytics 2024",
    "section": "",
    "text": "Welcome to Part 1 of the Urban Analytics course at NFI. This course is designed to empower scholars with the skills and knowledge to harness the power of urban analytics for analyzing and interpreting urban data.\nThis is a practical course and is best approached with a hands-on and explorative mindset. It consists of three parts:\n\nPart 1 includes information on general setup and provides an introduction to basic concepts. See the sidebar for the Sections.\nPart 2 will be released separately and delves into targeted themes and skills development.\nPart 3 will be framed around pilot cities specific workflows.\n\nWe hope you enjoy the course!",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#topics",
    "href": "index.html#topics",
    "title": "NFI Urban Analytics 2024",
    "section": "",
    "text": "This course is will touch on a number of broad topics. High-level guidance will be provided and scholars will then be encouraged to further explore these approaches and develop their skillsets based on methods of interest or relevance to their work:\n\nSpatial Analysis: Exploring Geographic Information Systems, spatial data handling, and mapping urban data.\nPython: Introduction to Python and its usage for geospatial analysis.\nData Handling: Techniques for handling datasets and an introduction to data science methods.\nProject Work: Lab work tasks and workflows.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#communication",
    "href": "index.html#communication",
    "title": "NFI Urban Analytics 2024",
    "section": "",
    "text": "You have been added to a Google Spaces for online discussions. This platform is your primary hub for:\n\nSeeking peer assistance.\nDiscussing course materials and urban analytics topics.\nCollaborating on projects and assignments.\nAnnouncements and updates from the course team.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "chapter_2/pandas.html",
    "href": "chapter_2/pandas.html",
    "title": "Pandas & Data",
    "section": "",
    "text": "Now that you know some of the basics of Python Notebooks, it’s time to start using them for data science (no, that simple math you did the last time doesn’t count as data science!).\nYou are about to enter the Python data ecosystem. This course does not explicitly cover the fundamentals of programming; if you’re stuck, confused, or need further explanation, the answers are more often than not easily accessible through online resources such as your favourite search engine, Stack Overflow, and the amazingly capable explanatory power of emerging AI tools such as ChatGPT. The internet is a friend of every programmer and you’re encouraged to use it effectively from the get-go. There has never been an easier time to learn how to code!\nLet’s dig in!\n\nThis notebook can be downloaded from here.\n\n\n\nThis section is adapted from the Spatial Data Science for Social Geography by Martin Fleischman, which in turn is based on A Course on Geographic Data Science by Dani Arribas-Bel, both licensed under CC-BY-SA 4.0. The content and datasets were reworked to accommodate the Madrid neighbourhood population statistics dataset.\n\n\n\nReal-world datasets are messy. There is no way around it: datasets have “holes” (missing data), the number of formats in which data can be stored can be endless, and the best structure to share data is not always the optimal for analysis. Hence the need to “munge” or “wrangle” data. Much of the time spent in what is called Data Science is related not only to modelling and insight but more often than not has to do with much more basic and less exotic tasks such as obtaining the data, data cleaning, and other preparation which makes analysis possible.\nSurprisingly, very little has been published on patterns, techniques, and best practices for quick and efficient data cleaning, manipulation, and transformation because of how labour-intensive this aspect is. In this session, we will use real-world datasets and learn how to transform and manipulate these, if necessary, prior to analysis. For this, we will introduce some of the bread-and-butter of data analysis and scientific computing in Python. These are fundamental tools that are widely and consistently used on almost any tasks relating to data analysis.\nThis notebook covers the basics and the content that needs to be grasped, and discusses several patterns to clean and structure data properly, including tidying, subsetting, and aggregating. We will conclude with some basic visualisation. An additional extension presents more advanced tricks to manipulate tabular data.\n\n\n\nWe will be exploring demographic characteristics of Madrid. The data has been aggregated to a neighbourhood level by the statistic’s office of Madrid’s City Hall. It contains information per year (from 2018 to 2023) and theme.\nAs with many datasets that will be used during this course, the data was originally found in an online data portal at the following link\nThe main tool we will use for this task is the pandas package. As with any Python module or package, the Pandas package has to be imported into the Notebook prior to usage.\n\nimport pandas as pd\n\nPandas let’s us work with datasets, and stores the information in a DataFrame consisting of rows and columns. Here, we will read an online csv dataset into a Pandas DataFrame. Pandas can open local files directly, but also has the nifty ability to download files directly from the web:\n\n# here we will directly fetch the CSV dataset and save it as a Pandas DataFrame\nmadrid_pop = pd.read_csv(\n    \"https://datos.madrid.es/egob/catalogo/300557-0-poblacion-distrito-barrio.csv\",\n    sep=\";\",\n)\n\n\nDelimiters\nIn this case we are using the Pandas read_csv method because the source file is a csv. By default, csv files are assumed to use commas for data delimitation, but this file uses semi-colons instead. This is why we are passing the optional sep parameter to specify a ; delimeter.\n\n\nFormats\nNote that pandas allows for many more formats to be read and written. A full list of formats supported may be found in the documentation.\n\nIt is also possible to download the file and read it locally. In this case, you would download the file by clicking on this link. Then place it in the same folder as the notebook where you intend to read it from, and run the below cell.\n\n# remember, this cell will only work if you have downloaded the file\n# and, if the filepath is correct!\n# uncomment the below lines to run!\n# madrid_pop = pd.read_csv(\n#     \"poblacion_1_enero.csv\",\n#     sep=\";\",\n# )\n\n\n\n\nNow, we are ready to start playing and interrogating the dataset! What you have at your fingertips is a table summarising, for each of the districts in Madrid, how many people lived there by gender. These tables are called DataFrame objects, and they have a lot of functionality built-in to explore and manipulate the data they contain. Let’s explore a few of those cool tricks!\n\n\nThe first aspect worth spending a bit of time on is the structure of a DataFrame. You can print it by simply typing its name:\n\nmadrid_pop\n\n\n\n\n\n\n\n\nfecha\ncod_municipio\nmunicipio\ncod_distrito\ndistrito\ncod_barrio\nbarrio\nnum_personas\nnum_personas_hombres\nnum_personas_mujeres\n\n\n\n\n0\n1 de enero de 2023\n28079\nMadrid\n1\nCentro\n1\nCentro\n139.687\n70.770\n68.917\n\n\n1\n1 de enero de 2023\n28079\nMadrid\n2\nArganzuela\n2\nArganzuela\n153.304\n71.754\n81.550\n\n\n2\n1 de enero de 2023\n28079\nMadrid\n3\nRetiro\n3\nRetiro\n117.918\n53.458\n64.460\n\n\n3\n1 de enero de 2023\n28079\nMadrid\n4\nSalamanca\n4\nSalamanca\n145.702\n64.452\n81.250\n\n\n4\n1 de enero de 2023\n28079\nMadrid\n5\nChamartín\n5\nChamartín\n144.796\n65.245\n79.551\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n913\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n212\nAeropuerto\n1.794\n922\n872\n\n\n914\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n213\nCascoHistóricodeBarajas\n7.336\n3.550\n3.786\n\n\n915\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n214\nTimón\n11.750\n5.651\n6.099\n\n\n916\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n215\nCorralejos\n7.510\n3.657\n3.853\n\n\n917\n1 de enero de 2018\n28079\nMadrid\nTodos\nTodos\nTodos\nTodos\n3.221.824\n1.500.340\n1.721.484\n\n\n\n\n918 rows × 10 columns\n\n\n\nAs you can expect, the dataset was provided in the original local language, so to make things a little easier, we are going to translate the column names using one list of texts. for this we will reassign the column names corresponding to their English equivalents.\n\nNote that there are multiple ways of arriving at the same output, the below is just one of them\n\n\nen_cols = [\n    \"date\",\n    \"code_municipality\",\n    \"municipality\",\n    \"code_district\",\n    \"district\",\n    \"code_neighbourhood\",\n    \"neighbourhood\",\n    \"num_people\",\n    \"num_people_men\",\n    \"num_people_women\",\n]\nmadrid_pop.columns = en_cols\nmadrid_pop\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ndistrict\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\n\n\n\n\n0\n1 de enero de 2023\n28079\nMadrid\n1\nCentro\n1\nCentro\n139.687\n70.770\n68.917\n\n\n1\n1 de enero de 2023\n28079\nMadrid\n2\nArganzuela\n2\nArganzuela\n153.304\n71.754\n81.550\n\n\n2\n1 de enero de 2023\n28079\nMadrid\n3\nRetiro\n3\nRetiro\n117.918\n53.458\n64.460\n\n\n3\n1 de enero de 2023\n28079\nMadrid\n4\nSalamanca\n4\nSalamanca\n145.702\n64.452\n81.250\n\n\n4\n1 de enero de 2023\n28079\nMadrid\n5\nChamartín\n5\nChamartín\n144.796\n65.245\n79.551\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n913\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n212\nAeropuerto\n1.794\n922\n872\n\n\n914\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n213\nCascoHistóricodeBarajas\n7.336\n3.550\n3.786\n\n\n915\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n214\nTimón\n11.750\n5.651\n6.099\n\n\n916\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n215\nCorralejos\n7.510\n3.657\n3.853\n\n\n917\n1 de enero de 2018\n28079\nMadrid\nTodos\nTodos\nTodos\nTodos\n3.221.824\n1.500.340\n1.721.484\n\n\n\n\n918 rows × 10 columns\n\n\n\nThe default printing is truncated to keep a compact view, but is enough to convey its structure. DataFrame objects have two dimensions: rows and columns. Rows are typically identified with an index column and columns are typically identified with names describing their content. The above example shows how the column names were automatically lifted from the csv file’s column headers.\nWe can set the row index using the set_index() method. Let’s set the index to use the district column.\n\n# Remove leading and trailing spaces from 'distrito'\nmadrid_pop[\"district\"] = madrid_pop[\"district\"].str.strip()\n# set the index based on the district column\nmadrid_pop.set_index(\"district\", inplace=True)\n\nColumns can be assigned as one of various forms of data such as integers as int, decimals as float, text as str, or if pandas is unable to use a standard type, it might use the object type as a catch-all type.\nTo extract a single column from this DataFrame, specify its name in square brackets ([]). Note that the name is a string - a piece of text - which needs to be denoted with single (') or double quotes (\"). Without the quotes, Python will think you are referring to a variable, and will then complain that the variable can’t be found!\nA single column is not a DataFrame but a Series, which also means that a DataFrame is a collection of Series!\n\n# this will fetch and return the num_people_women column as a Series\nmadrid_pop[\"num_people_women\"]\n\ndistrict\nCentro           68.917\nArganzuela       81.550\nRetiro           64.460\nSalamanca        81.250\nChamartín        79.551\n                ...    \nBarajas             872\nBarajas           3.786\nBarajas           6.099\nBarajas           3.853\nTodos         1.721.484\nName: num_people_women, Length: 918, dtype: object\n\n\n\n\n\nYou can specifically print the DataFrames top (or bottom) lines by passing a number to the method head or tail. For example, for the top or bottom three lines:\n\nmadrid_pop.head(3)\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\n\n\ndistrict\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n1\nCentro\n139.687\n70.770\n68.917\n\n\nArganzuela\n1 de enero de 2023\n28079\nMadrid\n2\n2\nArganzuela\n153.304\n71.754\n81.550\n\n\nRetiro\n1 de enero de 2023\n28079\nMadrid\n3\n3\nRetiro\n117.918\n53.458\n64.460\n\n\n\n\n\n\n\n\nmadrid_pop.tail(3)\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\n\n\ndistrict\n\n\n\n\n\n\n\n\n\n\n\n\n\nBarajas\n1 de enero de 2018\n28079\nMadrid\n21\n214\nTimón\n11.750\n5.651\n6.099\n\n\nBarajas\n1 de enero de 2018\n28079\nMadrid\n21\n215\nCorralejos\n7.510\n3.657\n3.853\n\n\nTodos\n1 de enero de 2018\n28079\nMadrid\nTodos\nTodos\nTodos\n3.221.824\n1.500.340\n1.721.484\n\n\n\n\n\n\n\nInspecting datasets is vital to find errors that might skew your analysis. The last row of the dataset contains the sums of other column values ( Spanish: Todos -&gt; English: All ).\nBefore continuing, let’s fix this by removing these columns from the dataset. First, let’s check if we have any more occurrences of Todos. We can use Pandas’ loc indexer to fetch rows where the index column contains the text string Todos.\n\nmadrid_pop.loc[\"Todos\"]\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\n\n\ndistrict\n\n\n\n\n\n\n\n\n\n\n\n\n\nTodos\n1 de enero de 2023\n28079\nMadrid\nTodos\nTodos\nTodos\n3.339.931\n1.559.866\n1.780.065\n\n\nTodos\n1 de enero de 2022\n28079\nMadrid\nTodos\nTodos\nTodos\n3.286.662\n1.534.824\n1.751.838\n\n\nTodos\n1 de enero de 2021\n28079\nMadrid\nTodos\nTodos\nTodos\n3.312.310\n1.545.157\n1.767.153\n\n\nTodos\n1 de enero de 2020\n28079\nMadrid\nTodos\nTodos\nTodos\n3.334.730\n1.554.732\n1.779.998\n\n\nTodos\n1 de enero de 2019\n28079\nMadrid\nTodos\nTodos\nTodos\n3.266.126\n1.521.178\n1.744.948\n\n\nTodos\n1 de enero de 2018\n28079\nMadrid\nTodos\nTodos\nTodos\n3.221.824\n1.500.340\n1.721.484\n\n\n\n\n\n\n\nNow that we know that for every year the data has been aggregated and added back as a new row, we know that removing the last row would not be enough to correct our DataFrame.\nWe will make use of the drop function in combination with the native loc function of DataFrames.\n\nprint(\"Rows before droping values: \", len(madrid_pop))\nmadrid_pop.drop(index=\"Todos\", inplace=True)\nprint(\"Rows after droping values: \", len(madrid_pop))\n\nRows before droping values:  918\nRows after droping values:  912\n\n\nNow, let’s get an overview of the table:\n\nmadrid_pop.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 912 entries, Centro to Barajas\nData columns (total 9 columns):\n #   Column              Non-Null Count  Dtype \n---  ------              --------------  ----- \n 0   date                912 non-null    object\n 1   code_municipality   912 non-null    int64 \n 2   municipality        912 non-null    object\n 3   code_district       912 non-null    object\n 4   code_neighbourhood  912 non-null    object\n 5   neighbourhood       912 non-null    object\n 6   num_people          912 non-null    object\n 7   num_people_men      912 non-null    object\n 8   num_people_women    912 non-null    object\ndtypes: int64(1), object(8)\nmemory usage: 71.2+ KB\n\n\nCan you spot something wrong?\nInteger numbers are sometimes stored as text values, especially if the input dataset contained commas, periods, or null data values. The actual values of population counts would be better stored as full integers so that we can more effectively work with this data as a numeric data type. Let’s take the following steps:\n\nCreate a list containg the column names that have people counts\nLoop through the list, removing periods from the numbers, then casting the values to integer types while allowing for null data values\n\n\n# List of columns to process\ncolumns_to_process = [\"num_people\", \"num_people_men\", \"num_people_women\"]\n\n# Loop through columns\nfor column in columns_to_process:\n    # create a copy of the column\n    dirty_numbers = madrid_pop[column]\n    # remove \".\" from the numbers\n    clean_numbers = dirty_numbers.str.replace(\".\", \"\")\n    # convert to numeric - coerce will convert non numeric values to NaN (Not a Number) or \"null\" types\n    numeric_numbers = pd.to_numeric(clean_numbers, errors=\"coerce\")\n    # overwrite the original column\n    madrid_pop[column] = numeric_numbers\n\nmadrid_pop.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 912 entries, Centro to Barajas\nData columns (total 9 columns):\n #   Column              Non-Null Count  Dtype \n---  ------              --------------  ----- \n 0   date                912 non-null    object\n 1   code_municipality   912 non-null    int64 \n 2   municipality        912 non-null    object\n 3   code_district       912 non-null    object\n 4   code_neighbourhood  912 non-null    object\n 5   neighbourhood       912 non-null    object\n 6   num_people          912 non-null    int64 \n 7   num_people_men      912 non-null    int64 \n 8   num_people_women    912 non-null    int64 \ndtypes: int64(4), object(5)\nmemory usage: 71.2+ KB\n\n\n\n\n\n\nmadrid_pop.describe()\n\n\n\n\n\n\n\n\ncode_municipality\nnum_people\nnum_people_men\nnum_people_women\n\n\n\n\ncount\n912.0\n912.000000\n912.000000\n912.000000\n\n\nmean\n28079.0\n43336.804825\n20210.739035\n23126.065789\n\n\nstd\n0.0\n51564.255101\n24072.657139\n27522.444389\n\n\nmin\n28079.0\n945.000000\n490.000000\n455.000000\n\n\n25%\n28079.0\n17330.500000\n8132.000000\n9267.250000\n\n\n50%\n28079.0\n24700.500000\n11580.000000\n13640.000000\n\n\n75%\n28079.0\n42166.500000\n19700.250000\n22349.750000\n\n\nmax\n28079.0\n262339.000000\n122632.000000\n139707.000000\n\n\n\n\n\n\n\nNote how the output is also a DataFrame object, so you can manipulate it the same way that you would with the original table (e.g. writing it to a file).\nIn this case, the summary might be better presented if the table is “transposed”:\n\nmadrid_pop[columns_to_process].describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nnum_people\n912.0\n43336.804825\n51564.255101\n945.0\n17330.50\n24700.5\n42166.50\n262339.0\n\n\nnum_people_men\n912.0\n20210.739035\n24072.657139\n490.0\n8132.00\n11580.0\n19700.25\n122632.0\n\n\nnum_people_women\n912.0\n23126.065789\n27522.444389\n455.0\n9267.25\n13640.0\n22349.75\n139707.0\n\n\n\n\n\n\n\nEqually, common descriptive statistics are also available. To obtain minimum values for each column, you can use .min().\n\nmadrid_pop.min()\n\ndate                  1 de enero de 2018\ncode_municipality                  28079\nmunicipality                      Madrid\ncode_district                          1\ncode_neighbourhood                     1\nneighbourhood                 Arganzuela\nnum_people                           945\nnum_people_men                       490\nnum_people_women                     455\ndtype: object\n\n\nOr to obtain a minimum for a single column only.\n\nmadrid_pop[\"num_people_women\"].min()\n\n455\n\n\nNote here how you have restricted the calculation of the minimum value to one column only by getting the Series and calling .min() on that.\nSimilarly, you can restrict the calculations to a single district using .loc[] indexer:\n\nmadrid_pop.loc[\"Centro\"].min()\n\ndate                  1 de enero de 2018\ncode_municipality                  28079\nmunicipality                      Madrid\ncode_district                          1\ncode_neighbourhood                     1\nneighbourhood                     Centro\nnum_people                          7201\nnum_people_men                      3672\nnum_people_women                    3529\ndtype: object\n\n\nLet’s see when and where the said minimum occurred.\n\n# we can use comparators to index into the DataFrame where a specific column equals a specific value\nmadrid_pop[madrid_pop[\"num_people_women\"] == 3529]\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\n\n\ndistrict\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n16\nSol\n7201\n3672\n3529\n\n\n\n\n\n\n\n\n\n\nYou can generate new variables by applying operations to existing ones. For example, you can calculate the ratio of women.\n\nratio_women = madrid_pop[\"num_people_women\"] / madrid_pop[\"num_people\"]\nratio_women.head()\n\ndistrict\nCentro        0.493367\nArganzuela    0.531950\nRetiro        0.546651\nSalamanca     0.557645\nChamartín     0.549401\ndtype: float64\n\n\nOnce you have created the variable, you can make it part of the table:\n\nmadrid_pop[\"ratio_women\"] = ratio_women\nmadrid_pop.head()\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\n\n\ndistrict\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n1\nCentro\n139687\n70770\n68917\n0.493367\n\n\nArganzuela\n1 de enero de 2023\n28079\nMadrid\n2\n2\nArganzuela\n153304\n71754\n81550\n0.531950\n\n\nRetiro\n1 de enero de 2023\n28079\nMadrid\n3\n3\nRetiro\n117918\n53458\n64460\n0.546651\n\n\nSalamanca\n1 de enero de 2023\n28079\nMadrid\n4\n4\nSalamanca\n145702\n64452\n81250\n0.557645\n\n\nChamartín\n1 de enero de 2023\n28079\nMadrid\n5\n5\nChamartín\n144796\n65245\n79551\n0.549401\n\n\n\n\n\n\n\n\n\n\nHere, you explore how to subset parts of a DataFrame if you know exactly which bits you want.\nFor example, if you want to extract the “date”, “neighbourhood”, and “ratio_women” columns for the “Centro” and “Retiro” districts, you can use the Pandas loc indexer. Indexing in Pandas is very flexible and powerful, but can also be a bit confusing for the same reason. The loc documentation and some back-and-forth with AI can be helpful in clearing up points of confusion!\n\nwomen_ratio_2districts = madrid_pop.loc[\n    [\"Centro\", \"Retiro\"],  # the rows to retrieve\n    [\"date\", \"neighbourhood\", \"ratio_women\"],  # the columns to retrieve\n]\nwomen_ratio_2districts\n\n\n\n\n\n\n\n\ndate\nneighbourhood\nratio_women\n\n\ndistrict\n\n\n\n\n\n\n\nCentro\n1 de enero de 2023\nCentro\n0.493367\n\n\nCentro\n1 de enero de 2023\nPalacio\n0.508234\n\n\nCentro\n1 de enero de 2023\nEmbajadores\n0.478530\n\n\nCentro\n1 de enero de 2023\nCortes\n0.505362\n\n\nCentro\n1 de enero de 2023\nJusticia\n0.491685\n\n\n...\n...\n...\n...\n\n\nRetiro\n1 de enero de 2018\nAdelfas\n0.540638\n\n\nRetiro\n1 de enero de 2018\nEstrella\n0.535416\n\n\nRetiro\n1 de enero de 2018\nIbiza\n0.562966\n\n\nRetiro\n1 de enero de 2018\nLosJerónimos\n0.531449\n\n\nRetiro\n1 de enero de 2018\nNiñoJesús\n0.545746\n\n\n\n\n84 rows × 3 columns\n\n\n\nYou can see how you can create a list with the names (index IDs) along each of the two dimensions of a DataFrame (rows and columns), and loc will return a subset of the original table only with the elements queried for.\nAn alternative to list-based queries is what is called “range-based” queries. These work on the indices of the table, but instead of requiring the ID of each item you want to retrieve, they operate by requiring only two IDs: the first and last element in a range of items. Range queries are expressed with a colon (:). However, to perform this operation Index IDs need to be unique. Since this is not our case we will first create a new index composed the year and the code_neighbourhood as a new index to our DataFrame.\n\n# Reset the index to move 'district' back to a regular column\nmadrid_pop.reset_index(inplace=True)\n# Extract the last 4 digits from the 'date' column and create a new 'year' column\nmadrid_pop[\"year\"] = madrid_pop[\"date\"].str[-4:]\n# Create a new column with the combination of 'year' and 'code_neighbourhood'\nmadrid_pop[\"new_index\"] = madrid_pop[\"year\"] + \"_\" + madrid_pop[\"code_neighbourhood\"]\n# Set this as the new index\nmadrid_pop.set_index(\"new_index\", inplace=True)\nmadrid_pop.head(3)\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023_1\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n1\nCentro\n139687\n70770\n68917\n0.493367\n2023\n\n\n2023_2\nArganzuela\n1 de enero de 2023\n28079\nMadrid\n2\n2\nArganzuela\n153304\n71754\n81550\n0.531950\n2023\n\n\n2023_3\nRetiro\n1 de enero de 2023\n28079\nMadrid\n3\n3\nRetiro\n117918\n53458\n64460\n0.546651\n2023\n\n\n\n\n\n\n\nLook at the order of the row and column indexes. This is important becuase “range-based” queries assume you understand the order and arrangement.\n\n# return all content between rows \"2019_1\":\"2018_1\" and columns \"num_people\":\"num_people_women\"\nrange_query = madrid_pop.loc[\"2019_1\":\"2018_1\", \"num_people\":\"num_people_women\"]\n\nrange_query\n\n\n\n\n\n\n\n\nnum_people\nnum_people_men\nnum_people_women\n\n\nnew_index\n\n\n\n\n\n\n\n2019_1\n134881\n67829\n67052\n\n\n2019_2\n153830\n71631\n82199\n\n\n2019_3\n119379\n54098\n65281\n\n\n2019_4\n146148\n64395\n81753\n\n\n2019_5\n145865\n65565\n80300\n\n\n...\n...\n...\n...\n\n\n2019_212\n1851\n952\n899\n\n\n2019_213\n7565\n3648\n3917\n\n\n2019_214\n12388\n5916\n6472\n\n\n2019_215\n7642\n3746\n3896\n\n\n2018_1\n132352\n66320\n66032\n\n\n\n\n153 rows × 3 columns\n\n\n\nThe range query picks up all the elements between the specified IDs. Note that for this to work, the first ID in the range needs to be placed before the second one in the table’s index.\nOnce you know about list and range-based queries, you can combine them!\n\n\n\nHowever, sometimes, you do not know exactly which observations you want, but you do know what conditions they need to satisfy (e.g. areas with more than 2,000 inhabitants). For these cases, DataFrames support selection based on conditions. Let us see a few examples. Suppose you want to select…\n… neighbourhoods wich over time have had less than 50% of women\n\nfewer_women = madrid_pop[madrid_pop[\"ratio_women\"] &lt; 0.5]\nfewer_women\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023_1\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n1\nCentro\n139687\n70770\n68917\n0.493367\n2023\n\n\n2023_12\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n12\nEmbajadores\n46204\n24094\n22110\n0.478530\n2023\n\n\n2023_14\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n14\nJusticia\n18219\n9261\n8958\n0.491685\n2023\n\n\n2023_16\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n16\nSol\n8164\n4239\n3925\n0.480769\n2023\n\n\n2023_81\nFuencarral-El Pardo\n1 de enero de 2023\n28079\nMadrid\n8\n81\nElPardo\n3421\n1716\n1705\n0.498392\n2023\n\n\n2023_106\nLatina\n1 de enero de 2023\n28079\nMadrid\n10\n106\nCuatroVientos\n6122\n3068\n3054\n0.498857\n2023\n\n\n2023_194\nVicálvaro\n1 de enero de 2023\n28079\nMadrid\n19\n194\nElCañaveral\n13054\n6652\n6402\n0.490424\n2023\n\n\n2023_212\nBarajas\n1 de enero de 2023\n28079\nMadrid\n21\n212\nAeropuerto\n1902\n965\n937\n0.492639\n2023\n\n\n2022_1\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n1\nCentro\n139682\n70986\n68696\n0.491803\n2022\n\n\n2022_12\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n12\nEmbajadores\n46444\n24271\n22173\n0.477414\n2022\n\n\n2022_14\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n14\nJusticia\n18015\n9221\n8794\n0.488149\n2022\n\n\n2022_16\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n16\nSol\n8117\n4232\n3885\n0.478625\n2022\n\n\n2022_106\nLatina\n1 de enero de 2022\n28079\nMadrid\n10\n106\nCuatroVientos\n5966\n2996\n2970\n0.497821\n2022\n\n\n2022_194\nVicálvaro\n1 de enero de 2022\n28079\nMadrid\n19\n194\nElCañaveral\n8944\n4525\n4419\n0.494074\n2022\n\n\n2022_212\nBarajas\n1 de enero de 2022\n28079\nMadrid\n21\n212\nAeropuerto\n1895\n967\n928\n0.489710\n2022\n\n\n2021_1\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n1\nCentro\n141236\n71881\n69355\n0.491058\n2021\n\n\n2021_12\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n12\nEmbajadores\n47238\n24767\n22471\n0.475698\n2021\n\n\n2021_14\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n14\nJusticia\n18208\n9291\n8917\n0.489730\n2021\n\n\n2021_16\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n16\nSol\n7993\n4120\n3873\n0.484549\n2021\n\n\n2021_81\nFuencarral-El Pardo\n1 de enero de 2021\n28079\nMadrid\n8\n81\nElPardo\n3443\n1723\n1720\n0.499564\n2021\n\n\n2021_194\nVicálvaro\n1 de enero de 2021\n28079\nMadrid\n19\n194\nElCañaveral\n4430\n2254\n2176\n0.491196\n2021\n\n\n2021_212\nBarajas\n1 de enero de 2021\n28079\nMadrid\n21\n212\nAeropuerto\n1918\n988\n930\n0.484880\n2021\n\n\n2020_1\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n1\nCentro\n140473\n71127\n69346\n0.493661\n2020\n\n\n2020_12\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n12\nEmbajadores\n47048\n24497\n22551\n0.479319\n2020\n\n\n2020_14\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n14\nJusticia\n18021\n9161\n8860\n0.491649\n2020\n\n\n2020_16\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n16\nSol\n7622\n3895\n3727\n0.488979\n2020\n\n\n2020_106\nLatina\n1 de enero de 2020\n28079\nMadrid\n10\n106\nCuatroVientos\n5881\n2958\n2923\n0.497024\n2020\n\n\n2020_194\nVicálvaro\n1 de enero de 2020\n28079\nMadrid\n19\n194\nElCañaveral\n2398\n1230\n1168\n0.487073\n2020\n\n\n2020_212\nBarajas\n1 de enero de 2020\n28079\nMadrid\n21\n212\nAeropuerto\n1900\n975\n925\n0.486842\n2020\n\n\n2019_1\nCentro\n1 de enero de 2019\n28079\nMadrid\n1\n1\nCentro\n134881\n67829\n67052\n0.497120\n2019\n\n\n2019_12\nCentro\n1 de enero de 2019\n28079\nMadrid\n1\n12\nEmbajadores\n45259\n23390\n21869\n0.483197\n2019\n\n\n2019_14\nCentro\n1 de enero de 2019\n28079\nMadrid\n1\n14\nJusticia\n17153\n8675\n8478\n0.494258\n2019\n\n\n2019_16\nCentro\n1 de enero de 2019\n28079\nMadrid\n1\n16\nSol\n7337\n3749\n3588\n0.489028\n2019\n\n\n2019_106\nLatina\n1 de enero de 2019\n28079\nMadrid\n10\n106\nCuatroVientos\n5748\n2909\n2839\n0.493911\n2019\n\n\n2019_194\nVicálvaro\n1 de enero de 2019\n28079\nMadrid\n19\n194\nElCañaveral\n1530\n785\n745\n0.486928\n2019\n\n\n2019_212\nBarajas\n1 de enero de 2019\n28079\nMadrid\n21\n212\nAeropuerto\n1851\n952\n899\n0.485683\n2019\n\n\n2018_1\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n1\nCentro\n132352\n66320\n66032\n0.498912\n2018\n\n\n2018_12\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n12\nEmbajadores\n44630\n23031\n21599\n0.483957\n2018\n\n\n2018_14\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n14\nJusticia\n16578\n8334\n8244\n0.497286\n2018\n\n\n2018_16\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n16\nSol\n7201\n3672\n3529\n0.490071\n2018\n\n\n2018_106\nLatina\n1 de enero de 2018\n28079\nMadrid\n10\n106\nCuatroVientos\n5662\n2870\n2792\n0.493112\n2018\n\n\n2018_194\nVicálvaro\n1 de enero de 2018\n28079\nMadrid\n19\n194\nElCañaveral\n945\n490\n455\n0.481481\n2018\n\n\n2018_212\nBarajas\n1 de enero de 2018\n28079\nMadrid\n21\n212\nAeropuerto\n1794\n922\n872\n0.486065\n2018\n\n\n\n\n\n\n\n… most populated area across all years:\n\nlargest_hood_num = madrid_pop[\"num_people\"].max()\nlargest_hood = madrid_pop[madrid_pop[\"num_people\"] == largest_hood_num]\nlargest_hood\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023_11\nCarabanchel\n1 de enero de 2023\n28079\nMadrid\n11\n11\nCarabanchel\n262339\n122632\n139707\n0.532544\n2023\n\n\n\n\n\n\n\nIf you are interested, more detail about query is available in the pandas documentation. This is another way of slicing Dataframes, but for now we will stay with the loc function.\n\n\n\nNow, all of these queries can be combined with each other for further flexibility. For example, imagine you want to know the areas that have more than 100K inhabitants and have over 50% of women.\n\nwomen_power = madrid_pop.loc[\n    # the & symbol will combine True conditions for the left result set\n    # with the True conditions from the right result set\n    # there is also an \"or\" operator which uses the \"Pipe\" symbol \"|\" instead\n    (madrid_pop[\"num_people_women\"] &gt; 100000)\n    & (madrid_pop[\"ratio_women\"] &gt; 0.5)\n]\nwomen_power\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023_8\nFuencarral-El Pardo\n1 de enero de 2023\n28079\nMadrid\n8\n8\nFuencarral-El Pardo\n248443\n116944\n131499\n0.529292\n2023\n\n\n2023_10\nLatina\n1 de enero de 2023\n28079\nMadrid\n10\n10\nLatina\n241672\n112093\n129579\n0.536177\n2023\n\n\n2023_11\nCarabanchel\n1 de enero de 2023\n28079\nMadrid\n11\n11\nCarabanchel\n262339\n122632\n139707\n0.532544\n2023\n\n\n2023_13\nPuente de Vallecas\n1 de enero de 2023\n28079\nMadrid\n13\n13\nPuente de Vallecas\n241603\n114542\n127061\n0.525908\n2023\n\n\n2023_15\nCiudad Lineal\n1 de enero de 2023\n28079\nMadrid\n15\n15\nCiudad Lineal\n220345\n100759\n119586\n0.542722\n2023\n\n\n2023_16\nHortaleza\n1 de enero de 2023\n28079\nMadrid\n16\n16\nHortaleza\n198391\n94100\n104291\n0.525684\n2023\n\n\n2022_8\nFuencarral-El Pardo\n1 de enero de 2022\n28079\nMadrid\n8\n8\nFuencarral-El Pardo\n246281\n115955\n130326\n0.529176\n2022\n\n\n2022_10\nLatina\n1 de enero de 2022\n28079\nMadrid\n10\n10\nLatina\n237048\n109928\n127120\n0.536263\n2022\n\n\n2022_11\nCarabanchel\n1 de enero de 2022\n28079\nMadrid\n11\n11\nCarabanchel\n255514\n119381\n136133\n0.532781\n2022\n\n\n2022_13\nPuente de Vallecas\n1 de enero de 2022\n28079\nMadrid\n13\n13\nPuente de Vallecas\n235638\n111748\n123890\n0.525764\n2022\n\n\n2022_15\nCiudad Lineal\n1 de enero de 2022\n28079\nMadrid\n15\n15\nCiudad Lineal\n213905\n97357\n116548\n0.544859\n2022\n\n\n2022_16\nHortaleza\n1 de enero de 2022\n28079\nMadrid\n16\n16\nHortaleza\n195017\n92532\n102485\n0.525518\n2022\n\n\n2021_8\nFuencarral-El Pardo\n1 de enero de 2021\n28079\nMadrid\n8\n8\nFuencarral-ElPardo\n247692\n116520\n131172\n0.529577\n2021\n\n\n2021_10\nLatina\n1 de enero de 2021\n28079\nMadrid\n10\n10\nLatina\n240155\n111209\n128946\n0.536928\n2021\n\n\n2021_11\nCarabanchel\n1 de enero de 2021\n28079\nMadrid\n11\n11\nCarabanchel\n258633\n120600\n138033\n0.533702\n2021\n\n\n2021_13\nPuente de Vallecas\n1 de enero de 2021\n28079\nMadrid\n13\n13\nPuentedeVallecas\n239057\n113355\n125702\n0.525824\n2021\n\n\n2021_15\nCiudad Lineal\n1 de enero de 2021\n28079\nMadrid\n15\n15\nCiudadLineal\n216818\n98514\n118304\n0.545637\n2021\n\n\n2021_16\nHortaleza\n1 de enero de 2021\n28079\nMadrid\n16\n16\nHortaleza\n193228\n91585\n101643\n0.526026\n2021\n\n\n2020_8\nFuencarral-El Pardo\n1 de enero de 2020\n28079\nMadrid\n8\n8\nFuencarral-ElPardo\n249973\n117640\n132333\n0.529389\n2020\n\n\n2020_10\nLatina\n1 de enero de 2020\n28079\nMadrid\n10\n10\nLatina\n242139\n112282\n129857\n0.536291\n2020\n\n\n2020_11\nCarabanchel\n1 de enero de 2020\n28079\nMadrid\n11\n11\nCarabanchel\n260196\n121317\n138879\n0.533748\n2020\n\n\n2020_13\nPuente de Vallecas\n1 de enero de 2020\n28079\nMadrid\n13\n13\nPuentedeVallecas\n240867\n114235\n126632\n0.525734\n2020\n\n\n2020_15\nCiudad Lineal\n1 de enero de 2020\n28079\nMadrid\n15\n15\nCiudadLineal\n219867\n99966\n119901\n0.545334\n2020\n\n\n2020_16\nHortaleza\n1 de enero de 2020\n28079\nMadrid\n16\n16\nHortaleza\n193264\n91659\n101605\n0.525732\n2020\n\n\n2019_8\nFuencarral-El Pardo\n1 de enero de 2019\n28079\nMadrid\n8\n8\nFuencarral-ElPardo\n246021\n115797\n130224\n0.529321\n2019\n\n\n2019_10\nLatina\n1 de enero de 2019\n28079\nMadrid\n10\n10\nLatina\n238154\n110401\n127753\n0.536430\n2019\n\n\n2019_11\nCarabanchel\n1 de enero de 2019\n28079\nMadrid\n11\n11\nCarabanchel\n253040\n117802\n135238\n0.534453\n2019\n\n\n2019_13\nPuente de Vallecas\n1 de enero de 2019\n28079\nMadrid\n13\n13\nPuentedeVallecas\n234770\n111183\n123587\n0.526417\n2019\n\n\n2019_15\nCiudad Lineal\n1 de enero de 2019\n28079\nMadrid\n15\n15\nCiudadLineal\n216270\n98370\n117900\n0.545152\n2019\n\n\n2018_8\nFuencarral-El Pardo\n1 de enero de 2018\n28079\nMadrid\n8\n8\nFuencarral-ElPardo\n242928\n114433\n128495\n0.528943\n2018\n\n\n2018_10\nLatina\n1 de enero de 2018\n28079\nMadrid\n10\n10\nLatina\n235785\n109392\n126393\n0.536052\n2018\n\n\n2018_11\nCarabanchel\n1 de enero de 2018\n28079\nMadrid\n11\n11\nCarabanchel\n248220\n115525\n132695\n0.534586\n2018\n\n\n2018_13\nPuente de Vallecas\n1 de enero de 2018\n28079\nMadrid\n13\n13\nPuentedeVallecas\n230488\n109044\n121444\n0.526899\n2018\n\n\n2018_15\nCiudad Lineal\n1 de enero de 2018\n28079\nMadrid\n15\n15\nCiudadLineal\n214463\n97301\n117162\n0.546304\n2018\n\n\n\n\n\n\n\n\n\n\nAmong the many operations DataFrame objects support, one of the most useful ones is to sort a table based on a given column. For example, imagine you want to sort the table by the ratio of women:\n\nmadrid_sorted = madrid_pop.sort_values(\"ratio_women\", ascending=False)\nmadrid_sorted\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2018_158\nCiudad Lineal\n1 de enero de 2018\n28079\nMadrid\n15\n158\nAtalaya\n1575\n656\n919\n0.583492\n2018\n\n\n2019_158\nCiudad Lineal\n1 de enero de 2019\n28079\nMadrid\n15\n158\nAtalaya\n1568\n654\n914\n0.582908\n2019\n\n\n2023_158\nCiudad Lineal\n1 de enero de 2023\n28079\nMadrid\n15\n158\nAtalaya\n1622\n691\n931\n0.573983\n2023\n\n\n2020_158\nCiudad Lineal\n1 de enero de 2020\n28079\nMadrid\n15\n158\nAtalaya\n1555\n667\n888\n0.571061\n2020\n\n\n2020_45\nSalamanca\n1 de enero de 2020\n28079\nMadrid\n4\n45\nLista\n21211\n9111\n12100\n0.570459\n2020\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2020_12\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n12\nEmbajadores\n47048\n24497\n22551\n0.479319\n2020\n\n\n2022_16\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n16\nSol\n8117\n4232\n3885\n0.478625\n2022\n\n\n2023_12\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n12\nEmbajadores\n46204\n24094\n22110\n0.478530\n2023\n\n\n2022_12\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n12\nEmbajadores\n46444\n24271\n22173\n0.477414\n2022\n\n\n2021_12\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n12\nEmbajadores\n47238\n24767\n22471\n0.475698\n2021\n\n\n\n\n912 rows × 12 columns\n\n\n\nGiven the rates differ, it may be better to sort by neighbourhood and then by year.\n\nsort_ls = [\"code_neighbourhood\", \"year\"]\nmadrid_sorted = madrid_pop.sort_values(sort_ls, ascending=True)\nmadrid_sorted\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2018_1\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n1\nCentro\n132352\n66320\n66032\n0.498912\n2018\n\n\n2019_1\nCentro\n1 de enero de 2019\n28079\nMadrid\n1\n1\nCentro\n134881\n67829\n67052\n0.497120\n2019\n\n\n2020_1\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n1\nCentro\n140473\n71127\n69346\n0.493661\n2020\n\n\n2021_1\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n1\nCentro\n141236\n71881\n69355\n0.491058\n2021\n\n\n2022_1\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n1\nCentro\n139682\n70986\n68696\n0.491803\n2022\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2019_97\nMoncloa-Aravaca\n1 de enero de 2019\n28079\nMadrid\n9\n97\nAravaca\n26823\n12619\n14204\n0.529546\n2019\n\n\n2020_97\nMoncloa-Aravaca\n1 de enero de 2020\n28079\nMadrid\n9\n97\nAravaca\n27503\n12899\n14604\n0.530997\n2020\n\n\n2021_97\nMoncloa-Aravaca\n1 de enero de 2021\n28079\nMadrid\n9\n97\nAravaca\n27568\n12896\n14672\n0.532211\n2021\n\n\n2022_97\nMoncloa-Aravaca\n1 de enero de 2022\n28079\nMadrid\n9\n97\nAravaca\n27323\n12779\n14544\n0.532299\n2022\n\n\n2023_97\nMoncloa-Aravaca\n1 de enero de 2023\n28079\nMadrid\n9\n97\nAravaca\n27445\n12836\n14609\n0.532301\n2023\n\n\n\n\n912 rows × 12 columns\n\n\n\nThis allows you to do so-called hierarchical sorting: sort first based on one column then based on another.\n\n\n\n\nThe next step to continue exploring a dataset is to get a feel for what it looks like, visually. We have already learnt how to uncover and inspect specific parts of the data to check for particular cases we might be interested in. Now, we will see how to plot the data to get a sense of the overall distribution of values. For that, we can use the plotting capabilities of pandas.\n\n\nOne of the most common graphical devices to display the distribution of values in a variable is a histogram. Values are assigned into groups of equal intervals, and the groups are plotted as bars rising as high as the number of values into the group.\nA histogram is easily created with the following command. In this case, let’s have a look at the shape of the overall numbers of people:\n\nmadrid_pop[\"num_people\"].plot.hist(bins=15)\n\n\n\n\n\n\n\n\nHowever, the default pandas plots can be a bit dull. A better option is to use another package, called seaborn.\nseaborn is, by convention, imported as sns. Seaborn is a humorous reference to Samuel Normal Seaborn, a fictional character The West Wing show.\nThe same plot using seaborn has more agreeable default styles and more customisability.\n\nimport seaborn as sns\n\n# Set the style\nsns.set_style(\"darkgrid\")\nsns.histplot(madrid_pop[\"num_people\"], kde=True, bins=15)\n\n\n\n\n\n\n\n\nNote we are using sns instead of pd, as the function belongs to seaborn instead of pandas.\nWe can quickly see most of the areas have seen somewhere between 0 and 50K people; and very few have more than 200K. However, remember that in this case we are visualizing all years together, which could lead to misinterpretations.\n\n\n\nAnother very common way of visually displaying a variable is with a line or a bar chart. For example, if you want to generate a line plot of the (sorted) total population per year:\n\ntotal_people_per_year = madrid_pop.groupby(\"year\")[\"num_people\"].sum()\ntotal_people_per_year.plot()\n\n\n\n\n\n\n\n\nWhat is evident is the impact of COVID on the total population. But understanding that the data is reported on the 1st of January of each year is crucial to understand why you see the offset on the dates.\nFor a bar plot all you need to do is to change from plot to plot.bar:\n\ntotal_people_per_year.plot.bar()\n\n\n\n\n\n\n\n\nLet’s try to plot the ratio_women per neighbourhood, to see if we spot anything in particular.\n\nsns.lineplot(\n    x=\"year\",\n    y=\"ratio_women\",\n    hue=\"neighbourhood\",\n    data=madrid_pop.sort_values(\"year\", ascending=True),\n    legend=False,\n)\n\n\n\n\n\n\n\n\nWe can see some outliers, but the reality is that the data is hard to read so we probably would need some further analysis and visual considerations to efficiently communicate any possible trends.\n\n\n\n\n\nClean vs. Tidy\nThis section is a bit more advanced and hence considered optional. Feel free to skip it and return later when you feel more confident.\n\nOnce you can read your data in, explore specific cases, and have a first visual approach to the entire set, the next step can be preparing it for more sophisticated analysis. Maybe you are thinking of modelling it through regression, or on creating subgroups in the dataset with particular characteristics, or maybe you simply need to present summary measures that relate to a slightly different arrangement of the data than you have been presented with.\nFor all these cases, you first need what statistician, and general R wizard, Hadley Wickham calls “tidy data”. The general idea to “tidy” your data is to convert them from whatever structure they were handed to you into one that allows convenient and standardized manipulation, and that supports directly inputting the data into what he calls “tidy” analysis tools. But, at a more practical level, what is exactly “tidy data”? In Wickham’s own words:\n\nTidy data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types.\n\nHe then goes on to list the three fundamental characteristics of “tidy data”:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\nIf you are further interested in the concept of “tidy data”, we recommend the original paper (open access) and the associated public repository.\n\n\n\nOne of the advantage of tidy datasets is they allow advanced transformations in a more direct way. One of the most common ones is what is called “group-by” operations. These originated in the world of databases, and allow you to group observations from a data table by labels, index, or category, and to then apply operations on the data on a group by group basis.\nFor example, given our DataFrame, we might want to compute the total sum of the population by each district. This task can be split into two different steps:\n\nGroup the table in each of the different districts.\nCompute the sum of num_people for each of them.\n\nTo do this in pandas, meet one of its workhorses and also one of the reasons why the library has become so popular: the groupby operator.\n\nmad_grouped = madrid_pop.groupby(\"year\")\nmad_grouped\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x1442cc850&gt;\n\n\nThe object mad_grouped still hasn’t computed anything. It is only a convenient way of specifying the grouping. But this allows us then to perform a multitude of operations on it. For our example, the sum is calculated as follows:\n\nmad_grouped.sum(numeric_only=True)\n\n\n\n\n\n\n\n\ncode_municipality\nnum_people\nnum_people_men\nnum_people_women\nratio_women\n\n\nyear\n\n\n\n\n\n\n\n\n\n2018\n4268008\n6443648\n3000680\n3442968\n81.100391\n\n\n2019\n4268008\n6532252\n3042356\n3489896\n81.108315\n\n\n2020\n4268008\n6669460\n3109464\n3559996\n81.047788\n\n\n2021\n4268008\n6624620\n3090314\n3534306\n81.013452\n\n\n2022\n4268008\n6573324\n3069648\n3503676\n80.939052\n\n\n2023\n4268008\n6679862\n3119732\n3560130\n80.942860\n\n\n\n\n\n\n\nSimilarly, we can also obtain a summary of each group:\n\nmad_grouped.describe()\n\n\n\n\n\n\n\n\ncode_municipality\nnum_people\n...\nnum_people_women\nratio_women\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ncount\nmean\n...\n75%\nmax\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nyear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2018\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n42392.421053\n...\n21771.75\n132695.0\n152.0\n0.533555\n0.018390\n0.481481\n0.521591\n0.534767\n0.545880\n0.583492\n\n\n2019\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n42975.342105\n...\n21987.00\n135238.0\n152.0\n0.533607\n0.018198\n0.483197\n0.522356\n0.534355\n0.545650\n0.582908\n\n\n2020\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n43878.026316\n...\n22623.75\n138879.0\n152.0\n0.533209\n0.017944\n0.479319\n0.522586\n0.533905\n0.545599\n0.571061\n\n\n2021\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n43583.026316\n...\n22530.75\n138033.0\n152.0\n0.532983\n0.017984\n0.475698\n0.522263\n0.533724\n0.545682\n0.568987\n\n\n2022\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n43245.552632\n...\n22296.50\n136133.0\n152.0\n0.532494\n0.017716\n0.477414\n0.521691\n0.533305\n0.544920\n0.567140\n\n\n2023\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n43946.460526\n...\n22742.75\n139707.0\n152.0\n0.532519\n0.017486\n0.478530\n0.522637\n0.533363\n0.544488\n0.573983\n\n\n\n\n6 rows × 40 columns\n\n\n\nWe will not get into it today as it goes beyond the basics this chapter covers, but keep in mind that groupby allows us to not only call generic functions (like sum or describe), but also custom functions. This opens the door for virtually any kind of transformation and aggregation possible.\nAdditional reading\n\nA good introduction to data manipulation in Python is Wes McKinney’s Python for Data Analysis.\nTo further explore some of the visualization capabilities, the Python library seaborn is an excellent choice. Its online tutorial is a fantastic place to start.\nA good extension is Hadley Wickham’s “Tidy data” paper, which presents a very popular way of organising tabular data for efficient manipulation.",
    "crumbs": [
      "Basic Coding",
      "Pandas & Data"
    ]
  },
  {
    "objectID": "chapter_2/pandas.html#acknowledgements",
    "href": "chapter_2/pandas.html#acknowledgements",
    "title": "Pandas & Data",
    "section": "",
    "text": "This section is adapted from the Spatial Data Science for Social Geography by Martin Fleischman, which in turn is based on A Course on Geographic Data Science by Dani Arribas-Bel, both licensed under CC-BY-SA 4.0. The content and datasets were reworked to accommodate the Madrid neighbourhood population statistics dataset.",
    "crumbs": [
      "Basic Coding",
      "Pandas & Data"
    ]
  },
  {
    "objectID": "chapter_2/pandas.html#munging-and-wrangling",
    "href": "chapter_2/pandas.html#munging-and-wrangling",
    "title": "Pandas & Data",
    "section": "",
    "text": "Real-world datasets are messy. There is no way around it: datasets have “holes” (missing data), the number of formats in which data can be stored can be endless, and the best structure to share data is not always the optimal for analysis. Hence the need to “munge” or “wrangle” data. Much of the time spent in what is called Data Science is related not only to modelling and insight but more often than not has to do with much more basic and less exotic tasks such as obtaining the data, data cleaning, and other preparation which makes analysis possible.\nSurprisingly, very little has been published on patterns, techniques, and best practices for quick and efficient data cleaning, manipulation, and transformation because of how labour-intensive this aspect is. In this session, we will use real-world datasets and learn how to transform and manipulate these, if necessary, prior to analysis. For this, we will introduce some of the bread-and-butter of data analysis and scientific computing in Python. These are fundamental tools that are widely and consistently used on almost any tasks relating to data analysis.\nThis notebook covers the basics and the content that needs to be grasped, and discusses several patterns to clean and structure data properly, including tidying, subsetting, and aggregating. We will conclude with some basic visualisation. An additional extension presents more advanced tricks to manipulate tabular data.",
    "crumbs": [
      "Basic Coding",
      "Pandas & Data"
    ]
  },
  {
    "objectID": "chapter_2/pandas.html#dataset",
    "href": "chapter_2/pandas.html#dataset",
    "title": "Pandas & Data",
    "section": "",
    "text": "We will be exploring demographic characteristics of Madrid. The data has been aggregated to a neighbourhood level by the statistic’s office of Madrid’s City Hall. It contains information per year (from 2018 to 2023) and theme.\nAs with many datasets that will be used during this course, the data was originally found in an online data portal at the following link\nThe main tool we will use for this task is the pandas package. As with any Python module or package, the Pandas package has to be imported into the Notebook prior to usage.\n\nimport pandas as pd\n\nPandas let’s us work with datasets, and stores the information in a DataFrame consisting of rows and columns. Here, we will read an online csv dataset into a Pandas DataFrame. Pandas can open local files directly, but also has the nifty ability to download files directly from the web:\n\n# here we will directly fetch the CSV dataset and save it as a Pandas DataFrame\nmadrid_pop = pd.read_csv(\n    \"https://datos.madrid.es/egob/catalogo/300557-0-poblacion-distrito-barrio.csv\",\n    sep=\";\",\n)\n\n\nDelimiters\nIn this case we are using the Pandas read_csv method because the source file is a csv. By default, csv files are assumed to use commas for data delimitation, but this file uses semi-colons instead. This is why we are passing the optional sep parameter to specify a ; delimeter.\n\n\nFormats\nNote that pandas allows for many more formats to be read and written. A full list of formats supported may be found in the documentation.\n\nIt is also possible to download the file and read it locally. In this case, you would download the file by clicking on this link. Then place it in the same folder as the notebook where you intend to read it from, and run the below cell.\n\n# remember, this cell will only work if you have downloaded the file\n# and, if the filepath is correct!\n# uncomment the below lines to run!\n# madrid_pop = pd.read_csv(\n#     \"poblacion_1_enero.csv\",\n#     sep=\";\",\n# )",
    "crumbs": [
      "Basic Coding",
      "Pandas & Data"
    ]
  },
  {
    "objectID": "chapter_2/pandas.html#pandas-101",
    "href": "chapter_2/pandas.html#pandas-101",
    "title": "Pandas & Data",
    "section": "",
    "text": "Now, we are ready to start playing and interrogating the dataset! What you have at your fingertips is a table summarising, for each of the districts in Madrid, how many people lived there by gender. These tables are called DataFrame objects, and they have a lot of functionality built-in to explore and manipulate the data they contain. Let’s explore a few of those cool tricks!\n\n\nThe first aspect worth spending a bit of time on is the structure of a DataFrame. You can print it by simply typing its name:\n\nmadrid_pop\n\n\n\n\n\n\n\n\nfecha\ncod_municipio\nmunicipio\ncod_distrito\ndistrito\ncod_barrio\nbarrio\nnum_personas\nnum_personas_hombres\nnum_personas_mujeres\n\n\n\n\n0\n1 de enero de 2023\n28079\nMadrid\n1\nCentro\n1\nCentro\n139.687\n70.770\n68.917\n\n\n1\n1 de enero de 2023\n28079\nMadrid\n2\nArganzuela\n2\nArganzuela\n153.304\n71.754\n81.550\n\n\n2\n1 de enero de 2023\n28079\nMadrid\n3\nRetiro\n3\nRetiro\n117.918\n53.458\n64.460\n\n\n3\n1 de enero de 2023\n28079\nMadrid\n4\nSalamanca\n4\nSalamanca\n145.702\n64.452\n81.250\n\n\n4\n1 de enero de 2023\n28079\nMadrid\n5\nChamartín\n5\nChamartín\n144.796\n65.245\n79.551\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n913\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n212\nAeropuerto\n1.794\n922\n872\n\n\n914\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n213\nCascoHistóricodeBarajas\n7.336\n3.550\n3.786\n\n\n915\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n214\nTimón\n11.750\n5.651\n6.099\n\n\n916\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n215\nCorralejos\n7.510\n3.657\n3.853\n\n\n917\n1 de enero de 2018\n28079\nMadrid\nTodos\nTodos\nTodos\nTodos\n3.221.824\n1.500.340\n1.721.484\n\n\n\n\n918 rows × 10 columns\n\n\n\nAs you can expect, the dataset was provided in the original local language, so to make things a little easier, we are going to translate the column names using one list of texts. for this we will reassign the column names corresponding to their English equivalents.\n\nNote that there are multiple ways of arriving at the same output, the below is just one of them\n\n\nen_cols = [\n    \"date\",\n    \"code_municipality\",\n    \"municipality\",\n    \"code_district\",\n    \"district\",\n    \"code_neighbourhood\",\n    \"neighbourhood\",\n    \"num_people\",\n    \"num_people_men\",\n    \"num_people_women\",\n]\nmadrid_pop.columns = en_cols\nmadrid_pop\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ndistrict\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\n\n\n\n\n0\n1 de enero de 2023\n28079\nMadrid\n1\nCentro\n1\nCentro\n139.687\n70.770\n68.917\n\n\n1\n1 de enero de 2023\n28079\nMadrid\n2\nArganzuela\n2\nArganzuela\n153.304\n71.754\n81.550\n\n\n2\n1 de enero de 2023\n28079\nMadrid\n3\nRetiro\n3\nRetiro\n117.918\n53.458\n64.460\n\n\n3\n1 de enero de 2023\n28079\nMadrid\n4\nSalamanca\n4\nSalamanca\n145.702\n64.452\n81.250\n\n\n4\n1 de enero de 2023\n28079\nMadrid\n5\nChamartín\n5\nChamartín\n144.796\n65.245\n79.551\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n913\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n212\nAeropuerto\n1.794\n922\n872\n\n\n914\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n213\nCascoHistóricodeBarajas\n7.336\n3.550\n3.786\n\n\n915\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n214\nTimón\n11.750\n5.651\n6.099\n\n\n916\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n215\nCorralejos\n7.510\n3.657\n3.853\n\n\n917\n1 de enero de 2018\n28079\nMadrid\nTodos\nTodos\nTodos\nTodos\n3.221.824\n1.500.340\n1.721.484\n\n\n\n\n918 rows × 10 columns\n\n\n\nThe default printing is truncated to keep a compact view, but is enough to convey its structure. DataFrame objects have two dimensions: rows and columns. Rows are typically identified with an index column and columns are typically identified with names describing their content. The above example shows how the column names were automatically lifted from the csv file’s column headers.\nWe can set the row index using the set_index() method. Let’s set the index to use the district column.\n\n# Remove leading and trailing spaces from 'distrito'\nmadrid_pop[\"district\"] = madrid_pop[\"district\"].str.strip()\n# set the index based on the district column\nmadrid_pop.set_index(\"district\", inplace=True)\n\nColumns can be assigned as one of various forms of data such as integers as int, decimals as float, text as str, or if pandas is unable to use a standard type, it might use the object type as a catch-all type.\nTo extract a single column from this DataFrame, specify its name in square brackets ([]). Note that the name is a string - a piece of text - which needs to be denoted with single (') or double quotes (\"). Without the quotes, Python will think you are referring to a variable, and will then complain that the variable can’t be found!\nA single column is not a DataFrame but a Series, which also means that a DataFrame is a collection of Series!\n\n# this will fetch and return the num_people_women column as a Series\nmadrid_pop[\"num_people_women\"]\n\ndistrict\nCentro           68.917\nArganzuela       81.550\nRetiro           64.460\nSalamanca        81.250\nChamartín        79.551\n                ...    \nBarajas             872\nBarajas           3.786\nBarajas           6.099\nBarajas           3.853\nTodos         1.721.484\nName: num_people_women, Length: 918, dtype: object\n\n\n\n\n\nYou can specifically print the DataFrames top (or bottom) lines by passing a number to the method head or tail. For example, for the top or bottom three lines:\n\nmadrid_pop.head(3)\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\n\n\ndistrict\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n1\nCentro\n139.687\n70.770\n68.917\n\n\nArganzuela\n1 de enero de 2023\n28079\nMadrid\n2\n2\nArganzuela\n153.304\n71.754\n81.550\n\n\nRetiro\n1 de enero de 2023\n28079\nMadrid\n3\n3\nRetiro\n117.918\n53.458\n64.460\n\n\n\n\n\n\n\n\nmadrid_pop.tail(3)\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\n\n\ndistrict\n\n\n\n\n\n\n\n\n\n\n\n\n\nBarajas\n1 de enero de 2018\n28079\nMadrid\n21\n214\nTimón\n11.750\n5.651\n6.099\n\n\nBarajas\n1 de enero de 2018\n28079\nMadrid\n21\n215\nCorralejos\n7.510\n3.657\n3.853\n\n\nTodos\n1 de enero de 2018\n28079\nMadrid\nTodos\nTodos\nTodos\n3.221.824\n1.500.340\n1.721.484\n\n\n\n\n\n\n\nInspecting datasets is vital to find errors that might skew your analysis. The last row of the dataset contains the sums of other column values ( Spanish: Todos -&gt; English: All ).\nBefore continuing, let’s fix this by removing these columns from the dataset. First, let’s check if we have any more occurrences of Todos. We can use Pandas’ loc indexer to fetch rows where the index column contains the text string Todos.\n\nmadrid_pop.loc[\"Todos\"]\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\n\n\ndistrict\n\n\n\n\n\n\n\n\n\n\n\n\n\nTodos\n1 de enero de 2023\n28079\nMadrid\nTodos\nTodos\nTodos\n3.339.931\n1.559.866\n1.780.065\n\n\nTodos\n1 de enero de 2022\n28079\nMadrid\nTodos\nTodos\nTodos\n3.286.662\n1.534.824\n1.751.838\n\n\nTodos\n1 de enero de 2021\n28079\nMadrid\nTodos\nTodos\nTodos\n3.312.310\n1.545.157\n1.767.153\n\n\nTodos\n1 de enero de 2020\n28079\nMadrid\nTodos\nTodos\nTodos\n3.334.730\n1.554.732\n1.779.998\n\n\nTodos\n1 de enero de 2019\n28079\nMadrid\nTodos\nTodos\nTodos\n3.266.126\n1.521.178\n1.744.948\n\n\nTodos\n1 de enero de 2018\n28079\nMadrid\nTodos\nTodos\nTodos\n3.221.824\n1.500.340\n1.721.484\n\n\n\n\n\n\n\nNow that we know that for every year the data has been aggregated and added back as a new row, we know that removing the last row would not be enough to correct our DataFrame.\nWe will make use of the drop function in combination with the native loc function of DataFrames.\n\nprint(\"Rows before droping values: \", len(madrid_pop))\nmadrid_pop.drop(index=\"Todos\", inplace=True)\nprint(\"Rows after droping values: \", len(madrid_pop))\n\nRows before droping values:  918\nRows after droping values:  912\n\n\nNow, let’s get an overview of the table:\n\nmadrid_pop.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 912 entries, Centro to Barajas\nData columns (total 9 columns):\n #   Column              Non-Null Count  Dtype \n---  ------              --------------  ----- \n 0   date                912 non-null    object\n 1   code_municipality   912 non-null    int64 \n 2   municipality        912 non-null    object\n 3   code_district       912 non-null    object\n 4   code_neighbourhood  912 non-null    object\n 5   neighbourhood       912 non-null    object\n 6   num_people          912 non-null    object\n 7   num_people_men      912 non-null    object\n 8   num_people_women    912 non-null    object\ndtypes: int64(1), object(8)\nmemory usage: 71.2+ KB\n\n\nCan you spot something wrong?\nInteger numbers are sometimes stored as text values, especially if the input dataset contained commas, periods, or null data values. The actual values of population counts would be better stored as full integers so that we can more effectively work with this data as a numeric data type. Let’s take the following steps:\n\nCreate a list containg the column names that have people counts\nLoop through the list, removing periods from the numbers, then casting the values to integer types while allowing for null data values\n\n\n# List of columns to process\ncolumns_to_process = [\"num_people\", \"num_people_men\", \"num_people_women\"]\n\n# Loop through columns\nfor column in columns_to_process:\n    # create a copy of the column\n    dirty_numbers = madrid_pop[column]\n    # remove \".\" from the numbers\n    clean_numbers = dirty_numbers.str.replace(\".\", \"\")\n    # convert to numeric - coerce will convert non numeric values to NaN (Not a Number) or \"null\" types\n    numeric_numbers = pd.to_numeric(clean_numbers, errors=\"coerce\")\n    # overwrite the original column\n    madrid_pop[column] = numeric_numbers\n\nmadrid_pop.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 912 entries, Centro to Barajas\nData columns (total 9 columns):\n #   Column              Non-Null Count  Dtype \n---  ------              --------------  ----- \n 0   date                912 non-null    object\n 1   code_municipality   912 non-null    int64 \n 2   municipality        912 non-null    object\n 3   code_district       912 non-null    object\n 4   code_neighbourhood  912 non-null    object\n 5   neighbourhood       912 non-null    object\n 6   num_people          912 non-null    int64 \n 7   num_people_men      912 non-null    int64 \n 8   num_people_women    912 non-null    int64 \ndtypes: int64(4), object(5)\nmemory usage: 71.2+ KB\n\n\n\n\n\n\nmadrid_pop.describe()\n\n\n\n\n\n\n\n\ncode_municipality\nnum_people\nnum_people_men\nnum_people_women\n\n\n\n\ncount\n912.0\n912.000000\n912.000000\n912.000000\n\n\nmean\n28079.0\n43336.804825\n20210.739035\n23126.065789\n\n\nstd\n0.0\n51564.255101\n24072.657139\n27522.444389\n\n\nmin\n28079.0\n945.000000\n490.000000\n455.000000\n\n\n25%\n28079.0\n17330.500000\n8132.000000\n9267.250000\n\n\n50%\n28079.0\n24700.500000\n11580.000000\n13640.000000\n\n\n75%\n28079.0\n42166.500000\n19700.250000\n22349.750000\n\n\nmax\n28079.0\n262339.000000\n122632.000000\n139707.000000\n\n\n\n\n\n\n\nNote how the output is also a DataFrame object, so you can manipulate it the same way that you would with the original table (e.g. writing it to a file).\nIn this case, the summary might be better presented if the table is “transposed”:\n\nmadrid_pop[columns_to_process].describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nnum_people\n912.0\n43336.804825\n51564.255101\n945.0\n17330.50\n24700.5\n42166.50\n262339.0\n\n\nnum_people_men\n912.0\n20210.739035\n24072.657139\n490.0\n8132.00\n11580.0\n19700.25\n122632.0\n\n\nnum_people_women\n912.0\n23126.065789\n27522.444389\n455.0\n9267.25\n13640.0\n22349.75\n139707.0\n\n\n\n\n\n\n\nEqually, common descriptive statistics are also available. To obtain minimum values for each column, you can use .min().\n\nmadrid_pop.min()\n\ndate                  1 de enero de 2018\ncode_municipality                  28079\nmunicipality                      Madrid\ncode_district                          1\ncode_neighbourhood                     1\nneighbourhood                 Arganzuela\nnum_people                           945\nnum_people_men                       490\nnum_people_women                     455\ndtype: object\n\n\nOr to obtain a minimum for a single column only.\n\nmadrid_pop[\"num_people_women\"].min()\n\n455\n\n\nNote here how you have restricted the calculation of the minimum value to one column only by getting the Series and calling .min() on that.\nSimilarly, you can restrict the calculations to a single district using .loc[] indexer:\n\nmadrid_pop.loc[\"Centro\"].min()\n\ndate                  1 de enero de 2018\ncode_municipality                  28079\nmunicipality                      Madrid\ncode_district                          1\ncode_neighbourhood                     1\nneighbourhood                     Centro\nnum_people                          7201\nnum_people_men                      3672\nnum_people_women                    3529\ndtype: object\n\n\nLet’s see when and where the said minimum occurred.\n\n# we can use comparators to index into the DataFrame where a specific column equals a specific value\nmadrid_pop[madrid_pop[\"num_people_women\"] == 3529]\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\n\n\ndistrict\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n16\nSol\n7201\n3672\n3529\n\n\n\n\n\n\n\n\n\n\nYou can generate new variables by applying operations to existing ones. For example, you can calculate the ratio of women.\n\nratio_women = madrid_pop[\"num_people_women\"] / madrid_pop[\"num_people\"]\nratio_women.head()\n\ndistrict\nCentro        0.493367\nArganzuela    0.531950\nRetiro        0.546651\nSalamanca     0.557645\nChamartín     0.549401\ndtype: float64\n\n\nOnce you have created the variable, you can make it part of the table:\n\nmadrid_pop[\"ratio_women\"] = ratio_women\nmadrid_pop.head()\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\n\n\ndistrict\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n1\nCentro\n139687\n70770\n68917\n0.493367\n\n\nArganzuela\n1 de enero de 2023\n28079\nMadrid\n2\n2\nArganzuela\n153304\n71754\n81550\n0.531950\n\n\nRetiro\n1 de enero de 2023\n28079\nMadrid\n3\n3\nRetiro\n117918\n53458\n64460\n0.546651\n\n\nSalamanca\n1 de enero de 2023\n28079\nMadrid\n4\n4\nSalamanca\n145702\n64452\n81250\n0.557645\n\n\nChamartín\n1 de enero de 2023\n28079\nMadrid\n5\n5\nChamartín\n144796\n65245\n79551\n0.549401\n\n\n\n\n\n\n\n\n\n\nHere, you explore how to subset parts of a DataFrame if you know exactly which bits you want.\nFor example, if you want to extract the “date”, “neighbourhood”, and “ratio_women” columns for the “Centro” and “Retiro” districts, you can use the Pandas loc indexer. Indexing in Pandas is very flexible and powerful, but can also be a bit confusing for the same reason. The loc documentation and some back-and-forth with AI can be helpful in clearing up points of confusion!\n\nwomen_ratio_2districts = madrid_pop.loc[\n    [\"Centro\", \"Retiro\"],  # the rows to retrieve\n    [\"date\", \"neighbourhood\", \"ratio_women\"],  # the columns to retrieve\n]\nwomen_ratio_2districts\n\n\n\n\n\n\n\n\ndate\nneighbourhood\nratio_women\n\n\ndistrict\n\n\n\n\n\n\n\nCentro\n1 de enero de 2023\nCentro\n0.493367\n\n\nCentro\n1 de enero de 2023\nPalacio\n0.508234\n\n\nCentro\n1 de enero de 2023\nEmbajadores\n0.478530\n\n\nCentro\n1 de enero de 2023\nCortes\n0.505362\n\n\nCentro\n1 de enero de 2023\nJusticia\n0.491685\n\n\n...\n...\n...\n...\n\n\nRetiro\n1 de enero de 2018\nAdelfas\n0.540638\n\n\nRetiro\n1 de enero de 2018\nEstrella\n0.535416\n\n\nRetiro\n1 de enero de 2018\nIbiza\n0.562966\n\n\nRetiro\n1 de enero de 2018\nLosJerónimos\n0.531449\n\n\nRetiro\n1 de enero de 2018\nNiñoJesús\n0.545746\n\n\n\n\n84 rows × 3 columns\n\n\n\nYou can see how you can create a list with the names (index IDs) along each of the two dimensions of a DataFrame (rows and columns), and loc will return a subset of the original table only with the elements queried for.\nAn alternative to list-based queries is what is called “range-based” queries. These work on the indices of the table, but instead of requiring the ID of each item you want to retrieve, they operate by requiring only two IDs: the first and last element in a range of items. Range queries are expressed with a colon (:). However, to perform this operation Index IDs need to be unique. Since this is not our case we will first create a new index composed the year and the code_neighbourhood as a new index to our DataFrame.\n\n# Reset the index to move 'district' back to a regular column\nmadrid_pop.reset_index(inplace=True)\n# Extract the last 4 digits from the 'date' column and create a new 'year' column\nmadrid_pop[\"year\"] = madrid_pop[\"date\"].str[-4:]\n# Create a new column with the combination of 'year' and 'code_neighbourhood'\nmadrid_pop[\"new_index\"] = madrid_pop[\"year\"] + \"_\" + madrid_pop[\"code_neighbourhood\"]\n# Set this as the new index\nmadrid_pop.set_index(\"new_index\", inplace=True)\nmadrid_pop.head(3)\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023_1\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n1\nCentro\n139687\n70770\n68917\n0.493367\n2023\n\n\n2023_2\nArganzuela\n1 de enero de 2023\n28079\nMadrid\n2\n2\nArganzuela\n153304\n71754\n81550\n0.531950\n2023\n\n\n2023_3\nRetiro\n1 de enero de 2023\n28079\nMadrid\n3\n3\nRetiro\n117918\n53458\n64460\n0.546651\n2023\n\n\n\n\n\n\n\nLook at the order of the row and column indexes. This is important becuase “range-based” queries assume you understand the order and arrangement.\n\n# return all content between rows \"2019_1\":\"2018_1\" and columns \"num_people\":\"num_people_women\"\nrange_query = madrid_pop.loc[\"2019_1\":\"2018_1\", \"num_people\":\"num_people_women\"]\n\nrange_query\n\n\n\n\n\n\n\n\nnum_people\nnum_people_men\nnum_people_women\n\n\nnew_index\n\n\n\n\n\n\n\n2019_1\n134881\n67829\n67052\n\n\n2019_2\n153830\n71631\n82199\n\n\n2019_3\n119379\n54098\n65281\n\n\n2019_4\n146148\n64395\n81753\n\n\n2019_5\n145865\n65565\n80300\n\n\n...\n...\n...\n...\n\n\n2019_212\n1851\n952\n899\n\n\n2019_213\n7565\n3648\n3917\n\n\n2019_214\n12388\n5916\n6472\n\n\n2019_215\n7642\n3746\n3896\n\n\n2018_1\n132352\n66320\n66032\n\n\n\n\n153 rows × 3 columns\n\n\n\nThe range query picks up all the elements between the specified IDs. Note that for this to work, the first ID in the range needs to be placed before the second one in the table’s index.\nOnce you know about list and range-based queries, you can combine them!\n\n\n\nHowever, sometimes, you do not know exactly which observations you want, but you do know what conditions they need to satisfy (e.g. areas with more than 2,000 inhabitants). For these cases, DataFrames support selection based on conditions. Let us see a few examples. Suppose you want to select…\n… neighbourhoods wich over time have had less than 50% of women\n\nfewer_women = madrid_pop[madrid_pop[\"ratio_women\"] &lt; 0.5]\nfewer_women\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023_1\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n1\nCentro\n139687\n70770\n68917\n0.493367\n2023\n\n\n2023_12\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n12\nEmbajadores\n46204\n24094\n22110\n0.478530\n2023\n\n\n2023_14\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n14\nJusticia\n18219\n9261\n8958\n0.491685\n2023\n\n\n2023_16\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n16\nSol\n8164\n4239\n3925\n0.480769\n2023\n\n\n2023_81\nFuencarral-El Pardo\n1 de enero de 2023\n28079\nMadrid\n8\n81\nElPardo\n3421\n1716\n1705\n0.498392\n2023\n\n\n2023_106\nLatina\n1 de enero de 2023\n28079\nMadrid\n10\n106\nCuatroVientos\n6122\n3068\n3054\n0.498857\n2023\n\n\n2023_194\nVicálvaro\n1 de enero de 2023\n28079\nMadrid\n19\n194\nElCañaveral\n13054\n6652\n6402\n0.490424\n2023\n\n\n2023_212\nBarajas\n1 de enero de 2023\n28079\nMadrid\n21\n212\nAeropuerto\n1902\n965\n937\n0.492639\n2023\n\n\n2022_1\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n1\nCentro\n139682\n70986\n68696\n0.491803\n2022\n\n\n2022_12\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n12\nEmbajadores\n46444\n24271\n22173\n0.477414\n2022\n\n\n2022_14\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n14\nJusticia\n18015\n9221\n8794\n0.488149\n2022\n\n\n2022_16\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n16\nSol\n8117\n4232\n3885\n0.478625\n2022\n\n\n2022_106\nLatina\n1 de enero de 2022\n28079\nMadrid\n10\n106\nCuatroVientos\n5966\n2996\n2970\n0.497821\n2022\n\n\n2022_194\nVicálvaro\n1 de enero de 2022\n28079\nMadrid\n19\n194\nElCañaveral\n8944\n4525\n4419\n0.494074\n2022\n\n\n2022_212\nBarajas\n1 de enero de 2022\n28079\nMadrid\n21\n212\nAeropuerto\n1895\n967\n928\n0.489710\n2022\n\n\n2021_1\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n1\nCentro\n141236\n71881\n69355\n0.491058\n2021\n\n\n2021_12\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n12\nEmbajadores\n47238\n24767\n22471\n0.475698\n2021\n\n\n2021_14\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n14\nJusticia\n18208\n9291\n8917\n0.489730\n2021\n\n\n2021_16\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n16\nSol\n7993\n4120\n3873\n0.484549\n2021\n\n\n2021_81\nFuencarral-El Pardo\n1 de enero de 2021\n28079\nMadrid\n8\n81\nElPardo\n3443\n1723\n1720\n0.499564\n2021\n\n\n2021_194\nVicálvaro\n1 de enero de 2021\n28079\nMadrid\n19\n194\nElCañaveral\n4430\n2254\n2176\n0.491196\n2021\n\n\n2021_212\nBarajas\n1 de enero de 2021\n28079\nMadrid\n21\n212\nAeropuerto\n1918\n988\n930\n0.484880\n2021\n\n\n2020_1\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n1\nCentro\n140473\n71127\n69346\n0.493661\n2020\n\n\n2020_12\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n12\nEmbajadores\n47048\n24497\n22551\n0.479319\n2020\n\n\n2020_14\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n14\nJusticia\n18021\n9161\n8860\n0.491649\n2020\n\n\n2020_16\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n16\nSol\n7622\n3895\n3727\n0.488979\n2020\n\n\n2020_106\nLatina\n1 de enero de 2020\n28079\nMadrid\n10\n106\nCuatroVientos\n5881\n2958\n2923\n0.497024\n2020\n\n\n2020_194\nVicálvaro\n1 de enero de 2020\n28079\nMadrid\n19\n194\nElCañaveral\n2398\n1230\n1168\n0.487073\n2020\n\n\n2020_212\nBarajas\n1 de enero de 2020\n28079\nMadrid\n21\n212\nAeropuerto\n1900\n975\n925\n0.486842\n2020\n\n\n2019_1\nCentro\n1 de enero de 2019\n28079\nMadrid\n1\n1\nCentro\n134881\n67829\n67052\n0.497120\n2019\n\n\n2019_12\nCentro\n1 de enero de 2019\n28079\nMadrid\n1\n12\nEmbajadores\n45259\n23390\n21869\n0.483197\n2019\n\n\n2019_14\nCentro\n1 de enero de 2019\n28079\nMadrid\n1\n14\nJusticia\n17153\n8675\n8478\n0.494258\n2019\n\n\n2019_16\nCentro\n1 de enero de 2019\n28079\nMadrid\n1\n16\nSol\n7337\n3749\n3588\n0.489028\n2019\n\n\n2019_106\nLatina\n1 de enero de 2019\n28079\nMadrid\n10\n106\nCuatroVientos\n5748\n2909\n2839\n0.493911\n2019\n\n\n2019_194\nVicálvaro\n1 de enero de 2019\n28079\nMadrid\n19\n194\nElCañaveral\n1530\n785\n745\n0.486928\n2019\n\n\n2019_212\nBarajas\n1 de enero de 2019\n28079\nMadrid\n21\n212\nAeropuerto\n1851\n952\n899\n0.485683\n2019\n\n\n2018_1\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n1\nCentro\n132352\n66320\n66032\n0.498912\n2018\n\n\n2018_12\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n12\nEmbajadores\n44630\n23031\n21599\n0.483957\n2018\n\n\n2018_14\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n14\nJusticia\n16578\n8334\n8244\n0.497286\n2018\n\n\n2018_16\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n16\nSol\n7201\n3672\n3529\n0.490071\n2018\n\n\n2018_106\nLatina\n1 de enero de 2018\n28079\nMadrid\n10\n106\nCuatroVientos\n5662\n2870\n2792\n0.493112\n2018\n\n\n2018_194\nVicálvaro\n1 de enero de 2018\n28079\nMadrid\n19\n194\nElCañaveral\n945\n490\n455\n0.481481\n2018\n\n\n2018_212\nBarajas\n1 de enero de 2018\n28079\nMadrid\n21\n212\nAeropuerto\n1794\n922\n872\n0.486065\n2018\n\n\n\n\n\n\n\n… most populated area across all years:\n\nlargest_hood_num = madrid_pop[\"num_people\"].max()\nlargest_hood = madrid_pop[madrid_pop[\"num_people\"] == largest_hood_num]\nlargest_hood\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023_11\nCarabanchel\n1 de enero de 2023\n28079\nMadrid\n11\n11\nCarabanchel\n262339\n122632\n139707\n0.532544\n2023\n\n\n\n\n\n\n\nIf you are interested, more detail about query is available in the pandas documentation. This is another way of slicing Dataframes, but for now we will stay with the loc function.\n\n\n\nNow, all of these queries can be combined with each other for further flexibility. For example, imagine you want to know the areas that have more than 100K inhabitants and have over 50% of women.\n\nwomen_power = madrid_pop.loc[\n    # the & symbol will combine True conditions for the left result set\n    # with the True conditions from the right result set\n    # there is also an \"or\" operator which uses the \"Pipe\" symbol \"|\" instead\n    (madrid_pop[\"num_people_women\"] &gt; 100000)\n    & (madrid_pop[\"ratio_women\"] &gt; 0.5)\n]\nwomen_power\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023_8\nFuencarral-El Pardo\n1 de enero de 2023\n28079\nMadrid\n8\n8\nFuencarral-El Pardo\n248443\n116944\n131499\n0.529292\n2023\n\n\n2023_10\nLatina\n1 de enero de 2023\n28079\nMadrid\n10\n10\nLatina\n241672\n112093\n129579\n0.536177\n2023\n\n\n2023_11\nCarabanchel\n1 de enero de 2023\n28079\nMadrid\n11\n11\nCarabanchel\n262339\n122632\n139707\n0.532544\n2023\n\n\n2023_13\nPuente de Vallecas\n1 de enero de 2023\n28079\nMadrid\n13\n13\nPuente de Vallecas\n241603\n114542\n127061\n0.525908\n2023\n\n\n2023_15\nCiudad Lineal\n1 de enero de 2023\n28079\nMadrid\n15\n15\nCiudad Lineal\n220345\n100759\n119586\n0.542722\n2023\n\n\n2023_16\nHortaleza\n1 de enero de 2023\n28079\nMadrid\n16\n16\nHortaleza\n198391\n94100\n104291\n0.525684\n2023\n\n\n2022_8\nFuencarral-El Pardo\n1 de enero de 2022\n28079\nMadrid\n8\n8\nFuencarral-El Pardo\n246281\n115955\n130326\n0.529176\n2022\n\n\n2022_10\nLatina\n1 de enero de 2022\n28079\nMadrid\n10\n10\nLatina\n237048\n109928\n127120\n0.536263\n2022\n\n\n2022_11\nCarabanchel\n1 de enero de 2022\n28079\nMadrid\n11\n11\nCarabanchel\n255514\n119381\n136133\n0.532781\n2022\n\n\n2022_13\nPuente de Vallecas\n1 de enero de 2022\n28079\nMadrid\n13\n13\nPuente de Vallecas\n235638\n111748\n123890\n0.525764\n2022\n\n\n2022_15\nCiudad Lineal\n1 de enero de 2022\n28079\nMadrid\n15\n15\nCiudad Lineal\n213905\n97357\n116548\n0.544859\n2022\n\n\n2022_16\nHortaleza\n1 de enero de 2022\n28079\nMadrid\n16\n16\nHortaleza\n195017\n92532\n102485\n0.525518\n2022\n\n\n2021_8\nFuencarral-El Pardo\n1 de enero de 2021\n28079\nMadrid\n8\n8\nFuencarral-ElPardo\n247692\n116520\n131172\n0.529577\n2021\n\n\n2021_10\nLatina\n1 de enero de 2021\n28079\nMadrid\n10\n10\nLatina\n240155\n111209\n128946\n0.536928\n2021\n\n\n2021_11\nCarabanchel\n1 de enero de 2021\n28079\nMadrid\n11\n11\nCarabanchel\n258633\n120600\n138033\n0.533702\n2021\n\n\n2021_13\nPuente de Vallecas\n1 de enero de 2021\n28079\nMadrid\n13\n13\nPuentedeVallecas\n239057\n113355\n125702\n0.525824\n2021\n\n\n2021_15\nCiudad Lineal\n1 de enero de 2021\n28079\nMadrid\n15\n15\nCiudadLineal\n216818\n98514\n118304\n0.545637\n2021\n\n\n2021_16\nHortaleza\n1 de enero de 2021\n28079\nMadrid\n16\n16\nHortaleza\n193228\n91585\n101643\n0.526026\n2021\n\n\n2020_8\nFuencarral-El Pardo\n1 de enero de 2020\n28079\nMadrid\n8\n8\nFuencarral-ElPardo\n249973\n117640\n132333\n0.529389\n2020\n\n\n2020_10\nLatina\n1 de enero de 2020\n28079\nMadrid\n10\n10\nLatina\n242139\n112282\n129857\n0.536291\n2020\n\n\n2020_11\nCarabanchel\n1 de enero de 2020\n28079\nMadrid\n11\n11\nCarabanchel\n260196\n121317\n138879\n0.533748\n2020\n\n\n2020_13\nPuente de Vallecas\n1 de enero de 2020\n28079\nMadrid\n13\n13\nPuentedeVallecas\n240867\n114235\n126632\n0.525734\n2020\n\n\n2020_15\nCiudad Lineal\n1 de enero de 2020\n28079\nMadrid\n15\n15\nCiudadLineal\n219867\n99966\n119901\n0.545334\n2020\n\n\n2020_16\nHortaleza\n1 de enero de 2020\n28079\nMadrid\n16\n16\nHortaleza\n193264\n91659\n101605\n0.525732\n2020\n\n\n2019_8\nFuencarral-El Pardo\n1 de enero de 2019\n28079\nMadrid\n8\n8\nFuencarral-ElPardo\n246021\n115797\n130224\n0.529321\n2019\n\n\n2019_10\nLatina\n1 de enero de 2019\n28079\nMadrid\n10\n10\nLatina\n238154\n110401\n127753\n0.536430\n2019\n\n\n2019_11\nCarabanchel\n1 de enero de 2019\n28079\nMadrid\n11\n11\nCarabanchel\n253040\n117802\n135238\n0.534453\n2019\n\n\n2019_13\nPuente de Vallecas\n1 de enero de 2019\n28079\nMadrid\n13\n13\nPuentedeVallecas\n234770\n111183\n123587\n0.526417\n2019\n\n\n2019_15\nCiudad Lineal\n1 de enero de 2019\n28079\nMadrid\n15\n15\nCiudadLineal\n216270\n98370\n117900\n0.545152\n2019\n\n\n2018_8\nFuencarral-El Pardo\n1 de enero de 2018\n28079\nMadrid\n8\n8\nFuencarral-ElPardo\n242928\n114433\n128495\n0.528943\n2018\n\n\n2018_10\nLatina\n1 de enero de 2018\n28079\nMadrid\n10\n10\nLatina\n235785\n109392\n126393\n0.536052\n2018\n\n\n2018_11\nCarabanchel\n1 de enero de 2018\n28079\nMadrid\n11\n11\nCarabanchel\n248220\n115525\n132695\n0.534586\n2018\n\n\n2018_13\nPuente de Vallecas\n1 de enero de 2018\n28079\nMadrid\n13\n13\nPuentedeVallecas\n230488\n109044\n121444\n0.526899\n2018\n\n\n2018_15\nCiudad Lineal\n1 de enero de 2018\n28079\nMadrid\n15\n15\nCiudadLineal\n214463\n97301\n117162\n0.546304\n2018\n\n\n\n\n\n\n\n\n\n\nAmong the many operations DataFrame objects support, one of the most useful ones is to sort a table based on a given column. For example, imagine you want to sort the table by the ratio of women:\n\nmadrid_sorted = madrid_pop.sort_values(\"ratio_women\", ascending=False)\nmadrid_sorted\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2018_158\nCiudad Lineal\n1 de enero de 2018\n28079\nMadrid\n15\n158\nAtalaya\n1575\n656\n919\n0.583492\n2018\n\n\n2019_158\nCiudad Lineal\n1 de enero de 2019\n28079\nMadrid\n15\n158\nAtalaya\n1568\n654\n914\n0.582908\n2019\n\n\n2023_158\nCiudad Lineal\n1 de enero de 2023\n28079\nMadrid\n15\n158\nAtalaya\n1622\n691\n931\n0.573983\n2023\n\n\n2020_158\nCiudad Lineal\n1 de enero de 2020\n28079\nMadrid\n15\n158\nAtalaya\n1555\n667\n888\n0.571061\n2020\n\n\n2020_45\nSalamanca\n1 de enero de 2020\n28079\nMadrid\n4\n45\nLista\n21211\n9111\n12100\n0.570459\n2020\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2020_12\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n12\nEmbajadores\n47048\n24497\n22551\n0.479319\n2020\n\n\n2022_16\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n16\nSol\n8117\n4232\n3885\n0.478625\n2022\n\n\n2023_12\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n12\nEmbajadores\n46204\n24094\n22110\n0.478530\n2023\n\n\n2022_12\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n12\nEmbajadores\n46444\n24271\n22173\n0.477414\n2022\n\n\n2021_12\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n12\nEmbajadores\n47238\n24767\n22471\n0.475698\n2021\n\n\n\n\n912 rows × 12 columns\n\n\n\nGiven the rates differ, it may be better to sort by neighbourhood and then by year.\n\nsort_ls = [\"code_neighbourhood\", \"year\"]\nmadrid_sorted = madrid_pop.sort_values(sort_ls, ascending=True)\nmadrid_sorted\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2018_1\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n1\nCentro\n132352\n66320\n66032\n0.498912\n2018\n\n\n2019_1\nCentro\n1 de enero de 2019\n28079\nMadrid\n1\n1\nCentro\n134881\n67829\n67052\n0.497120\n2019\n\n\n2020_1\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n1\nCentro\n140473\n71127\n69346\n0.493661\n2020\n\n\n2021_1\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n1\nCentro\n141236\n71881\n69355\n0.491058\n2021\n\n\n2022_1\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n1\nCentro\n139682\n70986\n68696\n0.491803\n2022\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2019_97\nMoncloa-Aravaca\n1 de enero de 2019\n28079\nMadrid\n9\n97\nAravaca\n26823\n12619\n14204\n0.529546\n2019\n\n\n2020_97\nMoncloa-Aravaca\n1 de enero de 2020\n28079\nMadrid\n9\n97\nAravaca\n27503\n12899\n14604\n0.530997\n2020\n\n\n2021_97\nMoncloa-Aravaca\n1 de enero de 2021\n28079\nMadrid\n9\n97\nAravaca\n27568\n12896\n14672\n0.532211\n2021\n\n\n2022_97\nMoncloa-Aravaca\n1 de enero de 2022\n28079\nMadrid\n9\n97\nAravaca\n27323\n12779\n14544\n0.532299\n2022\n\n\n2023_97\nMoncloa-Aravaca\n1 de enero de 2023\n28079\nMadrid\n9\n97\nAravaca\n27445\n12836\n14609\n0.532301\n2023\n\n\n\n\n912 rows × 12 columns\n\n\n\nThis allows you to do so-called hierarchical sorting: sort first based on one column then based on another.",
    "crumbs": [
      "Basic Coding",
      "Pandas & Data"
    ]
  },
  {
    "objectID": "chapter_2/pandas.html#visual-exploration",
    "href": "chapter_2/pandas.html#visual-exploration",
    "title": "Pandas & Data",
    "section": "",
    "text": "The next step to continue exploring a dataset is to get a feel for what it looks like, visually. We have already learnt how to uncover and inspect specific parts of the data to check for particular cases we might be interested in. Now, we will see how to plot the data to get a sense of the overall distribution of values. For that, we can use the plotting capabilities of pandas.\n\n\nOne of the most common graphical devices to display the distribution of values in a variable is a histogram. Values are assigned into groups of equal intervals, and the groups are plotted as bars rising as high as the number of values into the group.\nA histogram is easily created with the following command. In this case, let’s have a look at the shape of the overall numbers of people:\n\nmadrid_pop[\"num_people\"].plot.hist(bins=15)\n\n\n\n\n\n\n\n\nHowever, the default pandas plots can be a bit dull. A better option is to use another package, called seaborn.\nseaborn is, by convention, imported as sns. Seaborn is a humorous reference to Samuel Normal Seaborn, a fictional character The West Wing show.\nThe same plot using seaborn has more agreeable default styles and more customisability.\n\nimport seaborn as sns\n\n# Set the style\nsns.set_style(\"darkgrid\")\nsns.histplot(madrid_pop[\"num_people\"], kde=True, bins=15)\n\n\n\n\n\n\n\n\nNote we are using sns instead of pd, as the function belongs to seaborn instead of pandas.\nWe can quickly see most of the areas have seen somewhere between 0 and 50K people; and very few have more than 200K. However, remember that in this case we are visualizing all years together, which could lead to misinterpretations.\n\n\n\nAnother very common way of visually displaying a variable is with a line or a bar chart. For example, if you want to generate a line plot of the (sorted) total population per year:\n\ntotal_people_per_year = madrid_pop.groupby(\"year\")[\"num_people\"].sum()\ntotal_people_per_year.plot()\n\n\n\n\n\n\n\n\nWhat is evident is the impact of COVID on the total population. But understanding that the data is reported on the 1st of January of each year is crucial to understand why you see the offset on the dates.\nFor a bar plot all you need to do is to change from plot to plot.bar:\n\ntotal_people_per_year.plot.bar()\n\n\n\n\n\n\n\n\nLet’s try to plot the ratio_women per neighbourhood, to see if we spot anything in particular.\n\nsns.lineplot(\n    x=\"year\",\n    y=\"ratio_women\",\n    hue=\"neighbourhood\",\n    data=madrid_pop.sort_values(\"year\", ascending=True),\n    legend=False,\n)\n\n\n\n\n\n\n\n\nWe can see some outliers, but the reality is that the data is hard to read so we probably would need some further analysis and visual considerations to efficiently communicate any possible trends.",
    "crumbs": [
      "Basic Coding",
      "Pandas & Data"
    ]
  },
  {
    "objectID": "chapter_2/pandas.html#tidy-data",
    "href": "chapter_2/pandas.html#tidy-data",
    "title": "Pandas & Data",
    "section": "",
    "text": "Clean vs. Tidy\nThis section is a bit more advanced and hence considered optional. Feel free to skip it and return later when you feel more confident.\n\nOnce you can read your data in, explore specific cases, and have a first visual approach to the entire set, the next step can be preparing it for more sophisticated analysis. Maybe you are thinking of modelling it through regression, or on creating subgroups in the dataset with particular characteristics, or maybe you simply need to present summary measures that relate to a slightly different arrangement of the data than you have been presented with.\nFor all these cases, you first need what statistician, and general R wizard, Hadley Wickham calls “tidy data”. The general idea to “tidy” your data is to convert them from whatever structure they were handed to you into one that allows convenient and standardized manipulation, and that supports directly inputting the data into what he calls “tidy” analysis tools. But, at a more practical level, what is exactly “tidy data”? In Wickham’s own words:\n\nTidy data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types.\n\nHe then goes on to list the three fundamental characteristics of “tidy data”:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\nIf you are further interested in the concept of “tidy data”, we recommend the original paper (open access) and the associated public repository.",
    "crumbs": [
      "Basic Coding",
      "Pandas & Data"
    ]
  },
  {
    "objectID": "chapter_2/pandas.html#grouping-transforming-aggregating",
    "href": "chapter_2/pandas.html#grouping-transforming-aggregating",
    "title": "Pandas & Data",
    "section": "",
    "text": "One of the advantage of tidy datasets is they allow advanced transformations in a more direct way. One of the most common ones is what is called “group-by” operations. These originated in the world of databases, and allow you to group observations from a data table by labels, index, or category, and to then apply operations on the data on a group by group basis.\nFor example, given our DataFrame, we might want to compute the total sum of the population by each district. This task can be split into two different steps:\n\nGroup the table in each of the different districts.\nCompute the sum of num_people for each of them.\n\nTo do this in pandas, meet one of its workhorses and also one of the reasons why the library has become so popular: the groupby operator.\n\nmad_grouped = madrid_pop.groupby(\"year\")\nmad_grouped\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x1442cc850&gt;\n\n\nThe object mad_grouped still hasn’t computed anything. It is only a convenient way of specifying the grouping. But this allows us then to perform a multitude of operations on it. For our example, the sum is calculated as follows:\n\nmad_grouped.sum(numeric_only=True)\n\n\n\n\n\n\n\n\ncode_municipality\nnum_people\nnum_people_men\nnum_people_women\nratio_women\n\n\nyear\n\n\n\n\n\n\n\n\n\n2018\n4268008\n6443648\n3000680\n3442968\n81.100391\n\n\n2019\n4268008\n6532252\n3042356\n3489896\n81.108315\n\n\n2020\n4268008\n6669460\n3109464\n3559996\n81.047788\n\n\n2021\n4268008\n6624620\n3090314\n3534306\n81.013452\n\n\n2022\n4268008\n6573324\n3069648\n3503676\n80.939052\n\n\n2023\n4268008\n6679862\n3119732\n3560130\n80.942860\n\n\n\n\n\n\n\nSimilarly, we can also obtain a summary of each group:\n\nmad_grouped.describe()\n\n\n\n\n\n\n\n\ncode_municipality\nnum_people\n...\nnum_people_women\nratio_women\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ncount\nmean\n...\n75%\nmax\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nyear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2018\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n42392.421053\n...\n21771.75\n132695.0\n152.0\n0.533555\n0.018390\n0.481481\n0.521591\n0.534767\n0.545880\n0.583492\n\n\n2019\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n42975.342105\n...\n21987.00\n135238.0\n152.0\n0.533607\n0.018198\n0.483197\n0.522356\n0.534355\n0.545650\n0.582908\n\n\n2020\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n43878.026316\n...\n22623.75\n138879.0\n152.0\n0.533209\n0.017944\n0.479319\n0.522586\n0.533905\n0.545599\n0.571061\n\n\n2021\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n43583.026316\n...\n22530.75\n138033.0\n152.0\n0.532983\n0.017984\n0.475698\n0.522263\n0.533724\n0.545682\n0.568987\n\n\n2022\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n43245.552632\n...\n22296.50\n136133.0\n152.0\n0.532494\n0.017716\n0.477414\n0.521691\n0.533305\n0.544920\n0.567140\n\n\n2023\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n43946.460526\n...\n22742.75\n139707.0\n152.0\n0.532519\n0.017486\n0.478530\n0.522637\n0.533363\n0.544488\n0.573983\n\n\n\n\n6 rows × 40 columns\n\n\n\nWe will not get into it today as it goes beyond the basics this chapter covers, but keep in mind that groupby allows us to not only call generic functions (like sum or describe), but also custom functions. This opens the door for virtually any kind of transformation and aggregation possible.\nAdditional reading\n\nA good introduction to data manipulation in Python is Wes McKinney’s Python for Data Analysis.\nTo further explore some of the visualization capabilities, the Python library seaborn is an excellent choice. Its online tutorial is a fantastic place to start.\nA good extension is Hadley Wickham’s “Tidy data” paper, which presents a very popular way of organising tabular data for efficient manipulation.",
    "crumbs": [
      "Basic Coding",
      "Pandas & Data"
    ]
  },
  {
    "objectID": "chapter_2/open_source.html",
    "href": "chapter_2/open_source.html",
    "title": "Open Source",
    "section": "",
    "text": "Use open-source code wherever possible! Open source code consists of a dynamic ecosystem of constantly co-evolving code packages. This collaborative development has permitted a remarkable progression in the capabilities and sophistication of freely accessible code. This has benefitted geospatial and urban analysis immensely, making these tools more widely accessible to those who wish to use them.\nTo further cement some basic Python skills, we’ll be making use of the open-source learn-python3 repository, created and maintained by Github user jerry-git.\n\n\n\n\n\n\nTip\n\n\n\nWe will be using open-source packages extensively throughout this course. To show your appreciation for online open-source code, please click the “Star” button in the respective Github repositories! If the repository provides a citation or publication reference, please be sure to cite these clearly in any derivative work. If you continue to develop your Python skills you may one day be able to become an active contributor to the ongoing development of open-source projects.\n\n\n\n\n\nGo to the learn-python3 repository.\nBefore using code from online code repositories, double-check the license type. In this case, it is MIT licensed which is an open-source license.\nDownload the repository to a local folder by clicking on the green “code” dropdown button, and select “Download ZIP”.\nUnzip if necessary and move the folder to a working location of your choosing.\n\n\n\n\n\nOpen your Anaconda Navigator and launch Jupyter Lab as previously.\nOnce Jupyter Lab has launched, navigate to the downloaded folder and open the notebooks &gt; beginner &gt; notebooks folder.\nSequentially work through notebooks from 1 to 7. Feel free to experiment with the notebook content.\nEmerging AI tools such as ChatGPT are incredibly useful for understanding and adapting code. These tools are largely replacing previously used online resources such as Stack Overflow (though these can still be useful).",
    "crumbs": [
      "Basic Coding",
      "Open Source"
    ]
  },
  {
    "objectID": "chapter_2/open_source.html#fetching-the-notebooks",
    "href": "chapter_2/open_source.html#fetching-the-notebooks",
    "title": "Open Source",
    "section": "",
    "text": "Go to the learn-python3 repository.\nBefore using code from online code repositories, double-check the license type. In this case, it is MIT licensed which is an open-source license.\nDownload the repository to a local folder by clicking on the green “code” dropdown button, and select “Download ZIP”.\nUnzip if necessary and move the folder to a working location of your choosing.",
    "crumbs": [
      "Basic Coding",
      "Open Source"
    ]
  },
  {
    "objectID": "chapter_2/open_source.html#using-the-notebooks",
    "href": "chapter_2/open_source.html#using-the-notebooks",
    "title": "Open Source",
    "section": "",
    "text": "Open your Anaconda Navigator and launch Jupyter Lab as previously.\nOnce Jupyter Lab has launched, navigate to the downloaded folder and open the notebooks &gt; beginner &gt; notebooks folder.\nSequentially work through notebooks from 1 to 7. Feel free to experiment with the notebook content.\nEmerging AI tools such as ChatGPT are incredibly useful for understanding and adapting code. These tools are largely replacing previously used online resources such as Stack Overflow (though these can still be useful).",
    "crumbs": [
      "Basic Coding",
      "Open Source"
    ]
  },
  {
    "objectID": "chapter_0/anaconda.html",
    "href": "chapter_0/anaconda.html",
    "title": "Python Notebooks",
    "section": "",
    "text": "Welcome to the first hands-on section of the course where you will familiarise yourself with Jupyter Lab for running Python ‘Notebooks’.\nThis guide assumes the prior installation of Anaconda as detailed previously.\n\n\nCreate a new “environment” the first time you use Anaconda. Environments ensure that packages remain self-contained and make it easier to synchronise packages without unnecessary contamination from unrelated projects.\n\nOpen Anaconda Navigator.\nClick on Environments\nClick to create a new environment called nfi.\nSelect the newly created nfi environment from the list of environments.\nIn the right pane, change the dropdown view mode to “Not Installed”.\nSearch for jupyterlab, then select it and click to install.\nSelect the Home button to return to the main view.\n\n\n\n\nTo launch Juptyer Lab:\n\nOpen Anaconda Navigator.\nUsing the top dropdown menus, select “All applications” and select the newly created “nfi” environment.\nClick to launch “Jupyter Lab” from the list of applications. This will launch a new Jupyter Lab instance in a web browser.\nUse the “Python 3” kernel if prompted.\n\n\n\n\n\n\n\nTip\n\n\n\nDon’t confuse the “Jupyer Lab” with the “Jupyter Notebook” apps. The former is a newer replacement for the latter.\n\n\n\n\n\nYou can now create a new Notebook or, otherwise, open an existing Python Notebook. Notebooks are composed of cells.\n\n\n\nJupyter Notebook cell\n\n\nCells can either consist of code or Markdown formatted text. A typical notebook consists of a series of cells; some would include text with explanations or background information, while others would contain the executable code. The code in the cells has to be “executed” by ppressing the play button. Once executed, the code will generate outputs showing the results of the computation.\nYou can try creating your own cell content and experimenting with running the cells. Remember that the cells are only aware of previously executed cells. We can start with simple math that Python can do natively. Run the following code cell in your notebook. After inserting the text, you can either click the “play” button or press the Shift + Enter keys.\n\n# this will display the number \"2\" once the cell is executed\n1 + 1\n\n2\n\n\nYou can also try using variables and printing:\n\n# this stores the calculation in \"my_variable\"\nmy_variable = 2 * 10 / 3\n# this prints the variable using \"f-string\" interpolation\nprint(f'The calculation result is: {my_variable}')\n# let's round the number to two decimal places\nprint(f'The rounded result is: {round(my_variable, 2)}')\n\nThe calculation result is: 6.666666666666667\nThe rounded result is: 6.67\n\n\n\n\n\nShut down Jupyter Lab from the menubar in the web interface using File &gt; Shut Down.\n\n\n\nShut down Jupyter Lab\n\n\n\n\n\nIf you are unable to install an environment using the instructions above, you can follow the course using Google Colab. You will need to install any required packages to your Colab environment. Reach out in class if you need help getting set up.\n\n\n\nIf you already have experience with Python, IDEs, and the command line, feel free to setup your development environment accordingly. The method for doing so will depend on your operating system and your preference for IDEs. We recommend VSCode as an IDE.",
    "crumbs": [
      "Setup",
      "Python Notebooks"
    ]
  },
  {
    "objectID": "chapter_0/anaconda.html#overview",
    "href": "chapter_0/anaconda.html#overview",
    "title": "Python Notebooks",
    "section": "",
    "text": "Welcome to the first hands-on section of the course where you will familiarise yourself with Jupyter Lab for running Python ‘Notebooks’.\nThis guide assumes the prior installation of Anaconda as detailed previously.\n\n\nCreate a new “environment” the first time you use Anaconda. Environments ensure that packages remain self-contained and make it easier to synchronise packages without unnecessary contamination from unrelated projects.\n\nOpen Anaconda Navigator.\nClick on Environments\nClick to create a new environment called nfi.\nSelect the newly created nfi environment from the list of environments.\nIn the right pane, change the dropdown view mode to “Not Installed”.\nSearch for jupyterlab, then select it and click to install.\nSelect the Home button to return to the main view.\n\n\n\n\nTo launch Juptyer Lab:\n\nOpen Anaconda Navigator.\nUsing the top dropdown menus, select “All applications” and select the newly created “nfi” environment.\nClick to launch “Jupyter Lab” from the list of applications. This will launch a new Jupyter Lab instance in a web browser.\nUse the “Python 3” kernel if prompted.\n\n\n\n\n\n\n\nTip\n\n\n\nDon’t confuse the “Jupyer Lab” with the “Jupyter Notebook” apps. The former is a newer replacement for the latter.\n\n\n\n\n\nYou can now create a new Notebook or, otherwise, open an existing Python Notebook. Notebooks are composed of cells.\n\n\n\nJupyter Notebook cell\n\n\nCells can either consist of code or Markdown formatted text. A typical notebook consists of a series of cells; some would include text with explanations or background information, while others would contain the executable code. The code in the cells has to be “executed” by ppressing the play button. Once executed, the code will generate outputs showing the results of the computation.\nYou can try creating your own cell content and experimenting with running the cells. Remember that the cells are only aware of previously executed cells. We can start with simple math that Python can do natively. Run the following code cell in your notebook. After inserting the text, you can either click the “play” button or press the Shift + Enter keys.\n\n# this will display the number \"2\" once the cell is executed\n1 + 1\n\n2\n\n\nYou can also try using variables and printing:\n\n# this stores the calculation in \"my_variable\"\nmy_variable = 2 * 10 / 3\n# this prints the variable using \"f-string\" interpolation\nprint(f'The calculation result is: {my_variable}')\n# let's round the number to two decimal places\nprint(f'The rounded result is: {round(my_variable, 2)}')\n\nThe calculation result is: 6.666666666666667\nThe rounded result is: 6.67\n\n\n\n\n\nShut down Jupyter Lab from the menubar in the web interface using File &gt; Shut Down.\n\n\n\nShut down Jupyter Lab\n\n\n\n\n\nIf you are unable to install an environment using the instructions above, you can follow the course using Google Colab. You will need to install any required packages to your Colab environment. Reach out in class if you need help getting set up.\n\n\n\nIf you already have experience with Python, IDEs, and the command line, feel free to setup your development environment accordingly. The method for doing so will depend on your operating system and your preference for IDEs. We recommend VSCode as an IDE.",
    "crumbs": [
      "Setup",
      "Python Notebooks"
    ]
  },
  {
    "objectID": "chapter_1/sql.html",
    "href": "chapter_1/sql.html",
    "title": "SQL for GIS",
    "section": "",
    "text": "This is a draft outline and will be fleshed-out closer to class. To make visible, edit _quarto.yml to include in directory.\n\n\nSQL and PostgreSQL Basics:\n\n\nUnderstanding SQL (Structured Query Language)\nIntroduction to PostgreSQL as a Relational Database Management System (RDBMS)\nWorking with Schemas in PostgreSQL\n\n\nData Manipulation with SQL and PostgreSQL:\n\n\nCREATE, SELECT, INSERT, DELETE Statements\nPrimary Keys (PK) and Foreign Keys (FK)\nCreating and Managing Indexes for Performance Optimization\n\n\nSQL Concepts:\n\n\nAggregations: COUNT, SUM, AVG, MIN, MAX\nComplex Queries and Subqueries WITH, WHERE\nConditional Statements: CASE Statements\n\n\nIntroduction to GIS Integration with PostgreSQL:\n\n\nSpatial Data Types in PostgreSQL\nSpatial Indexing for Efficient Spatial Queries\nSpatial Functions and Operators in PostgreSQL\n\n\nSpatial and Non-Spatial Joins:\n\n\nPerforming Spatial Joins for GIS Analysis\nNon-Spatial Joins for Relational Data Analysis\nCombining Spatial and Non-Spatial Data in Queries\n\n\nHow to ask ChatGPT\n\n\nIterating query development\nDebugging"
  },
  {
    "objectID": "chapter_1/sql.html#key-topics",
    "href": "chapter_1/sql.html#key-topics",
    "title": "SQL for GIS",
    "section": "Key Topics",
    "text": "Key Topics\n\nIntroduction to Data Analysis and Databases:\n\n\nImportance of Data Analysis in Decision Making\nOverview of Relational Databases\n\n\nConnecting to Google Cloud Platform (GCP):\n\n\nConnecting to NFI’s database(s)\nSetting Up a GCP Account\n\n\nSQL and PostgreSQL Basics:\n\n\nUnderstanding SQL (Structured Query Language)\nIntroduction to PostgreSQL as a Relational Database Management System (RDBMS)\nWorking with Schemas in PostgreSQL\n\n\nData Manipulation with SQL and PostgreSQL:\n\n\nCREATE, SELECT, INSERT, DELETE Statements\nPrimary Keys (PK) and Foreign Keys (FK)\nCreating and Managing Indexes for Performance Optimization\n\n\nSQL Concepts:\n\n\nAggregations: COUNT, SUM, AVG, MIN, MAX\nComplex Queries and Subqueries WITH, WHERE\nConditional Statements: CASE Statements\n\n\nIntroduction to GIS Integration with PostgreSQL:\n\n\nSpatial Data Types in PostgreSQL\nSpatial Indexing for Efficient Spatial Queries\nSpatial Functions and Operators in PostgreSQL\n\n\nSpatial and Non-Spatial Joins:\n\n\nPerforming Spatial Joins for GIS Analysis\nNon-Spatial Joins for Relational Data Analysis\nCombining Spatial and Non-Spatial Data in Queries"
  },
  {
    "objectID": "chapter_1/sql.html#software-needed",
    "href": "chapter_1/sql.html#software-needed",
    "title": "SQL for GIS",
    "section": "Software needed",
    "text": "Software needed\n\nFor this session please make sure you have downloaded the following software(s):\n\nTablePlus | Modern, Native Tool for Database Management."
  },
  {
    "objectID": "chapter_1/sql.html#connecting-to-data-on-postgis",
    "href": "chapter_1/sql.html#connecting-to-data-on-postgis",
    "title": "SQL for GIS",
    "section": "Connecting to data on PostGIS 👨🏽‍💻",
    "text": "Connecting to data on PostGIS 👨🏽‍💻\n\n\n\n⚙\nCONNECTION SETTINGS\n\n\n\n\nIP Address\n(will be provided during class)\n\n\nDatabase names\nnfi / urban_analytics\n\n\nPort\n5432\n\n\nUsername\nscholar\n\n\nPassword\n(will be provided during class)\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote that the connections to the Databases are restricted by the source IP address."
  },
  {
    "objectID": "chapter_1/sql.html#to-dos",
    "href": "chapter_1/sql.html#to-dos",
    "title": "SQL for GIS",
    "section": "To-Dos",
    "text": "To-Dos\n\nCreate a table with the following characteristics.\nUse schema. ua0\nTable name : [yourinitials] + “_prof_exp”\nFields required: (assign data types as you think would be best)\n\nid\nscholar_id\nindustry_id\ninstitution\nyears_of_experience\neducation\nlocation (crs: 4326)\nend_year\nnotes\n\n** If you think more fields are necessary, feel free to add them\nCreate a GitHub account\n\nadd link to public.scholars.github_link (varchar)\n\n\n\nOptional\nCreate a local instance of PostGIS for use in the upcoming months\nLearn PostgreSQL Tutorial - Full Course for Beginners"
  },
  {
    "objectID": "chapter_1/sql.html#useful-resources",
    "href": "chapter_1/sql.html#useful-resources",
    "title": "SQL for GIS",
    "section": "Useful Resources",
    "text": "Useful Resources\n\nPOSTGRESQL & POSTGIS CHEATSHEETS\n\nPostgreSQL & PostGIS cheatsheet (a work in progress)\nPostgres Cheatsheet"
  },
  {
    "objectID": "chapter_1/gis.html",
    "href": "chapter_1/gis.html",
    "title": "Intro to GIS",
    "section": "",
    "text": "Vector and raster data are two fundamental types of spatial data used in Geographic Information Systems (GIS). Vector data represents geographical features as distinct shapes using points, lines, and polygons, each of which can carry detailed attribute information, making it ideal for precise mapping applications like boundaries, roads, and infrastructure networks. It excels in accuracy and detail for discrete data. In contrast, raster data consists of a grid of pixels, each holding a value, effectively representing continuous data. This format is well-suited for environmental analysis, land cover studies, and satellite imagery, as it captures variations over a wide area, such as elevation, temperature, or vegetation. While vector data is preferred for its precision in depicting specific geographic entities and managing related data, raster data is favored for its ability to represent complex, continuous phenomena over large spatial scales.\n\n\n\n\nVector data types are primarily categorized into Points, Linestrings, and Polygons, each with distinct characteristics and uses. Additionally, there are multi-geometry versions of these types, such as MultiPoints, MultiLinestrings, and MultiPolygons, which offer more complex feature groupings.\n\nPoints: Points are used to represent discrete locations on the earth’s surface. Each point is defined by a pair of coordinates (e.g. latitude and longitude, or an easting and northing) and can represent features such as the location of a tree or a landmark. They can have attributes further describing characteristics of interest, such as tree species or landmark name.\nLinestring: Lines connect a series of points to describe linear features such as roads or rivers. Linestring geometries inherently have length and may include attributes such as road type or river name.\nPolygons: Polygons are closed shapes used to represent areas such as city boundaries or land parcels. Polygon geometries inherently have an area and perimeter and may have attributes such as population counts or land use types.\n\n\n\n\nGeometry types\n\n\n\nMulti-Geometry Versions: Multi geometries provide a way to associated a grouping of Features which can share the same feature attributes.\n\nMultiPoint: For example, a grouping of trees.\nMultiLinestring: For example, a grouping of related segments describing a long-distance hiking trail.\nMultiPolygon: For example, a grouping of buildings which together represent a larger entity such as a hospital or university.\n\n\n\n\n\nEsri Shapefile\nShapefile is a prevalent geospatial file type in GIS software, requiring three mandatory files: - SHP (feature geometry) - SHX (shape index position) - DBF (attribute data) - Shapefiles often also include optional files such as PRJ (coordinate reference system).\nGeoPackage\nGeoPackage provides a modern and simpler alternative to shapefiles and is rapidly gaining acceptance. It supports vector data and can store multiple data layers and it’s use of SQLite means that you can use SQL queries.\nGeoJSON\nGeoJSON encodes geographic data structures using JSON, a format commonly used for web development.\n\nReference: https://gisgeography.com/gis-formats/\n\n\n\n\n\nRasters contain a grid of pixels, where the value used for each grid cell can represent a characteristic such as temperature or landcover. Geographic rasters typically include Geospatial information necessary to correctly project and locate the information in GIS software.\n\n\n\nRaster image grid cells\n\n\n\n\n\nASCII Grid\nASCII Grid files (ASC) store raster data as text in a grid format. Each cell in the grid is represented by a numeric value corresponding to the geographic attribute being mapped, such as elevation. ASCII Grid files include a header specifying metadata like cell size, number of rows and columns, and coordinates of the lower left corner. The format is simple and can be generated or edited with a text editor, and may use space, comma, or tab-delimiters.\nGeoTIFF\nGeoTIFF is widely becoming the defacto raster format in GIS. It embeds coordinate reference system information within a TIFF (Tagged Image File Format). GeoTIFFs are commonly used for storing satellite imagery, elevation data, and land surface temperatures.\n\nReference: https://gisgeography.com/gis-formats/\n\n\n\n\n\n\n\nGeographic data is intrinsically tied to the earth’s surface and is represented using coordinates within defined Coordinate Reference Systems (CRS). Broadly, these systems fall into two categories: Geographic Coordinate Systems and Projected Coordinate Systems.\nGeographic CRSs use a spherical surface to define locations on the earth’s surface. They are based on degrees in latitude and longitude and are most commonly referenced to the WGS84 datum. While they accurately represent the earth’s curved surface and preserve the true shape and angular relationships of features, distances and areas may not be as accurate, especially over larger distances.\nProjected CRSs translate the earth’s three-dimensional surface onto a two-dimensional plane, making it easier to analyze and visualize spatial data. They use a mathematical projection to flatten the curved surface of the earth, which can lead to distortions in shape, area, distance, or direction. Different types of projections are designed to minimize these distortions for specific regions or purposes. For example, projections can be optimized for preserving areas (equal-area projections), angles and shapes (conformal), or distances (equidistant).\n\n\n\nDimaxion Fuller projection. NFF image rights.\n\n\n\n\n\n\n\n\nTip\n\n\n\nA useful reference website for different CRS is EPSG.io: Coordinate Systems Worldwide\n\n\n\n\nGeographic Coordinate Reference Systems (CRS) use a spherical or ellipsoidal surface to represent locations on the Earth’s curved surface. Geographic CRSs are based on latitude and longitude coordinates and are useful for global or regional mapping.\n\nUnits: Degrees\nCommon Examples:\n\nWGS 84 (World Geodetic System 1984) EPSG:4326: A widely used CRS for global mapping and GPS navigation.\nNAD83 (North American Datum 1983) EPSG:4269: A CRS commonly used in North America.\n\n\n\n\n\nProjected Coordinate Reference Systems (CRS) use a Cartesian coordinate system to represent locations on a flat surface. These CRSs are created by “projecting” the curved Earth onto a flat surface, which introduces distortions. This inevitably requires tradeoffs in distances, areas, or shapes, with different projection systems involving different trade-offs. Projected CRSs are commonly used for local or small-scale mapping appropriate to specific countries or regions of the globe, where locally accurate measurements and comparisons are required.\n\nUnits: Meters (most commonly)\nCommon Examples:\n\nLAEA (Lambert Azimuthal Equal Area) Europe projection EPSG:3035: This CRS is commonly used for Europe, though note that countries will typically also have even more localised projected CRS.\nBritish National Grid (BNG) EPSG:27700: This CRS is commonly used in the United Kingdom.\nState Plane Coordinate System (SPCS): This CRS is widely used in the United States, and divides the country into zones, each with their own projection.\n\n\n\n\n\n\n\nhttps://gisgeography.com\n\n\n\n\n\n\n\n\nDiscovering sources for GIS (Geographic Information System) data is a handy skill for spatial analysis and mapping! There are various official and non-official sources, each offering different types of data suited to specific needs and applications. When searching for and using different data sources, try to remain cognisant of the merits and limitations of different sources, which may impact considerations such as data accuracy or bias.\n\nGovernment Agencies: National agencies often provide official and reliable forms of GIS data. However, these datasets may reflect governmental priorities and perspectives or they may be reluctant to make these datasets openly available.\nInternational Organizations: Organizations such as the United Nations (UN) and the World Bank offer global GIS datasets, but these may have limited local detail due to their broad scope.\nLocal Authorities: City and regional councils often create and maintain local GIS datasets, often tailored to specific local government projects and priorities. Their local specificity may make it tricky to compare to different datasets sourced from other cities.\nPrivate Sector: There are many private sector organisations which generate, curate, or distribute data, however, these datasets are less likely to be openly available and may incur substantial monetary cost.\nOpen Data Platforms: Platforms such as OpenStreetMap offer freely accessible GIS data, contributed by a community of users. These datasets may require additional validation if used for mission-critical purposes, though can otherwise be extremely useful, allowing for generalisable workflows and providing coverage in some instances where no other options exist.\nAPIs and Web Scraping: Involves the automated collection of data from online APIs or websites. The quality of data can vary but may otherwise be useful for obtaining interesting data. These techniques may involve significant technical skills for data extraction and processing. Legal and ethical considerations are important, as scraping can sometimes violate terms of service or data privacy laws.\n\n\n\n\nPurposely collected data is gathered specifically to address a certain research question or project goal, using structured approaches such as surveys, experiments, or observations. It’s tailored to meet the specific needs of a study, with careful control over variables to ensure the data’s relevance and accuracy. This type of data is highly pertinent to the project it’s collected for, due to its focused nature and methodical collection process.\n\n\n\nOn the other hand, data can also be adapted to new research needs from information originally collected for different purposes. This may include repurposing data from various sources such as public databases, existing research studies, or web scraping.\n\n\n\n\n\nAttribution or citation refers to giving proper credit to the original source or creator of the data. Just as with citing sources in academic papers, it’s important to acknowledge the origin of the data you are using. This helps provide transparency, accountability, and recognition to the creators of the dataset.\n\n\nWhen dealing with data, licenses refer to legal agreements or permissions that determine how the data can be used, distributed, or modified. Different datasets may have different licensing requirements, so it’s important to understand and comply with the specific license terms associated with the data you are using.\nExamples:\n\nCreative Commons (CC) licenses: These are standardized licenses that allow creators to specify permissions for their work. They are commonly used for a variety of creative works, including images, music, videos, and written content.\nOpen Government Licenses: These licenses have been adopted by governments and may allow individuals and organizations permissive use, reuse, and sharing of information.\nGNU General Public License (GPL): This is a copyleft license for open-source software. It permits users to modify and distribute the software, but any modifications or derivative works must also be licensed under the GPL. This license is popular for promoting open-source principles and collaborative software development.\nMIT / Apache / BSD Licenses: Permissive open-source software licenses. These may allow great flexibility such as using, copying, modifying, and distributing derivative work, even for commercial purposes. The primary requirement is to include the original copyright and license notice in any significant portions of the software.\n\n\n\n\n\n\nPrivacy considerations are important when working with data, especially if the data contains personally identifiable information or sensitive information. It’s very important to handle and store data in a way that respects privacy laws and regulations. This may involve anonymizing or de-identifying data, implementing security measures, and obtaining consent from individuals whose data is being used.\n\n\n\n\n\n\nNote\n\n\n\nPlease avoid using any and all data containing private or sensitive data unless you first review this and seek approval from faculty. Handling these forms of data comes with numerous steps and safeguards and will generally be strongly discouraged.\n\n\n\n\n\n\n\n\nThe Modifiable Areal Unit Problem (MAUP) is encountered when using different scales or zones of analysis to group data. This can cause significant changes in the balance and aggregation of data purely based on how it is grouped. This can have a potentially significant impact on visualisation or analysis. MAUP is exploited for political purposes through the process of gerrymandering.\n\nhttps://gisgeography.com\n\n\n\nYouTube\n\n\n\n\n\n\nThe following resources provide further information on common types of map visualisations and best practices:\n\nCARTO - 5 Popular Thematic map Types\nFundamentals of Data Visualisation\n\nColor scales\nVisualizing geospatial data\nCommon pitfalls of color use\n\nColor Brewer",
    "crumbs": [
      "GIS Tools",
      "Intro to GIS"
    ]
  },
  {
    "objectID": "chapter_1/gis.html#file-types",
    "href": "chapter_1/gis.html#file-types",
    "title": "Intro to GIS",
    "section": "",
    "text": "Vector and raster data are two fundamental types of spatial data used in Geographic Information Systems (GIS). Vector data represents geographical features as distinct shapes using points, lines, and polygons, each of which can carry detailed attribute information, making it ideal for precise mapping applications like boundaries, roads, and infrastructure networks. It excels in accuracy and detail for discrete data. In contrast, raster data consists of a grid of pixels, each holding a value, effectively representing continuous data. This format is well-suited for environmental analysis, land cover studies, and satellite imagery, as it captures variations over a wide area, such as elevation, temperature, or vegetation. While vector data is preferred for its precision in depicting specific geographic entities and managing related data, raster data is favored for its ability to represent complex, continuous phenomena over large spatial scales.\n\n\n\n\nVector data types are primarily categorized into Points, Linestrings, and Polygons, each with distinct characteristics and uses. Additionally, there are multi-geometry versions of these types, such as MultiPoints, MultiLinestrings, and MultiPolygons, which offer more complex feature groupings.\n\nPoints: Points are used to represent discrete locations on the earth’s surface. Each point is defined by a pair of coordinates (e.g. latitude and longitude, or an easting and northing) and can represent features such as the location of a tree or a landmark. They can have attributes further describing characteristics of interest, such as tree species or landmark name.\nLinestring: Lines connect a series of points to describe linear features such as roads or rivers. Linestring geometries inherently have length and may include attributes such as road type or river name.\nPolygons: Polygons are closed shapes used to represent areas such as city boundaries or land parcels. Polygon geometries inherently have an area and perimeter and may have attributes such as population counts or land use types.\n\n\n\n\nGeometry types\n\n\n\nMulti-Geometry Versions: Multi geometries provide a way to associated a grouping of Features which can share the same feature attributes.\n\nMultiPoint: For example, a grouping of trees.\nMultiLinestring: For example, a grouping of related segments describing a long-distance hiking trail.\nMultiPolygon: For example, a grouping of buildings which together represent a larger entity such as a hospital or university.\n\n\n\n\n\nEsri Shapefile\nShapefile is a prevalent geospatial file type in GIS software, requiring three mandatory files: - SHP (feature geometry) - SHX (shape index position) - DBF (attribute data) - Shapefiles often also include optional files such as PRJ (coordinate reference system).\nGeoPackage\nGeoPackage provides a modern and simpler alternative to shapefiles and is rapidly gaining acceptance. It supports vector data and can store multiple data layers and it’s use of SQLite means that you can use SQL queries.\nGeoJSON\nGeoJSON encodes geographic data structures using JSON, a format commonly used for web development.\n\nReference: https://gisgeography.com/gis-formats/\n\n\n\n\n\nRasters contain a grid of pixels, where the value used for each grid cell can represent a characteristic such as temperature or landcover. Geographic rasters typically include Geospatial information necessary to correctly project and locate the information in GIS software.\n\n\n\nRaster image grid cells\n\n\n\n\n\nASCII Grid\nASCII Grid files (ASC) store raster data as text in a grid format. Each cell in the grid is represented by a numeric value corresponding to the geographic attribute being mapped, such as elevation. ASCII Grid files include a header specifying metadata like cell size, number of rows and columns, and coordinates of the lower left corner. The format is simple and can be generated or edited with a text editor, and may use space, comma, or tab-delimiters.\nGeoTIFF\nGeoTIFF is widely becoming the defacto raster format in GIS. It embeds coordinate reference system information within a TIFF (Tagged Image File Format). GeoTIFFs are commonly used for storing satellite imagery, elevation data, and land surface temperatures.\n\nReference: https://gisgeography.com/gis-formats/",
    "crumbs": [
      "GIS Tools",
      "Intro to GIS"
    ]
  },
  {
    "objectID": "chapter_1/gis.html#coordinate-reference-systems",
    "href": "chapter_1/gis.html#coordinate-reference-systems",
    "title": "Intro to GIS",
    "section": "",
    "text": "Geographic data is intrinsically tied to the earth’s surface and is represented using coordinates within defined Coordinate Reference Systems (CRS). Broadly, these systems fall into two categories: Geographic Coordinate Systems and Projected Coordinate Systems.\nGeographic CRSs use a spherical surface to define locations on the earth’s surface. They are based on degrees in latitude and longitude and are most commonly referenced to the WGS84 datum. While they accurately represent the earth’s curved surface and preserve the true shape and angular relationships of features, distances and areas may not be as accurate, especially over larger distances.\nProjected CRSs translate the earth’s three-dimensional surface onto a two-dimensional plane, making it easier to analyze and visualize spatial data. They use a mathematical projection to flatten the curved surface of the earth, which can lead to distortions in shape, area, distance, or direction. Different types of projections are designed to minimize these distortions for specific regions or purposes. For example, projections can be optimized for preserving areas (equal-area projections), angles and shapes (conformal), or distances (equidistant).\n\n\n\nDimaxion Fuller projection. NFF image rights.\n\n\n\n\n\n\n\n\nTip\n\n\n\nA useful reference website for different CRS is EPSG.io: Coordinate Systems Worldwide\n\n\n\n\nGeographic Coordinate Reference Systems (CRS) use a spherical or ellipsoidal surface to represent locations on the Earth’s curved surface. Geographic CRSs are based on latitude and longitude coordinates and are useful for global or regional mapping.\n\nUnits: Degrees\nCommon Examples:\n\nWGS 84 (World Geodetic System 1984) EPSG:4326: A widely used CRS for global mapping and GPS navigation.\nNAD83 (North American Datum 1983) EPSG:4269: A CRS commonly used in North America.\n\n\n\n\n\nProjected Coordinate Reference Systems (CRS) use a Cartesian coordinate system to represent locations on a flat surface. These CRSs are created by “projecting” the curved Earth onto a flat surface, which introduces distortions. This inevitably requires tradeoffs in distances, areas, or shapes, with different projection systems involving different trade-offs. Projected CRSs are commonly used for local or small-scale mapping appropriate to specific countries or regions of the globe, where locally accurate measurements and comparisons are required.\n\nUnits: Meters (most commonly)\nCommon Examples:\n\nLAEA (Lambert Azimuthal Equal Area) Europe projection EPSG:3035: This CRS is commonly used for Europe, though note that countries will typically also have even more localised projected CRS.\nBritish National Grid (BNG) EPSG:27700: This CRS is commonly used in the United Kingdom.\nState Plane Coordinate System (SPCS): This CRS is widely used in the United States, and divides the country into zones, each with their own projection.\n\n\n\n\n\n\n\nhttps://gisgeography.com",
    "crumbs": [
      "GIS Tools",
      "Intro to GIS"
    ]
  },
  {
    "objectID": "chapter_1/gis.html#gis-data",
    "href": "chapter_1/gis.html#gis-data",
    "title": "Intro to GIS",
    "section": "",
    "text": "Discovering sources for GIS (Geographic Information System) data is a handy skill for spatial analysis and mapping! There are various official and non-official sources, each offering different types of data suited to specific needs and applications. When searching for and using different data sources, try to remain cognisant of the merits and limitations of different sources, which may impact considerations such as data accuracy or bias.\n\nGovernment Agencies: National agencies often provide official and reliable forms of GIS data. However, these datasets may reflect governmental priorities and perspectives or they may be reluctant to make these datasets openly available.\nInternational Organizations: Organizations such as the United Nations (UN) and the World Bank offer global GIS datasets, but these may have limited local detail due to their broad scope.\nLocal Authorities: City and regional councils often create and maintain local GIS datasets, often tailored to specific local government projects and priorities. Their local specificity may make it tricky to compare to different datasets sourced from other cities.\nPrivate Sector: There are many private sector organisations which generate, curate, or distribute data, however, these datasets are less likely to be openly available and may incur substantial monetary cost.\nOpen Data Platforms: Platforms such as OpenStreetMap offer freely accessible GIS data, contributed by a community of users. These datasets may require additional validation if used for mission-critical purposes, though can otherwise be extremely useful, allowing for generalisable workflows and providing coverage in some instances where no other options exist.\nAPIs and Web Scraping: Involves the automated collection of data from online APIs or websites. The quality of data can vary but may otherwise be useful for obtaining interesting data. These techniques may involve significant technical skills for data extraction and processing. Legal and ethical considerations are important, as scraping can sometimes violate terms of service or data privacy laws.\n\n\n\n\nPurposely collected data is gathered specifically to address a certain research question or project goal, using structured approaches such as surveys, experiments, or observations. It’s tailored to meet the specific needs of a study, with careful control over variables to ensure the data’s relevance and accuracy. This type of data is highly pertinent to the project it’s collected for, due to its focused nature and methodical collection process.\n\n\n\nOn the other hand, data can also be adapted to new research needs from information originally collected for different purposes. This may include repurposing data from various sources such as public databases, existing research studies, or web scraping.",
    "crumbs": [
      "GIS Tools",
      "Intro to GIS"
    ]
  },
  {
    "objectID": "chapter_1/gis.html#attribution",
    "href": "chapter_1/gis.html#attribution",
    "title": "Intro to GIS",
    "section": "",
    "text": "Attribution or citation refers to giving proper credit to the original source or creator of the data. Just as with citing sources in academic papers, it’s important to acknowledge the origin of the data you are using. This helps provide transparency, accountability, and recognition to the creators of the dataset.\n\n\nWhen dealing with data, licenses refer to legal agreements or permissions that determine how the data can be used, distributed, or modified. Different datasets may have different licensing requirements, so it’s important to understand and comply with the specific license terms associated with the data you are using.\nExamples:\n\nCreative Commons (CC) licenses: These are standardized licenses that allow creators to specify permissions for their work. They are commonly used for a variety of creative works, including images, music, videos, and written content.\nOpen Government Licenses: These licenses have been adopted by governments and may allow individuals and organizations permissive use, reuse, and sharing of information.\nGNU General Public License (GPL): This is a copyleft license for open-source software. It permits users to modify and distribute the software, but any modifications or derivative works must also be licensed under the GPL. This license is popular for promoting open-source principles and collaborative software development.\nMIT / Apache / BSD Licenses: Permissive open-source software licenses. These may allow great flexibility such as using, copying, modifying, and distributing derivative work, even for commercial purposes. The primary requirement is to include the original copyright and license notice in any significant portions of the software.",
    "crumbs": [
      "GIS Tools",
      "Intro to GIS"
    ]
  },
  {
    "objectID": "chapter_1/gis.html#privacy",
    "href": "chapter_1/gis.html#privacy",
    "title": "Intro to GIS",
    "section": "",
    "text": "Privacy considerations are important when working with data, especially if the data contains personally identifiable information or sensitive information. It’s very important to handle and store data in a way that respects privacy laws and regulations. This may involve anonymizing or de-identifying data, implementing security measures, and obtaining consent from individuals whose data is being used.\n\n\n\n\n\n\nNote\n\n\n\nPlease avoid using any and all data containing private or sensitive data unless you first review this and seek approval from faculty. Handling these forms of data comes with numerous steps and safeguards and will generally be strongly discouraged.",
    "crumbs": [
      "GIS Tools",
      "Intro to GIS"
    ]
  },
  {
    "objectID": "chapter_1/gis.html#quirks-part-1",
    "href": "chapter_1/gis.html#quirks-part-1",
    "title": "Intro to GIS",
    "section": "",
    "text": "The Modifiable Areal Unit Problem (MAUP) is encountered when using different scales or zones of analysis to group data. This can cause significant changes in the balance and aggregation of data purely based on how it is grouped. This can have a potentially significant impact on visualisation or analysis. MAUP is exploited for political purposes through the process of gerrymandering.\n\nhttps://gisgeography.com\n\n\n\nYouTube",
    "crumbs": [
      "GIS Tools",
      "Intro to GIS"
    ]
  },
  {
    "objectID": "chapter_1/gis.html#resources",
    "href": "chapter_1/gis.html#resources",
    "title": "Intro to GIS",
    "section": "",
    "text": "The following resources provide further information on common types of map visualisations and best practices:\n\nCARTO - 5 Popular Thematic map Types\nFundamentals of Data Visualisation\n\nColor scales\nVisualizing geospatial data\nCommon pitfalls of color use\n\nColor Brewer",
    "crumbs": [
      "GIS Tools",
      "Intro to GIS"
    ]
  }
]