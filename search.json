[
  {
    "objectID": "hello.html",
    "href": "hello.html",
    "title": "Urban Analytics 2024",
    "section": "",
    "text": "Chapter 0: Infrastructure\nChapter 1: Notebooks\nChapter 2: Pandas"
  },
  {
    "objectID": "hello.html#toc",
    "href": "hello.html#toc",
    "title": "Urban Analytics 2024",
    "section": "",
    "text": "Chapter 0: Infrastructure\nChapter 1: Notebooks\nChapter 2: Pandas"
  },
  {
    "objectID": "chapter_0/infrastructure.html",
    "href": "chapter_0/infrastructure.html",
    "title": "Infrastructure",
    "section": "",
    "text": "The course does not use any specific resources. It is designed to run entirely on students’ machines using solely open-source software."
  },
  {
    "objectID": "chapter_0/infrastructure.html#communication",
    "href": "chapter_0/infrastructure.html#communication",
    "title": "Infrastructure",
    "section": "Communication",
    "text": "Communication\nScholars enrolled in the Sustainable Cities program at the Norman Foster Institute have been added to a Google Spaces for online discussion. Scholars are encouraged to use the platform to seek help from peers, share thoughts and tips, and other meaningful contributions the rest of the group can benefit from. Learn from the mistakes of the others.\nThe tutor will be part of the discussion forum but the priority is to help each other, rather than seek guidance outside of class."
  },
  {
    "objectID": "chapter_0/infrastructure.html#software-stack",
    "href": "chapter_0/infrastructure.html#software-stack",
    "title": "Infrastructure",
    "section": "Software stack",
    "text": "Software stack\nThere are many options on how to run Python. You may already have a copy on your machine, you can run it in the browser and install it in a myriad ways. This course uses what is considered a standard for scientific computing based on two key components - a package manager (you will be using mamba, built on top conda) and an interface (Jupyter Lab).\nThe basics will be explained in the first lesson but it would be beneficial if you could create an environment before the class.\n\nCreate an environment\nTo run the course material, you will need a Python environment with all the packages for spatial data science installed. Unless you already know how to work with conda or mamba, follow the instructions below.\n\nWindowsmacOS & Linux\n\n\n\n\n\n\n\n\nCaution\n\n\n\nWindows may complain that the app is not recognised. Click More information and you will be able to run the installer.\n\n\n\nDownload miniforge package manager from Github for your operating system.\nExecute the installer and make sure to create start menu shortcuts (there’s a tick box during installation).\nOpen the installed Miniforge Prompt application.\nCreate a Python environment using the following command:\n\nmamba env create -f https://github.com/Norman-Foster-Institute/ua_2024/blob/main/environment.yml\n\nActivate the environment using:\n\nmamba activate nfiua\n\nStart JupyterLab interface:\n\njupyter lab\n\n\n\n\n\n\nImportant\n\n\n\nEnsure that you install miniforge in a directory without any special characters in the name. It may occasionally break things.\n\n\n\n\n\nDownload miniforge package manager from Github for your operating system and install it.\nOpen Terminal application.\nCreate a Python environment using the following command:\n\nmamba env create -f https://github.com/Norman-Foster-Institute/ua_2024/blob/main/environment.yml\n\nActivate the environment using:\n\nmamba activate nfiua\n\nStart JupyterLab interface:\n\njupyter lab\n\n\n\nA Jupyter Lab interface will show up in your browser. If it hasn’t opened automatically, copy the link printed in your command line/terminal.\n\n\nClosing Jupyter Lab\nThe best way to close the Jupyter Lab and shut down its process running in the terminal is to use the Jupyter Lab’s interface. In the menu find File &gt; Shut Down.\n\n\n\nShut down Jupyter Lab\n\n\n\n\nOpening Jupyter Lab next time\nThe steps above are needed only the first time. Once you create the environment, it won’t disappear and you can use it until you don’t delete it. Once you close Jupyter Lab session, you can always start a new one:\n\nWindowsmacOS & Linux\n\n\n\nOpen Miniforge Prompt\nActivate the environment:\n\nmamba activate nfiua\n\nStart JupyterLab interface:\n\njupyter lab\n\n\n\nOpen Terminal\nActivate the environment:\n\nmamba activate nfiua\n\nStart JupyterLab interface:\n\njupyter lab\n\n\n\n\n\nPlan B with Google Colab\nIf you are unable to install an environment using the instructions above, you can follow the course using Google Colab. You will just need to install the required packages to your Colab environment. Reach out in class or via Discord if you need to set it up.\n\n\nTroubleshooting\nIn case of any issues related to environment creation, reach out in the class or via Google Spaces."
  },
  {
    "objectID": "chapter_1/notebooks.html",
    "href": "chapter_1/notebooks.html",
    "title": "Jupyter and Python",
    "section": "",
    "text": "Welcome to the first hands-on section of the course. You will familiarise yourself with the tools you will be using, ensure that all works as it should, and prepare for some Python code."
  },
  {
    "objectID": "chapter_1/notebooks.html#computer-science-101",
    "href": "chapter_1/notebooks.html#computer-science-101",
    "title": "Jupyter and Python",
    "section": "Computer Science 101",
    "text": "Computer Science 101\nSpatial Data Science depends on code, and coding environments can be unfriendly to an average user. People designing the tools are often computer scientists or have a strong knowledge of CS-related environments. It means we sometimes need to deal with the tools that look a bit scary, like a Terminal or a Command line. Below is a brief introduction to the tools you will need for this course.\n\nTerminal and Command line\nDepending on your operating system, you will have either Terminal (macOS, Linux) or Miniforge Prompt application installed. It will look like this:\n\n\n\nTerminal on MacOS\n\n\nTerminal (and Miniforge Prompt or Command line, but we will refer to all as the terminal for simplicity) is used to interact with applications that do not have any graphic interface or with the apps that do have one, but you want to use them programmatically. The terminal usage is straightforward. Let’s start with a few examples.\n\nCreate a folder (or maybe you already have) to store files for this course.\nDownload this notebook by clicking on the Jupyter option on the right side of this page and move the notebook to the folder.\n\nYou want to see a list of files and folders you have in the folder. First, you need to navigate to the folder. For that, you can use the cd command, which stands for current directory.\ncd sustainable_cities/ua/\nLet’s assume that you are going to create a folder with the course material called ua in another folder called sustainable_cities. The full command is then composed of the cd part, saying set the current directory to… and waits for the parameter, which is a path to the folder in this case - sustainable_cities/sds/.\nOnce in the correct folder, you can use another command, ls, which stands for list and allows you to list the contents of the current directory.\nls\nThe output would look like currently empty but this is where you would see all the contents of a directory:\n\n\n\nOutput of the ‘ls’ command\n\n\nYou can also pass a parameter -l, specifying that you want a long listing including attributes.\nLet’s first navigate a couple of directories up (cd ..), and then list the contents of Documents.\ncd ..\nls -l\nThat changes the output to this (depending on what you have in that folder):\n\nThe syntax is always the same, starting with the app name and then followed by parameters."
  },
  {
    "objectID": "chapter_1/notebooks.html#conda-and-mamba",
    "href": "chapter_1/notebooks.html#conda-and-mamba",
    "title": "Jupyter and Python",
    "section": "Conda and Mamba",
    "text": "Conda and Mamba\nIf you followed the installation of Python described in the infrastructure section, you have used mamba with parameters specifying that you want to use it to create an environment based on an environment.yml file. But what is mamba?\nMamba, and its predecessor, conda, are tools you will use to create “environments” and install Python and Python packages. It is a package manager, ensuring all the necessary parts work together. For example, if you want to create a new environment and install Python in it, you type the following command in your terminal.\nmamba create -n my_environment python\nYou don’t need to do that now but it is useful as you will probably need that at some point later. You can check more in the documentation of mamba.\n\nOther options\nMamba is not the only way of setting up a Python environment. If you don’t want to install anything on your local machine, you can use Google Colab, which gives you an environment with Python and an interface to work with Jupyter Notebooks."
  },
  {
    "objectID": "chapter_1/notebooks.html#jupyter-notebook-and-jupyter-lab",
    "href": "chapter_1/notebooks.html#jupyter-notebook-and-jupyter-lab",
    "title": "Jupyter and Python",
    "section": "Jupyter Notebook and Jupyter Lab",
    "text": "Jupyter Notebook and Jupyter Lab\nWhile you can interact with Python from the terminal, it is inconvenient. Instead, you will use Jypyter Notebooks and Jupyter Lab. Jupyter Notebooks are documents that allow you to mix text and code, execute small pieces of code one at a time and include graphical outputs of your code. Jupyter Lab is a handy interface that allows you to work with multiple notebooks and switch between your Python environments created with Mamba.\nIt is time to say goodbye to the terminal and start Jupyter Lab. You should ensure you have your nfiua environment activated first:\nmamba activate nfiua\nThen you can start Lab using:\njupyter lab\nThis command should open your browser and load the Jupyter Lab interface.\n\n\n\nJupyter Lab interface\n\n\nIn the launcher, we can create a new Notebook by clicking on the Python logo representing our current environment. If you have more of them, you will see them there, as well as other environments using different programming languages like R or Julia.\nThe notebook is composed of cells. This is a cell:\n\n\n\nJupyter Notebool cell\n\n\nCells can contain either code or text. A typical notebook is then a series of cells where some include text describing what is happening while others contain the code, either waiting for execution or already executed. The cells with the executed code may also contain outputs.\nWe can start with simple math that Python can do natively. Run the following code cell. To do that, you can either click the “play” button on top or hit Shift + Enter:\n\n1 + 1\n\n2\n\n\nYou now have a code cell with the output. Jupyter Lab automatically created a new cell. Change its type to Markdown and write a short text describing what the cell above does."
  },
  {
    "objectID": "chapter_1/notebooks.html#let-the-snake-in",
    "href": "chapter_1/notebooks.html#let-the-snake-in",
    "title": "Jupyter and Python",
    "section": "Let the snake in",
    "text": "Let the snake in\nYou can start exploring Python.\n\nSimple Python math\nAs shown above, Python can be used as a simple calculator. Remember, you can press Shift + Enter to execute the code in the cells below. Try it out by typing some simple math into new cells and see what you get.\n\n42 * 12\n\n504\n\n\n\n12 / 3\n\n4.0\n\n\nIf you want to edit and re-run some code, change the cell and press Shift + Enter to execute the modified code.\n\n\nFunctions\nYou can use Python for more advanced math by using a function. Functions are pieces of code that perform a single action, such as printing information to the screen (e.g., the print() function). Functions exist for a huge number of operations in Python.\nLet’s try out a few simple examples using functions to find the sin or square root of a value. You can type sin(3) or sqrt(4) into the cells below to test this out.\n\nsin(3)\n\nNameError: name 'sin' is not defined\n\n\n\nsqrt(4)\n\nWell, that didn’t work. Python can calculate square roots or do basic trigonometry, but we need one more step.\n\nMath operations\nThe table below shows the list of basic arithmetic operations that can be done by default in Python.\n\n\n\nOperation\nSymbol\nExample syntax\nReturned value\n\n\n\n\nAddition\n+\n2 + 2\n4\n\n\nSubtraction\n-\n4 - 2\n2\n\n\nMultiplication\n*\n2 * 3\n6\n\n\nDivision\n/\n4 / 2\n2\n\n\nExponentiation\n**\n2 ** 3\n8\n\n\n\nFor anything more advanced, we need to load a module or a package. For math operations, this module is called math and can be loaded by typing import math.\n\nimport math\n\nNow that we have access to functions in the math module, we can use it by typing the module name, a period (dot), and the name of the function we want to use. For example, math.sin(3). Try this with the sine and square root examples from above.\n\nmath.sin(3)\n\n\nmath.sqrt(4)\n\nLet’s summarise what you’ve just done with modules:\n\nA module is a group of code items, such as functions, related to one another. Individual modules are often in a group called a package.\nModules can be loaded using import. Functions that are part of the module modulename can then be used by typing modulename.functionname(). For example, sin() is a function that is part of the math module and is used by typing math.sin() with some number between the parentheses.\nWithin a Jupyter Notebook, the variables you defined earlier in the notebook will be available for use in the following cells as long as you have executed the cells.\nModules may also contain constants such as math.pi (notice no parentheses at the end). Type this in the cell below to see the constant’s math.pi value.\n\n\nmath.pi\n\n\n\nCombining functions\nFunctions can also be combined. The print() function returns values within the parentheses as text on the screen. Below, try printing the value of the square root of four.\n\nprint(math.sqrt(4))\n\nYou can also combine text with other calculated values using the print() function. For example, print('Two plus two is', 2+2) would generate the text reading 'Two plus two is 4'. Combine the print() function with the math.sqrt() function in the cell below to produce text that reads 'The square root of 4 is 2.0'.\n\nprint(\"The square root of 4 is\", math.sqrt(4))\n\n\n\n\nVariables\nA variable can store values calculated in expressions and used for other calculations. Assigning value to variables is straightforward. To assign a value, you type variable_name = value, where variable_name is the name of the variable you wish to define. In the cell below, define a variable called temp_celsius, assign it a value of 10.0, and then print that variable value using the print() function. Note that you should do this on two separate lines.\n\ntemp_celsius = 10.0\nprint(temp_celsius)\n\nAs we did above, you can combine text and even use some math when printing out variable values. The idea is similar to adding 2+2 or calculating the square root of four from the previous section. In the cell below, print out the value of temp_celsius in degrees Fahrenheit by multiplying temp_celsius by 9/5 and adding 32. This should be done within the print() function to produce output that reads 'Temperature in Fahrenheit: 50.0'.\n\nprint(\"Temperature in Fahrenheit:\", 9 / 5 * temp_celsius + 32)\n\n\n\nUpdating variables\nValues stored in variables can also be updated. Let’s redefine the value of temp_celsius to be equal to 15.0 and print its value in the cells below.\n\ntemp_celsius = 15.0\n\n\nprint(\"temperature in Celsius is now:\", temp_celsius)\n\n\nWarning\nIf you try to run some code that accesses a variable that has not yet been defined, you will get a NameError message. Try printing out the value of the variable temp_fahrenheit using the print() function in the cell below.\n\n\nprint(\"Temperature in Celsius:\", 5 / 9 * (temp_fahrenheit - 32))\n\n\nNote\nOne of the interesting things here is that if we define the undefined variable in a cell lower down in the notebook and execute that cell, we can return to the earlier cell, and the code should now work. That was a bit of a complicated sentence, so let’s test this all out. First, let’s define a variable called temp_fahrenheit in the cell below and assign it to be equal to 9/5 * temp_celsius + 32, the conversion factor from temperatures in Celsius to Fahrenheit. Then, return to the cell above this text and run that cell again. See how the error message has gone away? temp_fahrenheit has now been defined, and thus, the cell above no longer generates a NameError when the code is executed.\nAlso, the number beside the cell, for example, In [2], tells you the order in which the Python cells have been executed. This way, you can see a history of the order in which you have run the cells.\n\n\ntemp_fahrenheit = 9 / 5 * temp_celsius + 32\n\nTo check their current values, print out the values of temp_celsius and temp_fahrenheit in the cell below.\n\nprint(\"temperature in Celsius:\", temp_celsius, \"and in Fahrenheit:\", temp_fahrenheit)\n\n\n\nData types\nA data type determines the characteristics of data in a program. There are four basic data types in Python, as shown in the table below.\n\n\n\nData type name\nData type\nExample\n\n\n\n\nint\nWhole integer values\n4\n\n\nfloat\nDecimal values\n3.1415\n\n\nstr\nCharacter strings\n'Hot'\n\n\nbool\nTrue/false values\nTrue\n\n\n\nThe data type can be found using the type() function. As you will see, the data types are essential because some are incompatible.\nLet’s define a variable weather_forecast and assign it the value 'Hot'. After this, we can check its data type using the type() function.\n\nweather_forecast = \"Hot\"\ntype(weather_forecast)\n\nLet’s also check the type of temp_fahrenheit. What happens if you try to combine temp_fahrenheit and weather_forecast in a single math equation such as temp_fahrenheit = temp_fahrenheit + 5.0 * weather_forecast?\n\ntype(temp_fahrenheit)\n\n\ntemp_fahrenheit = temp_fahrenheit + 5.0 * weather_forecast\n\nIn this case, we get at TypeError because we are trying to execute a math operation with data types that are not compatible. There is no way in Python to multiply numbers with a character string."
  },
  {
    "objectID": "chapter_1/notebooks.html#acknowledgements",
    "href": "chapter_1/notebooks.html#acknowledgements",
    "title": "Jupyter and Python",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe ‘Let the snake in’ section is derived from A taste of Python section of the Geo-Python course 2022 by D. Whipp, H. Tenkanen, V. Heikinheimo, H. Aagesen, and C. Fink from the Department of Geosciences and Geography, University of Helsinki, licensed under CC-BY-SA 4.0."
  },
  {
    "objectID": "chapter_2/pandas.html",
    "href": "chapter_2/pandas.html",
    "title": "Data wrangling",
    "section": "",
    "text": "You know the basics. What are Jupyter notebooks, how do they work, and how do you run Python in them. It is time to start using them for data science (no, that simple math you did the last time doesn’t count as data science).\nYou are about to enter the PyData ecosystem. It means that you will start learning how to work with Python from the middle. This course does not explicitly cover the fundamentals of programming. It is expected that those parts you need you’ll be able to pick as you go through the specialised data science stack. If you’re stuck, confused or need further explanation, use Google (or your favourite search engine), ask AI to explain the code or ask on Discord or during the class. Not everything will be told during the course (by design), and the internet is a friend of every programmer, so let’s figure out how to use it efficiently from the beginning.\nLet’s dig in!"
  },
  {
    "objectID": "chapter_2/pandas.html#munging-and-wrangling",
    "href": "chapter_2/pandas.html#munging-and-wrangling",
    "title": "Data wrangling",
    "section": "Munging and wrangling",
    "text": "Munging and wrangling\nReal-world datasets are messy. There is no way around it: datasets have “holes” (missing data), the amount of formats in which data can be stored is endless, and the best structure to share data is not always the optimum to analyse them, hence the need to munge[1] them. As has been correctly pointed out in many outlets, much of the time spent in what is called Data Science is related not only to sophisticated modelling and insight but has to do with much more basic and less exotic tasks such as obtaining data, processing, and turning them into a shape that makes analysis possible, and exploring it to get to know their basic properties.\nSurprisingly, very little has been published on patterns, techniques, and best practices for quick and efficient data cleaning, manipulation, and transformation because of how labour-intensive and relevant this aspect is. In this session, you will use a few real-world datasets and learn how to process them into Python so they can be transformed and manipulated, if necessary, and analysed. For this, you will introduce some of the bread and butter of data analysis and scientific computing in Python. These are fundamental tools that are constantly used in almost any task relating to data analysis.\nThis notebook covers the basics and the content that is expected to be learnt by every student. You use a prepared dataset that saves us much of the more intricate processing that goes beyond the introductory level the session is aimed at. If you are interested in how it was done, there is a notebook.\nThis notebook discusses several patterns to clean and structure data properly, including tidying, subsetting, and aggregating. You finish with some basic visualisation. An additional extension presents more advanced tricks to manipulate tabular data."
  },
  {
    "objectID": "chapter_2/pandas.html#dataset",
    "href": "chapter_2/pandas.html#dataset",
    "title": "Data wrangling",
    "section": "Dataset",
    "text": "Dataset\nYou will be exploring demographic characteristics of Madrid. The data has been aggregated to a neighbourhood level by the statistic’s office of Madrid’s City Hall. It contains information per year (from 2018 to 2023) and genre.\nAs with many datasets that will be used during this course, the data was originally gound in the data portal on the following link\nThe main tool you should use for this task is the pandas package. As with the math you used before, you must import it first.\n[1] Data munging and data wrangling are used interchangeably. Pick the one you like.\n\nimport pandas as pd\n\nThe data is stored in a CSV file. To make things easier, you can read data from a file posted online so, for now, you do not need to download any dataset:\n\nmadrid_pop = pd.read_csv(\n    \"https://datos.madrid.es/egob/catalogo/300557-0-poblacion-distrito-barrio.csv\",\n    sep=\";\",\n)\n\n\nTip\nYou are using read_csv because the file you want to read is in CSV format. However, the current data is actually not separated by commas , but with a semicolon ;, hence the additional parameter in the code.\nNote that pandas allows for many more formats to be read and write. A full list of formats supported may be found in the documentation.\n\n\nAlternative\nInstead of reading the file directly off the web, it is possible to download it manually, store it on your computer, and read it locally. To do that, you can follow these steps:\n\nDownload the file by clicking on this link and saving the file\nPlace the file in the same folder as the notebook where you intend to read it\nReplace the code in the cell above with:\n\nmadrid_pop = pd.read_csv(\n    \"poblacion_1_enero.csv\",\n    sep=\";\",\n    index_col=\"distrito\",\n)"
  },
  {
    "objectID": "chapter_2/pandas.html#pandas-101",
    "href": "chapter_2/pandas.html#pandas-101",
    "title": "Data wrangling",
    "section": "Pandas 101",
    "text": "Pandas 101\nNow, you are ready to start playing and interrogating the dataset! What you have at your fingertips is a table summarising, for each of the districts in Madrid, how many people lived there by genre. These tables are called DataFrame objects, and they have a lot of functionality built-in to explore and manipulate the data they contain. Let’s explore a few of those cool tricks!\n\nData Structures\nThe first aspect worth spending a bit of time on is the structure of a DataFrame. You can print it by simply typing its name:\n\nmadrid_pop\n\n\n\n\n\n\n\n\nfecha\ncod_municipio\nmunicipio\ncod_distrito\ndistrito\ncod_barrio\nbarrio\nnum_personas\nnum_personas_hombres\nnum_personas_mujeres\n\n\n\n\n0\n1 de enero de 2023\n28079\nMadrid\n1\nCentro\n1\nCentro\n139.687\n70.770\n68.917\n\n\n1\n1 de enero de 2023\n28079\nMadrid\n2\nArganzuela\n2\nArganzuela\n153.304\n71.754\n81.550\n\n\n2\n1 de enero de 2023\n28079\nMadrid\n3\nRetiro\n3\nRetiro\n117.918\n53.458\n64.460\n\n\n3\n1 de enero de 2023\n28079\nMadrid\n4\nSalamanca\n4\nSalamanca\n145.702\n64.452\n81.250\n\n\n4\n1 de enero de 2023\n28079\nMadrid\n5\nChamartín\n5\nChamartín\n144.796\n65.245\n79.551\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n913\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n212\nAeropuerto\n1.794\n922\n872\n\n\n914\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n213\nCascoHistóricodeBarajas\n7.336\n3.550\n3.786\n\n\n915\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n214\nTimón\n11.750\n5.651\n6.099\n\n\n916\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n215\nCorralejos\n7.510\n3.657\n3.853\n\n\n917\n1 de enero de 2018\n28079\nMadrid\nTodos\nTodos\nTodos\nTodos\n3.221.824\n1.500.340\n1.721.484\n\n\n\n\n918 rows × 10 columns\n\n\n\nAs you can expect, the dataset was inputed in the original local languange so to make things a little easier, we are going to translate the column names using one list of texts.\n\nNote that there are multiple ways of arriving at the same output, the below is just one of them\n\n\nen_cols = [\n    \"date\",\n    \"code_municipality\",\n    \"municipality\",\n    \"code_district\",\n    \"district\",\n    \"code_neighbourhood\",\n    \"neighbourhood\",\n    \"num_people\",\n    \"num_people_men\",\n    \"num_people_women\",\n]\n\nmadrid_pop.columns = en_cols\nmadrid_pop\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ndistrict\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\n\n\n\n\n0\n1 de enero de 2023\n28079\nMadrid\n1\nCentro\n1\nCentro\n139.687\n70.770\n68.917\n\n\n1\n1 de enero de 2023\n28079\nMadrid\n2\nArganzuela\n2\nArganzuela\n153.304\n71.754\n81.550\n\n\n2\n1 de enero de 2023\n28079\nMadrid\n3\nRetiro\n3\nRetiro\n117.918\n53.458\n64.460\n\n\n3\n1 de enero de 2023\n28079\nMadrid\n4\nSalamanca\n4\nSalamanca\n145.702\n64.452\n81.250\n\n\n4\n1 de enero de 2023\n28079\nMadrid\n5\nChamartín\n5\nChamartín\n144.796\n65.245\n79.551\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n913\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n212\nAeropuerto\n1.794\n922\n872\n\n\n914\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n213\nCascoHistóricodeBarajas\n7.336\n3.550\n3.786\n\n\n915\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n214\nTimón\n11.750\n5.651\n6.099\n\n\n916\n1 de enero de 2018\n28079\nMadrid\n21\nBarajas\n215\nCorralejos\n7.510\n3.657\n3.853\n\n\n917\n1 de enero de 2018\n28079\nMadrid\nTodos\nTodos\nTodos\nTodos\n3.221.824\n1.500.340\n1.721.484\n\n\n\n\n918 rows × 10 columns\n\n\n\nNote the printing is cut to keep a nice and compact view but enough to see its structure. Since they represent a table of data, DataFrame objects have two dimensions: rows and columns. Each of these is automatically assigned a name in what you will call its index. When printing, the index of each dimension is rendered in bold, as opposed to the standard rendering for the content. The example above shows how the column index is automatically picked up from the .csv file’s column names. For rows, we will specify disctrict to be used as the index.\n\nIn our case, because we had not established the index_col from the loading of the file pandas automatically generates a sequence starting in 0 and going all the way to the number of rows minus one. This is the standard structure of a DataFrame object, so you will come to it over and over. Importantly, even when you move to spatial data, your datasets will have a similar structure.\n\n\n# Remove leading and trailing spaces from 'distrito'\nmadrid_pop[\"district\"] = madrid_pop[\"district\"].str.strip()\nmadrid_pop.set_index(\"district\", inplace=True)\n\nOne final feature that is worth mentioning about these tables is that they can hold columns with different types of data. In this example, you have counts (or int for integer types) and ratios (or ‘float’ for floating point numbers - a number with decimals) for each column. But it is useful to keep in mind that you can combine this with columns that hold other types of data such as categories, text (str, for string), dates or, as you will see later in the course, geographic features.\nTo extract a single column from this DataFrame, specify its name in the square brackets ([]). Note that the name, in this case, is a string. A piece of text. As such, it needs to be within single (') or double quotes (\"). The resulting data structure is no longer a DataFrame, but you have a Series because you deal with a single column.\n\nmadrid_pop[\"num_people_women\"]\n\ndistrict\nCentro           68.917\nArganzuela       81.550\nRetiro           64.460\nSalamanca        81.250\nChamartín        79.551\n                ...    \nBarajas             872\nBarajas           3.786\nBarajas           6.099\nBarajas           3.853\nTodos         1.721.484\nName: num_people_women, Length: 918, dtype: object\n\n\n\n\nInspect\nInspecting what it looks like. You can check the table’s top (or bottom) X lines by passing X to the method head (tail). For example, for the top/bottom five lines:\n\nmadrid_pop.head()\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\n\n\ndistrict\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n1\nCentro\n139.687\n70.770\n68.917\n\n\nArganzuela\n1 de enero de 2023\n28079\nMadrid\n2\n2\nArganzuela\n153.304\n71.754\n81.550\n\n\nRetiro\n1 de enero de 2023\n28079\nMadrid\n3\n3\nRetiro\n117.918\n53.458\n64.460\n\n\nSalamanca\n1 de enero de 2023\n28079\nMadrid\n4\n4\nSalamanca\n145.702\n64.452\n81.250\n\n\nChamartín\n1 de enero de 2023\n28079\nMadrid\n5\n5\nChamartín\n144.796\n65.245\n79.551\n\n\n\n\n\n\n\n\nmadrid_pop.tail(3)\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\n\n\ndistrict\n\n\n\n\n\n\n\n\n\n\n\n\n\nBarajas\n1 de enero de 2018\n28079\nMadrid\n21\n214\nTimón\n11.750\n5.651\n6.099\n\n\nBarajas\n1 de enero de 2018\n28079\nMadrid\n21\n215\nCorralejos\n7.510\n3.657\n3.853\n\n\nTodos\n1 de enero de 2018\n28079\nMadrid\nTodos\nTodos\nTodos\n3.221.824\n1.500.340\n1.721.484\n\n\n\n\n\n\n\nInspecting your datasets is vital to find errors that could skew your analysis. As you can see, the last row of our dataset has been aggregated to consolidate the sum of all column values ( Spanish: Todos -&gt; English: All ).\nBefore continuing, let’s fix that. First we are going to check if we have any more occurrences of Todos.\n\nmadrid_pop.loc[\"Todos\"]\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\n\n\ndistrict\n\n\n\n\n\n\n\n\n\n\n\n\n\nTodos\n1 de enero de 2023\n28079\nMadrid\nTodos\nTodos\nTodos\n3.339.931\n1.559.866\n1.780.065\n\n\nTodos\n1 de enero de 2022\n28079\nMadrid\nTodos\nTodos\nTodos\n3.286.662\n1.534.824\n1.751.838\n\n\nTodos\n1 de enero de 2021\n28079\nMadrid\nTodos\nTodos\nTodos\n3.312.310\n1.545.157\n1.767.153\n\n\nTodos\n1 de enero de 2020\n28079\nMadrid\nTodos\nTodos\nTodos\n3.334.730\n1.554.732\n1.779.998\n\n\nTodos\n1 de enero de 2019\n28079\nMadrid\nTodos\nTodos\nTodos\n3.266.126\n1.521.178\n1.744.948\n\n\nTodos\n1 de enero de 2018\n28079\nMadrid\nTodos\nTodos\nTodos\n3.221.824\n1.500.340\n1.721.484\n\n\n\n\n\n\n\nNow that we know that for every year the data has been aggregated and added back as a new row, we know that removing the last row would not be enough to correct our DataFrame.\nWe will make use of the drop function in combination with the native loc function of DataFrames.\n\nprint(\"Rows before droping values: \", len(madrid_pop))\nmadrid_pop.drop(index=\"Todos\", inplace=True)\nprint(\"Rows after droping values: \", len(madrid_pop))\n\nRows before droping values:  918\nRows after droping values:  912\n\n\nNow, let’s get an overview of the table:\n\nmadrid_pop.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 912 entries, Centro to Barajas\nData columns (total 9 columns):\n #   Column              Non-Null Count  Dtype \n---  ------              --------------  ----- \n 0   date                912 non-null    object\n 1   code_municipality   912 non-null    int64 \n 2   municipality        912 non-null    object\n 3   code_district       912 non-null    object\n 4   code_neighbourhood  912 non-null    object\n 5   neighbourhood       912 non-null    object\n 6   num_people          912 non-null    object\n 7   num_people_men      912 non-null    object\n 8   num_people_women    912 non-null    object\ndtypes: int64(1), object(8)\nmemory usage: 71.2+ KB\n\n\n\nCan you spot something wrong ?\n\nIntereger numbers can sometimes be used as category tags (like for the case of code_district or code_neighbourhood). However, the actual values of population counts would be better stored as full integers. For that, we will perform the following operations :\n\nList the columns that have people counts\nLoop throuhg each one and :\n\n\nRemove all dots .\nConvert the values to numeric; allowing for potentia NaN values\nSpecify that the values are Int64\n\n\n# List of columns to process\ncolumns_to_process = [\"num_people\", \"num_people_men\", \"num_people_women\"]\n\n# Loop through columns and perform operations\nfor column in columns_to_process:\n    madrid_pop[column] = madrid_pop[column].str.replace(\".\", \"\")  # Remove dots\n    madrid_pop[column] = pd.to_numeric(\n        madrid_pop[column], errors=\"coerce\"\n    )  # Convert to numeric\n    madrid_pop[column] = madrid_pop[column].astype(\"Int64\")  # Convert to Int64\n\nmadrid_pop.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 912 entries, Centro to Barajas\nData columns (total 9 columns):\n #   Column              Non-Null Count  Dtype \n---  ------              --------------  ----- \n 0   date                912 non-null    object\n 1   code_municipality   912 non-null    int64 \n 2   municipality        912 non-null    object\n 3   code_district       912 non-null    object\n 4   code_neighbourhood  912 non-null    object\n 5   neighbourhood       912 non-null    object\n 6   num_people          912 non-null    Int64 \n 7   num_people_men      912 non-null    Int64 \n 8   num_people_women    912 non-null    Int64 \ndtypes: Int64(3), int64(1), object(5)\nmemory usage: 73.9+ KB\n\n\n\n\nSummarise\nOr of the values of the table:\n\nmadrid_pop.describe()\n\n\n\n\n\n\n\n\ncode_municipality\nnum_people\nnum_people_men\nnum_people_women\n\n\n\n\ncount\n912.0\n912.0\n912.0\n912.0\n\n\nmean\n28079.0\n43336.804825\n20210.739035\n23126.065789\n\n\nstd\n0.0\n51564.255101\n24072.657139\n27522.444389\n\n\nmin\n28079.0\n945.0\n490.0\n455.0\n\n\n25%\n28079.0\n17330.5\n8132.0\n9267.25\n\n\n50%\n28079.0\n24700.5\n11580.0\n13640.0\n\n\n75%\n28079.0\n42166.5\n19700.25\n22349.75\n\n\nmax\n28079.0\n262339.0\n122632.0\n139707.0\n\n\n\n\n\n\n\nNote how the output is also a DataFrame object, so you can do with it the same things you would with the original table (e.g. writing it to a file).\nIn this case, the summary might be better presented if the table is “transposed”:\n\nmadrid_pop[columns_to_process].describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nnum_people\n912.0\n43336.804825\n51564.255101\n945.0\n17330.5\n24700.5\n42166.5\n262339.0\n\n\nnum_people_men\n912.0\n20210.739035\n24072.657139\n490.0\n8132.0\n11580.0\n19700.25\n122632.0\n\n\nnum_people_women\n912.0\n23126.065789\n27522.444389\n455.0\n9267.25\n13640.0\n22349.75\n139707.0\n\n\n\n\n\n\n\nEqually, common descriptive statistics are also available. To obtain minimum values for each column, you can use .min().\n\nmadrid_pop.min()\n\ndate                  1 de enero de 2018\ncode_municipality                  28079\nmunicipality                      Madrid\ncode_district                          1\ncode_neighbourhood                     1\nneighbourhood                 Arganzuela\nnum_people                           945\nnum_people_men                       490\nnum_people_women                     455\ndtype: object\n\n\nOr to obtain a minimum for a single column only.\n\nmadrid_pop[\"num_people_women\"].min()\n\n455\n\n\nNote here how you have restricted the calculation of the minimum value to one column only by getting the Series and calling .min() on that.\nSimilarly, you can restrict the calculations to a single district using .loc[] indexer:\n\nmadrid_pop.loc[\"Centro\"].min()\n\ndate                  1 de enero de 2018\ncode_municipality                  28079\nmunicipality                      Madrid\ncode_district                          1\ncode_neighbourhood                     1\nneighbourhood                     Centro\nnum_people                          7201\nnum_people_men                      3672\nnum_people_women                    3529\ndtype: object\n\n\nLet’s see when and where did said minimum ocurred.\n\nmadrid_pop[madrid_pop[\"num_people_women\"] == 3529]\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\n\n\ndistrict\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n16\nSol\n7201\n3672\n3529\n\n\n\n\n\n\n\n\n\nCreate new columns\nYou can generate new variables by applying operations to existing ones. For example, you can calculate the ratio of women.\n\nratio_women = madrid_pop[\"num_people_women\"] / madrid_pop[\"num_people\"]\nratio_women.head()\n\ndistrict\nCentro        0.493367\nArganzuela     0.53195\nRetiro        0.546651\nSalamanca     0.557645\nChamartín     0.549401\ndtype: Float64\n\n\nOnce you have created the variable, you can make it part of the table:\n\nmadrid_pop[\"ratio_women\"] = ratio_women\nmadrid_pop.head()\n\n\n\n\n\n\n\n\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\n\n\ndistrict\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n1\nCentro\n139687\n70770\n68917\n0.493367\n\n\nArganzuela\n1 de enero de 2023\n28079\nMadrid\n2\n2\nArganzuela\n153304\n71754\n81550\n0.53195\n\n\nRetiro\n1 de enero de 2023\n28079\nMadrid\n3\n3\nRetiro\n117918\n53458\n64460\n0.546651\n\n\nSalamanca\n1 de enero de 2023\n28079\nMadrid\n4\n4\nSalamanca\n145702\n64452\n81250\n0.557645\n\n\nChamartín\n1 de enero de 2023\n28079\nMadrid\n5\n5\nChamartín\n144796\n65245\n79551\n0.549401\n\n\n\n\n\n\n\n\n\nIndex-based queries\nHere, you explore how to subset parts of a DataFrame if you know exactly which bits you want. For example, if you want to extract the influenza mortality and total population of the first four areas in the table, you use loc with lists:\n\nwomen_ratio_2districts = madrid_pop.loc[\n    [\"Centro\", \"Retiro\"],\n    [\"date\", \"ratio_women\"],\n]\nwomen_ratio_2districts\n\n\n\n\n\n\n\n\ndate\nratio_women\n\n\ndistrict\n\n\n\n\n\n\nCentro\n1 de enero de 2023\n0.493367\n\n\nCentro\n1 de enero de 2023\n0.508234\n\n\nCentro\n1 de enero de 2023\n0.47853\n\n\nCentro\n1 de enero de 2023\n0.505362\n\n\nCentro\n1 de enero de 2023\n0.491685\n\n\n...\n...\n...\n\n\nRetiro\n1 de enero de 2018\n0.540638\n\n\nRetiro\n1 de enero de 2018\n0.535416\n\n\nRetiro\n1 de enero de 2018\n0.562966\n\n\nRetiro\n1 de enero de 2018\n0.531449\n\n\nRetiro\n1 de enero de 2018\n0.545746\n\n\n\n\n84 rows × 2 columns\n\n\n\nYou can see how you can create a list with the names (index IDs) along each of the two dimensions of a DataFrame (rows and columns), and loc will return a subset of the original table only with the elements queried for.\nAn alternative to list-based queries is what is called “range-based” queries. These work on the indices of the table, but instead of requiring the ID of each item you want to retrieve, they operate by requiring only two IDs: the first and last element in a range of items. Range queries are expressed with a colon (:). However, to perform this operation Index IDs need to be unique. Since this is not our case we will create a new index composed the year and the code_neighbourhood as a new index to our DataFrame.\n\n# Reset the index to bring back 'district' as a regular column\nmadrid_pop.reset_index(inplace=True)\n# Extract the last 4 digits from the 'date' column and create a new 'year' column\nmadrid_pop[\"year\"] = madrid_pop[\"date\"].str[-4:]\n# Create a new column with the combination of 'year' and 'code_neighbourhood'\nmadrid_pop[\"new_index\"] = madrid_pop[\"year\"] + \"_\" + madrid_pop[\"code_neighbourhood\"]\n# Set the new index\nmadrid_pop.set_index(\"new_index\", inplace=True)\nmadrid_pop.head(3)\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023_1\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n1\nCentro\n139687\n70770\n68917\n0.493367\n2023\n\n\n2023_2\nArganzuela\n1 de enero de 2023\n28079\nMadrid\n2\n2\nArganzuela\n153304\n71754\n81550\n0.53195\n2023\n\n\n2023_3\nRetiro\n1 de enero de 2023\n28079\nMadrid\n3\n3\nRetiro\n117918\n53458\n64460\n0.546651\n2023\n\n\n\n\n\n\n\nLook at the order of the index. This is important beacuase “range-based” queries assume you understand what come first in the data to input the index cutoff values.\n\nrange_query = madrid_pop.loc[\"2019_1\":\"2018_1\", \"num_people\":\"num_people_women\"]\n\nrange_query\n\n\n\n\n\n\n\n\nnum_people\nnum_people_men\nnum_people_women\n\n\nnew_index\n\n\n\n\n\n\n\n2019_1\n134881\n67829\n67052\n\n\n2019_2\n153830\n71631\n82199\n\n\n2019_3\n119379\n54098\n65281\n\n\n2019_4\n146148\n64395\n81753\n\n\n2019_5\n145865\n65565\n80300\n\n\n...\n...\n...\n...\n\n\n2019_212\n1851\n952\n899\n\n\n2019_213\n7565\n3648\n3917\n\n\n2019_214\n12388\n5916\n6472\n\n\n2019_215\n7642\n3746\n3896\n\n\n2018_1\n132352\n66320\n66032\n\n\n\n\n153 rows × 3 columns\n\n\n\nThe range query picks up all the elements between the specified IDs. Note that for this to work, the first ID in the range needs to be placed before the second one in the table’s index.\nOnce you know about list and range-based queries, you can combine them!\n\n\nCondition-based queries\nHowever, sometimes, you do not know exactly which observations you want, but you do know what conditions they need to satisfy (e.g. areas with more than 2,000 inhabitants). For these cases, DataFrames support selection based on conditions. Let us see a few examples. Suppose you want to select…\n… neighbourhoods wich over time have had less than 50% of women\n\nfewer_women = madrid_pop[madrid_pop[\"ratio_women\"] &lt; 0.5]\nfewer_women\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023_1\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n1\nCentro\n139687\n70770\n68917\n0.493367\n2023\n\n\n2023_12\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n12\nEmbajadores\n46204\n24094\n22110\n0.47853\n2023\n\n\n2023_14\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n14\nJusticia\n18219\n9261\n8958\n0.491685\n2023\n\n\n2023_16\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n16\nSol\n8164\n4239\n3925\n0.480769\n2023\n\n\n2023_81\nFuencarral-El Pardo\n1 de enero de 2023\n28079\nMadrid\n8\n81\nElPardo\n3421\n1716\n1705\n0.498392\n2023\n\n\n2023_106\nLatina\n1 de enero de 2023\n28079\nMadrid\n10\n106\nCuatroVientos\n6122\n3068\n3054\n0.498857\n2023\n\n\n2023_194\nVicálvaro\n1 de enero de 2023\n28079\nMadrid\n19\n194\nElCañaveral\n13054\n6652\n6402\n0.490424\n2023\n\n\n2023_212\nBarajas\n1 de enero de 2023\n28079\nMadrid\n21\n212\nAeropuerto\n1902\n965\n937\n0.492639\n2023\n\n\n2022_1\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n1\nCentro\n139682\n70986\n68696\n0.491803\n2022\n\n\n2022_12\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n12\nEmbajadores\n46444\n24271\n22173\n0.477414\n2022\n\n\n2022_14\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n14\nJusticia\n18015\n9221\n8794\n0.488149\n2022\n\n\n2022_16\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n16\nSol\n8117\n4232\n3885\n0.478625\n2022\n\n\n2022_106\nLatina\n1 de enero de 2022\n28079\nMadrid\n10\n106\nCuatroVientos\n5966\n2996\n2970\n0.497821\n2022\n\n\n2022_194\nVicálvaro\n1 de enero de 2022\n28079\nMadrid\n19\n194\nElCañaveral\n8944\n4525\n4419\n0.494074\n2022\n\n\n2022_212\nBarajas\n1 de enero de 2022\n28079\nMadrid\n21\n212\nAeropuerto\n1895\n967\n928\n0.48971\n2022\n\n\n2021_1\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n1\nCentro\n141236\n71881\n69355\n0.491058\n2021\n\n\n2021_12\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n12\nEmbajadores\n47238\n24767\n22471\n0.475698\n2021\n\n\n2021_14\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n14\nJusticia\n18208\n9291\n8917\n0.48973\n2021\n\n\n2021_16\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n16\nSol\n7993\n4120\n3873\n0.484549\n2021\n\n\n2021_81\nFuencarral-El Pardo\n1 de enero de 2021\n28079\nMadrid\n8\n81\nElPardo\n3443\n1723\n1720\n0.499564\n2021\n\n\n2021_194\nVicálvaro\n1 de enero de 2021\n28079\nMadrid\n19\n194\nElCañaveral\n4430\n2254\n2176\n0.491196\n2021\n\n\n2021_212\nBarajas\n1 de enero de 2021\n28079\nMadrid\n21\n212\nAeropuerto\n1918\n988\n930\n0.48488\n2021\n\n\n2020_1\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n1\nCentro\n140473\n71127\n69346\n0.493661\n2020\n\n\n2020_12\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n12\nEmbajadores\n47048\n24497\n22551\n0.479319\n2020\n\n\n2020_14\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n14\nJusticia\n18021\n9161\n8860\n0.491649\n2020\n\n\n2020_16\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n16\nSol\n7622\n3895\n3727\n0.488979\n2020\n\n\n2020_106\nLatina\n1 de enero de 2020\n28079\nMadrid\n10\n106\nCuatroVientos\n5881\n2958\n2923\n0.497024\n2020\n\n\n2020_194\nVicálvaro\n1 de enero de 2020\n28079\nMadrid\n19\n194\nElCañaveral\n2398\n1230\n1168\n0.487073\n2020\n\n\n2020_212\nBarajas\n1 de enero de 2020\n28079\nMadrid\n21\n212\nAeropuerto\n1900\n975\n925\n0.486842\n2020\n\n\n2019_1\nCentro\n1 de enero de 2019\n28079\nMadrid\n1\n1\nCentro\n134881\n67829\n67052\n0.49712\n2019\n\n\n2019_12\nCentro\n1 de enero de 2019\n28079\nMadrid\n1\n12\nEmbajadores\n45259\n23390\n21869\n0.483197\n2019\n\n\n2019_14\nCentro\n1 de enero de 2019\n28079\nMadrid\n1\n14\nJusticia\n17153\n8675\n8478\n0.494258\n2019\n\n\n2019_16\nCentro\n1 de enero de 2019\n28079\nMadrid\n1\n16\nSol\n7337\n3749\n3588\n0.489028\n2019\n\n\n2019_106\nLatina\n1 de enero de 2019\n28079\nMadrid\n10\n106\nCuatroVientos\n5748\n2909\n2839\n0.493911\n2019\n\n\n2019_194\nVicálvaro\n1 de enero de 2019\n28079\nMadrid\n19\n194\nElCañaveral\n1530\n785\n745\n0.486928\n2019\n\n\n2019_212\nBarajas\n1 de enero de 2019\n28079\nMadrid\n21\n212\nAeropuerto\n1851\n952\n899\n0.485683\n2019\n\n\n2018_1\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n1\nCentro\n132352\n66320\n66032\n0.498912\n2018\n\n\n2018_12\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n12\nEmbajadores\n44630\n23031\n21599\n0.483957\n2018\n\n\n2018_14\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n14\nJusticia\n16578\n8334\n8244\n0.497286\n2018\n\n\n2018_16\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n16\nSol\n7201\n3672\n3529\n0.490071\n2018\n\n\n2018_106\nLatina\n1 de enero de 2018\n28079\nMadrid\n10\n106\nCuatroVientos\n5662\n2870\n2792\n0.493112\n2018\n\n\n2018_194\nVicálvaro\n1 de enero de 2018\n28079\nMadrid\n19\n194\nElCañaveral\n945\n490\n455\n0.481481\n2018\n\n\n2018_212\nBarajas\n1 de enero de 2018\n28079\nMadrid\n21\n212\nAeropuerto\n1794\n922\n872\n0.486065\n2018\n\n\n\n\n\n\n\n… most populated area across all years:\n\nlargest_hood_num = madrid_pop[\"num_people\"].max()\nlargest_hood = madrid_pop[madrid_pop[\"num_people\"] == largest_hood_num]\nlargest_hood\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023_11\nCarabanchel\n1 de enero de 2023\n28079\nMadrid\n11\n11\nCarabanchel\n262339\n122632\n139707\n0.532544\n2023\n\n\n\n\n\n\n\n\nTip\nIf you are interested, more detail about query is available in the pandas documentation. This is another way of slicing Dataframes, but for now we will stay with the loc function.\n\n\n\nCombining queries\nNow, all of these queries can be combined with each other for further flexibility. For example, imagine you want to know the areas that have more than 100K inhabitants and have over 50% of women.\n\nwomen_power = madrid_pop.loc[\n    (madrid_pop[\"num_people_women\"] &gt; 100000) & (madrid_pop[\"ratio_women\"] &gt; 0.5)\n]\nwomen_power\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023_8\nFuencarral-El Pardo\n1 de enero de 2023\n28079\nMadrid\n8\n8\nFuencarral-El Pardo\n248443\n116944\n131499\n0.529292\n2023\n\n\n2023_10\nLatina\n1 de enero de 2023\n28079\nMadrid\n10\n10\nLatina\n241672\n112093\n129579\n0.536177\n2023\n\n\n2023_11\nCarabanchel\n1 de enero de 2023\n28079\nMadrid\n11\n11\nCarabanchel\n262339\n122632\n139707\n0.532544\n2023\n\n\n2023_13\nPuente de Vallecas\n1 de enero de 2023\n28079\nMadrid\n13\n13\nPuente de Vallecas\n241603\n114542\n127061\n0.525908\n2023\n\n\n2023_15\nCiudad Lineal\n1 de enero de 2023\n28079\nMadrid\n15\n15\nCiudad Lineal\n220345\n100759\n119586\n0.542722\n2023\n\n\n2023_16\nHortaleza\n1 de enero de 2023\n28079\nMadrid\n16\n16\nHortaleza\n198391\n94100\n104291\n0.525684\n2023\n\n\n2022_8\nFuencarral-El Pardo\n1 de enero de 2022\n28079\nMadrid\n8\n8\nFuencarral-El Pardo\n246281\n115955\n130326\n0.529176\n2022\n\n\n2022_10\nLatina\n1 de enero de 2022\n28079\nMadrid\n10\n10\nLatina\n237048\n109928\n127120\n0.536263\n2022\n\n\n2022_11\nCarabanchel\n1 de enero de 2022\n28079\nMadrid\n11\n11\nCarabanchel\n255514\n119381\n136133\n0.532781\n2022\n\n\n2022_13\nPuente de Vallecas\n1 de enero de 2022\n28079\nMadrid\n13\n13\nPuente de Vallecas\n235638\n111748\n123890\n0.525764\n2022\n\n\n2022_15\nCiudad Lineal\n1 de enero de 2022\n28079\nMadrid\n15\n15\nCiudad Lineal\n213905\n97357\n116548\n0.544859\n2022\n\n\n2022_16\nHortaleza\n1 de enero de 2022\n28079\nMadrid\n16\n16\nHortaleza\n195017\n92532\n102485\n0.525518\n2022\n\n\n2021_8\nFuencarral-El Pardo\n1 de enero de 2021\n28079\nMadrid\n8\n8\nFuencarral-ElPardo\n247692\n116520\n131172\n0.529577\n2021\n\n\n2021_10\nLatina\n1 de enero de 2021\n28079\nMadrid\n10\n10\nLatina\n240155\n111209\n128946\n0.536928\n2021\n\n\n2021_11\nCarabanchel\n1 de enero de 2021\n28079\nMadrid\n11\n11\nCarabanchel\n258633\n120600\n138033\n0.533702\n2021\n\n\n2021_13\nPuente de Vallecas\n1 de enero de 2021\n28079\nMadrid\n13\n13\nPuentedeVallecas\n239057\n113355\n125702\n0.525824\n2021\n\n\n2021_15\nCiudad Lineal\n1 de enero de 2021\n28079\nMadrid\n15\n15\nCiudadLineal\n216818\n98514\n118304\n0.545637\n2021\n\n\n2021_16\nHortaleza\n1 de enero de 2021\n28079\nMadrid\n16\n16\nHortaleza\n193228\n91585\n101643\n0.526026\n2021\n\n\n2020_8\nFuencarral-El Pardo\n1 de enero de 2020\n28079\nMadrid\n8\n8\nFuencarral-ElPardo\n249973\n117640\n132333\n0.529389\n2020\n\n\n2020_10\nLatina\n1 de enero de 2020\n28079\nMadrid\n10\n10\nLatina\n242139\n112282\n129857\n0.536291\n2020\n\n\n2020_11\nCarabanchel\n1 de enero de 2020\n28079\nMadrid\n11\n11\nCarabanchel\n260196\n121317\n138879\n0.533748\n2020\n\n\n2020_13\nPuente de Vallecas\n1 de enero de 2020\n28079\nMadrid\n13\n13\nPuentedeVallecas\n240867\n114235\n126632\n0.525734\n2020\n\n\n2020_15\nCiudad Lineal\n1 de enero de 2020\n28079\nMadrid\n15\n15\nCiudadLineal\n219867\n99966\n119901\n0.545334\n2020\n\n\n2020_16\nHortaleza\n1 de enero de 2020\n28079\nMadrid\n16\n16\nHortaleza\n193264\n91659\n101605\n0.525732\n2020\n\n\n2019_8\nFuencarral-El Pardo\n1 de enero de 2019\n28079\nMadrid\n8\n8\nFuencarral-ElPardo\n246021\n115797\n130224\n0.529321\n2019\n\n\n2019_10\nLatina\n1 de enero de 2019\n28079\nMadrid\n10\n10\nLatina\n238154\n110401\n127753\n0.53643\n2019\n\n\n2019_11\nCarabanchel\n1 de enero de 2019\n28079\nMadrid\n11\n11\nCarabanchel\n253040\n117802\n135238\n0.534453\n2019\n\n\n2019_13\nPuente de Vallecas\n1 de enero de 2019\n28079\nMadrid\n13\n13\nPuentedeVallecas\n234770\n111183\n123587\n0.526417\n2019\n\n\n2019_15\nCiudad Lineal\n1 de enero de 2019\n28079\nMadrid\n15\n15\nCiudadLineal\n216270\n98370\n117900\n0.545152\n2019\n\n\n2018_8\nFuencarral-El Pardo\n1 de enero de 2018\n28079\nMadrid\n8\n8\nFuencarral-ElPardo\n242928\n114433\n128495\n0.528943\n2018\n\n\n2018_10\nLatina\n1 de enero de 2018\n28079\nMadrid\n10\n10\nLatina\n235785\n109392\n126393\n0.536052\n2018\n\n\n2018_11\nCarabanchel\n1 de enero de 2018\n28079\nMadrid\n11\n11\nCarabanchel\n248220\n115525\n132695\n0.534586\n2018\n\n\n2018_13\nPuente de Vallecas\n1 de enero de 2018\n28079\nMadrid\n13\n13\nPuentedeVallecas\n230488\n109044\n121444\n0.526899\n2018\n\n\n2018_15\nCiudad Lineal\n1 de enero de 2018\n28079\nMadrid\n15\n15\nCiudadLineal\n214463\n97301\n117162\n0.546304\n2018\n\n\n\n\n\n\n\n\n\nSorting\nAmong the many operations DataFrame objects support, one of the most useful ones is to sort a table based on a given column. For example, imagine you want to sort the table by the influenza cases:\n\nmadrid_sorted = madrid_pop.sort_values(\"ratio_women\", ascending=False)\nmadrid_sorted\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2018_158\nCiudad Lineal\n1 de enero de 2018\n28079\nMadrid\n15\n158\nAtalaya\n1575\n656\n919\n0.583492\n2018\n\n\n2019_158\nCiudad Lineal\n1 de enero de 2019\n28079\nMadrid\n15\n158\nAtalaya\n1568\n654\n914\n0.582908\n2019\n\n\n2023_158\nCiudad Lineal\n1 de enero de 2023\n28079\nMadrid\n15\n158\nAtalaya\n1622\n691\n931\n0.573983\n2023\n\n\n2020_158\nCiudad Lineal\n1 de enero de 2020\n28079\nMadrid\n15\n158\nAtalaya\n1555\n667\n888\n0.571061\n2020\n\n\n2020_45\nSalamanca\n1 de enero de 2020\n28079\nMadrid\n4\n45\nLista\n21211\n9111\n12100\n0.570459\n2020\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2020_12\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n12\nEmbajadores\n47048\n24497\n22551\n0.479319\n2020\n\n\n2022_16\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n16\nSol\n8117\n4232\n3885\n0.478625\n2022\n\n\n2023_12\nCentro\n1 de enero de 2023\n28079\nMadrid\n1\n12\nEmbajadores\n46204\n24094\n22110\n0.47853\n2023\n\n\n2022_12\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n12\nEmbajadores\n46444\n24271\n22173\n0.477414\n2022\n\n\n2021_12\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n12\nEmbajadores\n47238\n24767\n22471\n0.475698\n2021\n\n\n\n\n912 rows × 12 columns\n\n\n\nGiven the rates differ, it may be better to sort by neighbourhood and then by year.\n\nsort_ls = [\"code_neighbourhood\", \"year\"]\nmadrid_sorted = madrid_pop.sort_values(sort_ls, ascending=True)\nmadrid_sorted\n\n\n\n\n\n\n\n\ndistrict\ndate\ncode_municipality\nmunicipality\ncode_district\ncode_neighbourhood\nneighbourhood\nnum_people\nnum_people_men\nnum_people_women\nratio_women\nyear\n\n\nnew_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2018_1\nCentro\n1 de enero de 2018\n28079\nMadrid\n1\n1\nCentro\n132352\n66320\n66032\n0.498912\n2018\n\n\n2019_1\nCentro\n1 de enero de 2019\n28079\nMadrid\n1\n1\nCentro\n134881\n67829\n67052\n0.49712\n2019\n\n\n2020_1\nCentro\n1 de enero de 2020\n28079\nMadrid\n1\n1\nCentro\n140473\n71127\n69346\n0.493661\n2020\n\n\n2021_1\nCentro\n1 de enero de 2021\n28079\nMadrid\n1\n1\nCentro\n141236\n71881\n69355\n0.491058\n2021\n\n\n2022_1\nCentro\n1 de enero de 2022\n28079\nMadrid\n1\n1\nCentro\n139682\n70986\n68696\n0.491803\n2022\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2019_97\nMoncloa-Aravaca\n1 de enero de 2019\n28079\nMadrid\n9\n97\nAravaca\n26823\n12619\n14204\n0.529546\n2019\n\n\n2020_97\nMoncloa-Aravaca\n1 de enero de 2020\n28079\nMadrid\n9\n97\nAravaca\n27503\n12899\n14604\n0.530997\n2020\n\n\n2021_97\nMoncloa-Aravaca\n1 de enero de 2021\n28079\nMadrid\n9\n97\nAravaca\n27568\n12896\n14672\n0.532211\n2021\n\n\n2022_97\nMoncloa-Aravaca\n1 de enero de 2022\n28079\nMadrid\n9\n97\nAravaca\n27323\n12779\n14544\n0.532299\n2022\n\n\n2023_97\nMoncloa-Aravaca\n1 de enero de 2023\n28079\nMadrid\n9\n97\nAravaca\n27445\n12836\n14609\n0.532301\n2023\n\n\n\n\n912 rows × 12 columns\n\n\n\nThis allows you to do so-called hierarchical sorting: sort first based on one column, if equal, then based on another column, etc."
  },
  {
    "objectID": "chapter_2/pandas.html#visual-exploration",
    "href": "chapter_2/pandas.html#visual-exploration",
    "title": "Data wrangling",
    "section": "Visual Exploration",
    "text": "Visual Exploration\nThe next step to continue exploring a dataset is to get a feel for what it looks like, visually. You have already learnt how to unconver and inspect specific parts of the data, to check for particular cases you might be interested in. Now, you will see how to plot the data to get a sense of the overall distribution of values. For that, you can use the plotting capabilities of pandas.\n\nHistograms\nOne of the most common graphical devices to display the distribution of values in a variable is a histogram. Values are assigned into groups of equal intervals, and the groups are plotted as bars rising as high as the number of values into the group.\nA histogram is easily created with the following command. In this case, let us have a look at the shape of the overall numbers of people:\n\n_ = madrid_pop[\"num_people\"].plot.hist()\n\n\n\n\n\nAssigning to _\npandas returns an object with the drawing from its plotting methods. Since you are in Jupyter environment, and you don’t need to work further with that object; you can assign it to _, a convention for an unused variable.\n\nHowever, the default pandas plots can be a bit dull. A better option is to use another package, called seaborn.\n\nimport seaborn as sns\n\n\nWhy sns?\nseaborn is, by convention, imported as sns. That came as a joke after Samuel Normal Seaborn, a fictional character The West Wing show.\n\nThe same plot using seaborn has a more pleasant default style and more customisability.\n\nsns.displot(madrid_pop[\"num_people\"])\n\n\n\n\nNote you are using sns instead of pd, as the function belongs to seaborn instead of pandas.\nYou can quickly see most of the areas have seen somewhere between 0 and 50K people; and very few have more than 200K. However, remember that in this case we are visualizing all years together, which could lead to missinterpretations.\n\n\nLine and bar plots\nAnother very common way of visually displaying a variable is with a line or a bar chart. For example, if you want to generate a line plot of the (sorted) total population per year:\n\ntotal_people_per_year = madrid_pop.groupby(\"year\")[\"num_people\"].sum()\ntotal_people_per_year.plot()\n\n&lt;Axes: xlabel='year'&gt;\n\n\n\n\n\nWhat is evident is the impact of COVID on the total population. But understanding that the data is reported on the 1st of January of each year is crucial to understand why you see the offset on the dates.\nFor a bar plot all you need to do is to change from plot to plot.bar:\n\ntotal_people_per_year.plot.bar()\n\n&lt;Axes: xlabel='year'&gt;\n\n\n\n\n\nLet’s try to plot the ratio_women per neighbourhood, to see if we spot anything in particular.\n\nsns.lineplot(\n    x=\"year\",\n    y=\"ratio_women\",\n    hue=\"neighbourhood\",\n    data=madrid_pop.sort_values(\"year\", ascending=True),\n    legend=False,\n)\n\n&lt;Axes: xlabel='year', ylabel='ratio_women'&gt;\n\n\n\n\n\nWe can see some outliers, but the reality is that the data is hard to read so we probably would need some further analysis and visual considerations to efficiently communicate any possible trend.\n\nOne line or multiple lines?\nYou may have noticed that in some cases, the code is on a single line, but longer code is split into multiple lines. Python requires you to follow the indentation rules, but apart from that, there are not a lot of other limits."
  },
  {
    "objectID": "chapter_2/pandas.html#tidy-data",
    "href": "chapter_2/pandas.html#tidy-data",
    "title": "Data wrangling",
    "section": "Tidy data",
    "text": "Tidy data\n\nClean vs. Tidy\nThis section is a bit more advanced and hence considered optional. Feel free to skip it, move to the next, and return later when you feel more confident.\n\nOnce you can read your data in, explore specific cases, and have a first visual approach to the entire set, the next step can be preparing it for more sophisticated analysis. Maybe you are thinking of modeling it through regression, or on creating subgroups in the dataset with particular characteristics, or maybe you simply need to present summary measures that relate to a slightly different arrangement of the data than you have been presented with.\nFor all these cases, you first need what statistician, and general R wizard, Hadley Wickham calls “tidy data”. The general idea to “tidy” your data is to convert them from whatever structure they were handed in to you into one that allows convenient and standardized manipulation, and that supports directly inputting the data into what he calls “tidy” analysis tools. But, at a more practical level, what is exactly “tidy data”? In Wickham’s own words:\n\nTidy data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types.\n\nHe then goes on to list the three fundamental characteristics of “tidy data”:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\nIf you are further interested in the concept of “tidy data”, I recommend you check out the original paper (open access) and the public repository associated with it."
  },
  {
    "objectID": "chapter_2/pandas.html#grouping-transforming-aggregating",
    "href": "chapter_2/pandas.html#grouping-transforming-aggregating",
    "title": "Data wrangling",
    "section": "Grouping, transforming, aggregating",
    "text": "Grouping, transforming, aggregating\nOne of the advantage of tidy datasets is they allow to perform advanced transformations in a more direct way. One of the most common ones is what is called “group-by” operations. Originated in the world of databases, these operations allow you to group observations in a table by one of its labels, index, or category, and apply operations on the data group by group.\nFor example, given our dataframe, you might want to compute the total sum of the population by each district. This task can be split into two different ones:\n\nGroup the table in each of the different districts.\nCompute the sum of num_people for each of them.\n\nTo do this in pandas, meet one of its workhorses, and also one of the reasons why the library has become so popular: the groupby operator.\n\nmad_grouped = madrid_pop.groupby(\"year\")\nmad_grouped\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x12aea2c90&gt;\n\n\nThe object mad_grouped still hasn’t computed anything. It is only a convenient way of specifying the grouping. But this allows us then to perform a multitude of operations on it. For our example, the sum is calculated as follows:\n\nmad_grouped.sum(numeric_only=True)\n\n\n\n\n\n\n\n\ncode_municipality\nnum_people\nnum_people_men\nnum_people_women\nratio_women\n\n\nyear\n\n\n\n\n\n\n\n\n\n2018\n4268008\n6443648\n3000680\n3442968\n81.100391\n\n\n2019\n4268008\n6532252\n3042356\n3489896\n81.108315\n\n\n2020\n4268008\n6669460\n3109464\n3559996\n81.047788\n\n\n2021\n4268008\n6624620\n3090314\n3534306\n81.013452\n\n\n2022\n4268008\n6573324\n3069648\n3503676\n80.939052\n\n\n2023\n4268008\n6679862\n3119732\n3560130\n80.94286\n\n\n\n\n\n\n\nSimilarly, you can also obtain a summary of each group:\n\nmad_grouped.describe()\n\n\n\n\n\n\n\n\ncode_municipality\nnum_people\n...\nnum_people_women\nratio_women\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ncount\nmean\n...\n75%\nmax\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nyear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2018\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n42392.421053\n...\n21771.75\n132695.0\n152.0\n0.533555\n0.01839\n0.481481\n0.521591\n0.534767\n0.54588\n0.583492\n\n\n2019\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n42975.342105\n...\n21987.0\n135238.0\n152.0\n0.533607\n0.018198\n0.483197\n0.522356\n0.534355\n0.54565\n0.582908\n\n\n2020\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n43878.026316\n...\n22623.75\n138879.0\n152.0\n0.533209\n0.017944\n0.479319\n0.522586\n0.533905\n0.545599\n0.571061\n\n\n2021\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n43583.026316\n...\n22530.75\n138033.0\n152.0\n0.532983\n0.017984\n0.475698\n0.522263\n0.533724\n0.545682\n0.568987\n\n\n2022\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n43245.552632\n...\n22296.5\n136133.0\n152.0\n0.532494\n0.017716\n0.477414\n0.521691\n0.533305\n0.54492\n0.56714\n\n\n2023\n152.0\n28079.0\n0.0\n28079.0\n28079.0\n28079.0\n28079.0\n28079.0\n152.0\n43946.460526\n...\n22742.75\n139707.0\n152.0\n0.532519\n0.017486\n0.47853\n0.522637\n0.533363\n0.544488\n0.573983\n\n\n\n\n6 rows × 40 columns\n\n\n\nYou will not get into it today as it goes beyond the basics this chapter wants to cover, but keep in mind that groupby allows you to not only call generic functions (like sum or describe), but also your own functions. This opens the door for virtually any kind of transformation and aggregation possible.\n\nAdditional reading\n\nA good introduction to data manipulation in Python is Wes McKinney’s “Python for Data Analysis” [@mckinney2012python].\nTo explore further some of the visualization capabilities in at your fingertips, the Python library seaborn is an excellent choice. Its online tutorial is a fantastic place to start.\nA good extension is Hadley Wickham’s “Tidy data” paper [@wickham2014tidy], which presents a very popular way of organising tabular data for efficient manipulation."
  },
  {
    "objectID": "chapter_2/pandas.html#acknowledgements",
    "href": "chapter_2/pandas.html#acknowledgements",
    "title": "Data wrangling",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis section is derived from SDS by @martinfleis which in turn is based on A Course on Geographic Data Science by @darribas_gds_course, both licensed under CC-BY-SA 4.0. The text was slightly adapted, mostly to accommodate a different dataset used."
  }
]